{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnIfQCkU6NR5"
   },
   "source": [
    "# **Neural Machine Translation**\n",
    "\n",
    "### **Team Members:**\n",
    "\n",
    "1. Monesa Thoguluva Janardhanan - 801167556<br>\n",
    "2. Sai Bharadwaj Reddy Arrabelly - 801166672<br>\n",
    "3. Prashanth Minkuri - 801166901"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOzjKaXh7jT9"
   },
   "source": [
    "# **Introduction**\n",
    "\n",
    "The main aim of our project is to build a Neural Machine Translation model by using Transformer model to translate Japanese sentences to English sentences.\n",
    "\n",
    "We have followed the same approach followed in the following paper \"Attention is all you need\" \n",
    "https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "\n",
    "This paper has built a transformer model for English to German and English to French and has accquired about 41.0 BLEU score. \n",
    "\n",
    "We have built the same Transformer model by following the paper for Japanese to English and have succeeded in achieving the state of the art BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RocOUYgOyqc-",
    "outputId": "0d828d94-816b-47d9-cf7f-f59f66240efd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oODvL3e9cq7"
   },
   "source": [
    "## **Dataset Explanation**\n",
    "\n",
    "As a first step we used a Jaoanese English corpus from the following link for our model.\n",
    "\n",
    "https://www.kaggle.com/team-ai/japaneseenglish-bilingual-corpus\n",
    "\n",
    "This dataset mainly deals about traditional Japanese culture, religion, and history. As we went through the dataset it didnt have any daily life conversation or normal words that Japanese people use frequently. It was all about government offices, festivals etc. When we built our baseline model (we used seq2seq model as our basline) and ran this dataset we achieved a very low BLEU score of 4.86. When we built a transformer model and trained with this dataset, it gave us a BLEU score of 14.This made us look into more datasets and we felt if we have huge data for training the model will predict well. So we found another dataset from the following link \n",
    "\n",
    "https://www.manythings.org/anki/\n",
    "\n",
    "This has a normal daily life conversation of Japanese and\n",
    "we trained our model with this dataset. To our suprise we got about 61.0 BLEU score and the next step was to merge our dataset and predicted the BLEU score. This gave us 41.0 which is the state of art BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpgEiMaC_yw-"
   },
   "outputs": [],
   "source": [
    "ANKI_LEXICON_PATH = '/content/drive/MyDrive/NLP/Final Project Dataset/eng_jpn.txt'\n",
    "KYOTO_LEXICON_PATH = '/content/drive/MyDrive/NLP/Final Project Dataset/kyoto_lexicon.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCoJpiRD_2Mn"
   },
   "source": [
    "## **Hyperparamters**\n",
    "\n",
    "The below are the Hyperparamters for our model. \n",
    "We trained our model for 100 Epoch and the rest of the hyperparamters are exactly same as that of the paper becuase we tried tweaking them but we got low BLEU score and high preplexity. \n",
    "\n",
    "So we moved forward with the same values of hyperparamters as given in \"Attention is all you need\" paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "lU-RdLCcr03u"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "EPOCHS = 100\n",
    "\n",
    "D_MODEL = 512\n",
    "HEADS = 8\n",
    "N = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeAXGXZNA6rz"
   },
   "source": [
    "We are training this model on CUDA. For 100 epoch our model takes about 2 hour 30 mins or max 3 hours to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XuACo7PsG0L",
    "outputId": "6a1200e2-d4bd-46b6-d36f-a1a748cd4823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# from janome.tokenizer import Tokenizer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pBLlSqXBIhF"
   },
   "source": [
    "## **Preprocessing**\n",
    "\n",
    "We are using spacy library for tokenization of english and japanese language. spaCy is a free open source library for Natural Language Processing in Python. https://spacy.io/ says spaCy is the best way to prepare text for deep learning. It interoperates seamlessly with TensorFlow, PyTorch, scikit-learn, Gensim and the rest of Python's awesome AI ecosystem. With spaCy, you can easily construct linguistically sophisticated statistical models for a variety of NLP problems.\n",
    "<br/>\n",
    "And for Japanese language there are no spaces between the words. For example I lived in Tokyo sentence translates to 私は東京に住んでいました. Here in the above example there are no spaces between the words. But, <br/><br/>\n",
    "私 - I <br/>\n",
    "は - Pronoun <br/>\n",
    "東京 - Tokyo <br/>\n",
    "に - Pronoun <br/>\n",
    "住ん - Live <br/>\n",
    "で - at <br/>\n",
    "い - continuous <br/>\n",
    "まし - present <br/>\n",
    "た - represents past actions in this case (lived) <br/>\n",
    "\n",
    "<br/>\n",
    "If we use normal tokenizer it would not have recognized as there are no spaces in between. But spaCy library tokenizes into each words which is meaniful.\n",
    "\n",
    "\n",
    "The below libraries are needed to be installed for preprocessing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RO3UAbGTsNpb",
    "outputId": "967012c6-5e49-484a-f33f-9744ff2627f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/bf/ca7bb25edd21f1cf9d498d0023808279672a664a70585e1962617ca2740c/spacy-2.3.5-cp36-cp36m-manylinux2014_x86_64.whl (10.4MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4MB 7.6MB/s \n",
      "\u001b[?25h  Saved ./spacy-2.3.5-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/1a/c3e4ab982214c63d743fad57c45c5e68ee49e4ea4384d27b28595a26ad26/thinc-7.4.5-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 48.3MB/s \n",
      "\u001b[?25h  Saved ./thinc-7.4.5-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/54/115f0c28a61d56674c3a5e05c46d6c3523ad196e1dcd3e2d8b119026df36/tqdm-4.54.1-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
      "\u001b[?25h  Saved ./tqdm-4.54.1-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.15.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/86/753182c9085ba4936c0076269a571613387cdb77ae2bf537448bfd63472c/numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5MB 237kB/s \n",
      "\u001b[?25h  Saved ./numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/9f/e526cd764e2a2cf6958a1389a59c42acda7a65f24949256100cc94c2ff6d/blis-0.7.4-cp36-cp36m-manylinux2014_x86_64.whl (9.8MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8MB 52.3MB/s \n",
      "\u001b[?25h  Saved ./blis-0.7.4-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/fc/f91eac5a39a65f75a7adb58eac7fa78871ea9872283fb9c44e6545998134/requests-2.25.0-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
      "\u001b[?25h  Saved ./requests-2.25.0-py2.py3-none-any.whl\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "  Saved ./plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting setuptools\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/f2/1489d3b6c72d68bf79cd0fba6b6c7497df4ebf7d40970e2d7eceb8d0ea9c/setuptools-51.0.0-py3-none-any.whl (785kB)\n",
      "\u001b[K     |████████████████████████████████| 788kB 51.2MB/s \n",
      "\u001b[?25h  Saved ./setuptools-51.0.0-py3-none-any.whl\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/10/55f3cf6b52cc89107b3e1b88fcf39719392b377a3d78ca61da85934d0d10/wasabi-0.8.0-py3-none-any.whl\n",
      "  Saved ./wasabi-0.8.0-py3-none-any.whl\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/52/d4c2070b724faa3168339c610e909e990f17b3d5d2e56083338410a1a3ee/cymem-2.0.5-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "  Saved ./cymem-2.0.5-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
      "  Saved ./catalogue-1.0.0-py2.py3-none-any.whl\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/32/f18e7430fb5d4e1b136f318cd043e68ae1c708205c92afb9840e5a69d174/preshed-3.0.5-cp36-cp36m-manylinux2014_x86_64.whl (126kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 56.0MB/s \n",
      "\u001b[?25h  Saved ./preshed-3.0.5-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading https://files.pythonhosted.org/packages/93/52/426310135d4e2b1f51acb44c1d44a5ae7cf72f609625775eaedb1afa6fb8/murmurhash-1.0.5-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "  Saved ./murmurhash-1.0.5-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/61/56e8aac918a220f53b11ce341eab7789e6b50e5d9b191a281dbb983e838e/srsly-1.0.5-cp36-cp36m-manylinux2014_x86_64.whl (184kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 57.5MB/s \n",
      "\u001b[?25h  Saved ./srsly-1.0.5-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 54.5MB/s \n",
      "\u001b[?25h  Saved ./urllib3-1.26.2-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/a0/5f06e1e1d463903cf0c0eebeb751791119ed7a4b3737fdc9a77f1cdfb51f/certifi-2020.12.5-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 60.1MB/s \n",
      "\u001b[?25h  Saved ./certifi-2020.12.5-py2.py3-none-any.whl\n",
      "Collecting chardet<4,>=3.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 56.3MB/s \n",
      "\u001b[?25h  Saved ./chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n",
      "\u001b[?25h  Saved ./idna-2.10-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata>=0.20; python_version < \"3.8\"\n",
      "  Downloading https://files.pythonhosted.org/packages/16/0b/a297d4dc836598d44db2da6f5d4d5cb241258851aa9ffd2fe0ba840cbe90/importlib_metadata-3.2.0-py3-none-any.whl\n",
      "  Saved ./importlib_metadata-3.2.0-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
      "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "  Saved ./typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Collecting zipp>=0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/41/ad/6a4f1a124b325618a7fb758b885b68ff7b058eec47d9220a12ab38d90b1f/zipp-3.4.0-py3-none-any.whl\n",
      "  Saved ./zipp-3.4.0-py3-none-any.whl\n",
      "Successfully downloaded spacy thinc tqdm numpy blis requests plac setuptools wasabi cymem catalogue preshed murmurhash srsly urllib3 certifi chardet idna importlib-metadata typing-extensions zipp\n"
     ]
    }
   ],
   "source": [
    "pip download spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMZZfmv8suFl",
    "outputId": "ca1c283b-509c-4640-e032-a22ea2675662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sudachipy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/18/8531c4a1c904cb24d37a09e7dff3273857ee65e0d0adb62d09c6f5492f5d/SudachiPy-0.4.9.tar.gz (67kB)\n",
      "\r",
      "\u001b[K     |████▉                           | 10kB 23.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 20kB 28.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 30kB 15.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 40kB 11.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 51kB 7.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 61kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
      "\u001b[?25hCollecting sudachidict_core\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/8f/38c5bea51bf1c825c8774810d1bdd84f3c0b8e3526f00db954ba940e6191/SudachiDict-core-20200722.tar.gz\n",
      "Collecting sortedcontainers~=2.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n",
      "Collecting dartsclone~=0.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/34/987a076369ed086ee953e2f0b9ab5ff3e1a682ba4f781678ac5648144896/dartsclone-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (474kB)\n",
      "\u001b[K     |████████████████████████████████| 481kB 13.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.9.0->sudachipy) (0.29.21)\n",
      "Building wheels for collected packages: sudachipy, sudachidict-core\n",
      "  Building wheel for sudachipy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sudachipy: filename=SudachiPy-0.4.9-cp36-cp36m-linux_x86_64.whl size=870429 sha256=e499a9ca5ac0aef9e995e14b15da937e0b05f2d3aacc7cfa1463a6d77d5c7550\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/85/b4/bb2fbbf043bc8128a6cbb769604c410dc5ece89c88b7e57de0\n",
      "  Building wheel for sudachidict-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sudachidict-core: filename=SudachiDict_core-20200722-cp36-none-any.whl size=71120191 sha256=e20a4a0af694d18c242173c9295595bf5291b75ebb3a8f341fec9dd85f481ab5\n",
      "  Stored in directory: /root/.cache/pip/wheels/aa/79/cb/ee2ca7db8ba91560158dd4b1362ca1bcc1622541d476de5145\n",
      "Successfully built sudachipy sudachidict-core\n",
      "Installing collected packages: sortedcontainers, dartsclone, sudachipy, sudachidict-core\n",
      "  Found existing installation: sortedcontainers 2.3.0\n",
      "    Uninstalling sortedcontainers-2.3.0:\n",
      "      Successfully uninstalled sortedcontainers-2.3.0\n",
      "Successfully installed dartsclone-0.9.0 sortedcontainers-2.1.0 sudachidict-core-20200722 sudachipy-0.4.9\n"
     ]
    }
   ],
   "source": [
    "pip install sudachipy sudachidict_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "Fac6h7Gasv3u",
    "outputId": "db303815-d4d8-4f66-dd1e-33d947b866b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/e5/bf/ca7bb25edd21f1cf9d498d0023808279672a664a70585e1962617ca2740c/spacy-2.3.5-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/1a/c3e4ab982214c63d743fad57c45c5e68ee49e4ea4384d27b28595a26ad26/thinc-7.4.5-cp36-cp36m-manylinux2014_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
      "Installing collected packages: thinc, spacy\n",
      "  Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "  Found existing installation: spacy 2.2.4\n",
      "    Uninstalling spacy-2.2.4:\n",
      "      Successfully uninstalled spacy-2.2.4\n",
      "Successfully installed spacy-2.3.5 thinc-7.4.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "spacy",
         "thinc"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "H6bspaOB-dcK",
    "outputId": "3342e296-47be-426e-d748-dc3c1df81205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
      "\r",
      "\u001b[K     |█████                           | 10kB 21.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 20kB 28.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 30kB 14.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 40kB 10.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 51kB 7.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 61kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 10.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.18.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "  Found existing installation: torchtext 0.3.1\n",
      "    Uninstalling torchtext-0.3.1:\n",
      "      Successfully uninstalled torchtext-0.3.1\n",
      "Successfully installed sentencepiece-0.1.94 torchtext-0.6.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torchtext"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install torchtext==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jjLqKQLiszfc",
    "outputId": "8f96b6fd-67c2-4280-ec60-17903b5d179d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy[ja] in /usr/local/lib/python3.6/dist-packages (2.3.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (2.23.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (50.3.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (4.41.1)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (7.4.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (1.18.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (1.0.0)\n",
      "Requirement already satisfied: sudachipy>=0.4.5; extra == \"ja\" in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (0.4.9)\n",
      "Requirement already satisfied: sudachidict-core>=20200330; extra == \"ja\" in /usr/local/lib/python3.6/dist-packages (from spacy[ja]) (20200722)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[ja]) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[ja]) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[ja]) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[ja]) (1.24.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[ja]) (3.1.1)\n",
      "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from sudachipy>=0.4.5; extra == \"ja\"->spacy[ja]) (2.1.0)\n",
      "Requirement already satisfied: dartsclone~=0.9.0 in /usr/local/lib/python3.6/dist-packages (from sudachipy>=0.4.5; extra == \"ja\"->spacy[ja]) (0.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy[ja]) (3.4.0)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.9.0->sudachipy>=0.4.5; extra == \"ja\"->spacy[ja]) (0.29.21)\n"
     ]
    }
   ],
   "source": [
    "pip install spacy[ja]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhsLz_0as3R5",
    "outputId": "590ef449-eadc-4b68-955d-3ea59f5345b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.3.5)\n",
      "Collecting download\n",
      "  Downloading https://files.pythonhosted.org/packages/37/45/01e7455a9659528e77a414b222326d4c525796e4f571bbabcb2e0ff3d1f4/download-0.3.5-py3-none-any.whl\n",
      "Requirement already satisfied: en_core_web_sm in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from download) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
      "Installing collected packages: download\n",
      "Successfully installed download-0.3.5\n"
     ]
    }
   ],
   "source": [
    "pip install spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Gdaa8UcBrIS"
   },
   "source": [
    "## **Preprocessing Continued**\n",
    "\n",
    "The below steps are fetching the data and displaying the data. We have a total of 114458 records in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "T52-l6-0sJ09"
   },
   "outputs": [],
   "source": [
    "anki_dataset_df = pd.read_csv(ANKI_LEXICON_PATH,sep='\\t',names=['Japanese','English']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KzccheseNClb",
    "outputId": "f48b557e-ffa8-4db5-9da6-76c5826f9fe8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 2738: expected 3 fields, saw 4\\nSkipping line 2787: expected 3 fields, saw 4\\nSkipping line 2843: expected 3 fields, saw 4\\nSkipping line 2850: expected 3 fields, saw 4\\nSkipping line 2853: expected 3 fields, saw 4\\nSkipping line 2894: expected 3 fields, saw 4\\nSkipping line 3135: expected 3 fields, saw 4\\nSkipping line 3179: expected 3 fields, saw 4\\nSkipping line 3247: expected 3 fields, saw 4\\nSkipping line 3312: expected 3 fields, saw 4\\nSkipping line 3504: expected 3 fields, saw 4\\n'\n"
     ]
    }
   ],
   "source": [
    "kyoto_lexicon_df = pd.read_csv(KYOTO_LEXICON_PATH, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5OY-1TNsN_4",
    "outputId": "817f0614-12d7-44a7-82fe-735d403f9a61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Japanese', 'English'], dtype='object')"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anki_dataset_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "eoXc2TP7sQJD",
    "outputId": "f7a90f8e-7cd9-431e-847a-5f9e41703845"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Japanese</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>行け。</td>\n",
       "      <td>Go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>行きなさい。</td>\n",
       "      <td>Go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>こんにちは。</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>もしもし。</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>やっほー。</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>こんにちは！</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>走れ！</td>\n",
       "      <td>Run!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>走れ。</td>\n",
       "      <td>Run.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>走って！</td>\n",
       "      <td>Run.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>誰？</td>\n",
       "      <td>Who?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Japanese English\n",
       "0      行け。     Go.\n",
       "1   行きなさい。     Go.\n",
       "2   こんにちは。     Hi.\n",
       "3    もしもし。     Hi.\n",
       "4    やっほー。     Hi.\n",
       "5   こんにちは！     Hi.\n",
       "6      走れ！    Run!\n",
       "7      走れ。    Run.\n",
       "8     走って！    Run.\n",
       "9       誰？    Who?"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anki_dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNYLa-vvsRtz",
    "outputId": "5c07a0f9-8926-471f-d965-b87d9d9e1f13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62487"
      ]
     },
     "execution_count": 116,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anki_dataset_df['English'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZKbInH_wsTXU",
    "outputId": "76efa1b2-74fd-4ced-a8ad-25e3ee857d6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62487"
      ]
     },
     "execution_count": 117,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anki_dataset_df['Japanese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "WtYFOZtwtDAN"
   },
   "outputs": [],
   "source": [
    "anki_dataset_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fdYjVAyLtEdb",
    "outputId": "0ca4c218-30dc-404d-8f8a-cb9826820d5d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'電話してね。'"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anki_dataset_df['Japanese'].iloc[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "VFj1ACD1tF4r",
    "outputId": "eb679459-7d64-484d-d7f5-6493c788a847"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Japanese</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>行け。</td>\n",
       "      <td>Go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>行きなさい。</td>\n",
       "      <td>Go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>こんにちは。</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>もしもし。</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>やっほー。</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Japanese English\n",
       "0      行け。     Go.\n",
       "1   行きなさい。     Go.\n",
       "2   こんにちは。     Hi.\n",
       "3    もしもし。     Hi.\n",
       "4    やっほー。     Hi."
      ]
     },
     "execution_count": 120,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anki_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8uyubZ7Mpmj",
    "outputId": "d370dc5d-9c82-418b-961e-8a3bde9a5b38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['日本語', '英語', 'Unnamed: 2'], dtype='object')"
      ]
     },
     "execution_count": 121,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kyoto_lexicon_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "07Ith8gQNc2D",
    "outputId": "4a575cf7-8c1c-47cb-d2f5-5a4bec1dd693"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>日本語</th>\n",
       "      <th>英語</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102世吉田日厚貫首</td>\n",
       "      <td>the 102nd head priest, Nikko TOSHIDA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1月15日：成人祭、新年祭</td>\n",
       "      <td>15th January: Seijin-sai (Adult Festival), the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1月3日：家運隆盛、商売繁盛祈願祭</td>\n",
       "      <td>3rd January: Prayer Festival for the prosperit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1月7日：七種粥神事</td>\n",
       "      <td>7th January: Nanakusa-gayu shinji (a divine se...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21世紀COEプログラム</td>\n",
       "      <td>The 21st Century Center Of Excellence Program</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21世紀出陣弁当</td>\n",
       "      <td>21-seiki Shutsujin Bento (21st century \"kick-o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2月15日：初卯祭</td>\n",
       "      <td>15th February: Hatsuu-sai or The Rite of the F...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2代将軍足利義詮</td>\n",
       "      <td>the Second Shogun Yoshiakira ASHIKAGA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>305世中村日玄貫首</td>\n",
       "      <td>the 305th chief priest Nichigan NAKAMURA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32世済範入道親王</td>\n",
       "      <td>the 32nd priestly Imperial Prince Saihan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 日本語  ... Unnamed: 2\n",
       "0         102世吉田日厚貫首  ...        NaN\n",
       "1      1月15日：成人祭、新年祭  ...        NaN\n",
       "2  1月3日：家運隆盛、商売繁盛祈願祭  ...        NaN\n",
       "3         1月7日：七種粥神事  ...        NaN\n",
       "4       21世紀COEプログラム  ...        NaN\n",
       "5           21世紀出陣弁当  ...        NaN\n",
       "6          2月15日：初卯祭  ...        NaN\n",
       "7           2代将軍足利義詮  ...        NaN\n",
       "8         305世中村日玄貫首  ...        NaN\n",
       "9          32世済範入道親王  ...        NaN\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kyoto_lexicon_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "OrXkEwK3NcaZ"
   },
   "outputs": [],
   "source": [
    "kyoto_lexicon_df = kyoto_lexicon_df[['日本語', '英語']]\n",
    "kyoto_lexicon_df.columns = ['Japanese', 'English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "mtBcmfFyNbfb"
   },
   "outputs": [],
   "source": [
    "kyoto_lexicon_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rWmmdyzkNkcP",
    "outputId": "ba3cb72c-e012-4f4b-9d08-adc3c6e02705"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Japanese</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102世吉田日厚貫首</td>\n",
       "      <td>the 102nd head priest, Nikko TOSHIDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1月15日：成人祭、新年祭</td>\n",
       "      <td>15th January: Seijin-sai (Adult Festival), the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1月3日：家運隆盛、商売繁盛祈願祭</td>\n",
       "      <td>3rd January: Prayer Festival for the prosperit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1月7日：七種粥神事</td>\n",
       "      <td>7th January: Nanakusa-gayu shinji (a divine se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21世紀COEプログラム</td>\n",
       "      <td>The 21st Century Center Of Excellence Program</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Japanese                                            English\n",
       "0         102世吉田日厚貫首               the 102nd head priest, Nikko TOSHIDA\n",
       "1      1月15日：成人祭、新年祭  15th January: Seijin-sai (Adult Festival), the...\n",
       "2  1月3日：家運隆盛、商売繁盛祈願祭  3rd January: Prayer Festival for the prosperit...\n",
       "3         1月7日：七種粥神事  7th January: Nanakusa-gayu shinji (a divine se...\n",
       "4       21世紀COEプログラム      The 21st Century Center Of Excellence Program"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kyoto_lexicon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLJLgRrZtHn9",
    "outputId": "536c48d6-8954-4af5-8a90-8a50d2d3357c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "JA = spacy.blank('ja')\n",
    "EN = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "JCZ8y0kUtJoV"
   },
   "outputs": [],
   "source": [
    "def tokenize_ja(sentence):\n",
    "    return [tok.text for tok in JA.tokenizer(sentence)]\n",
    "\n",
    "def tokenize_en(sentence):\n",
    "    return [tok.text for tok in EN.tokenizer(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "bsEDR5QyyNaR"
   },
   "outputs": [],
   "source": [
    "JA_TEXT = Field(tokenize=tokenize_ja) \n",
    "EN_TEXT = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJvM3Qrb0XeG",
    "outputId": "d2828f11-cc40-418b-97b5-198ce5aef6ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.field.Field at 0x7f245f5e6828>"
      ]
     },
     "execution_count": 129,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JA_TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iIYOqiaCHx3"
   },
   "source": [
    "Merging the dataset here. (Both Kyoto_lexicon and anki_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "HSebMqhvN4e7"
   },
   "outputs": [],
   "source": [
    "frames = [kyoto_lexicon_df, anki_dataset_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "sT7hDzxdOF_j"
   },
   "outputs": [],
   "source": [
    "merged_dataset_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "lDn3_2_4OOGm",
    "outputId": "02c3bafd-ad67-47eb-c586-0fd809d731e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Japanese</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102世吉田日厚貫首</td>\n",
       "      <td>the 102nd head priest, Nikko TOSHIDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1月15日：成人祭、新年祭</td>\n",
       "      <td>15th January: Seijin-sai (Adult Festival), the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1月3日：家運隆盛、商売繁盛祈願祭</td>\n",
       "      <td>3rd January: Prayer Festival for the prosperit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1月7日：七種粥神事</td>\n",
       "      <td>7th January: Nanakusa-gayu shinji (a divine se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21世紀COEプログラム</td>\n",
       "      <td>The 21st Century Center Of Excellence Program</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Japanese                                            English\n",
       "0         102世吉田日厚貫首               the 102nd head priest, Nikko TOSHIDA\n",
       "1      1月15日：成人祭、新年祭  15th January: Seijin-sai (Adult Festival), the...\n",
       "2  1月3日：家運隆盛、商売繁盛祈願祭  3rd January: Prayer Festival for the prosperit...\n",
       "3         1月7日：七種粥神事  7th January: Nanakusa-gayu shinji (a divine se...\n",
       "4       21世紀COEプログラム      The 21st Century Center Of Excellence Program"
      ]
     },
     "execution_count": 132,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtQx1_tuOQKt",
    "outputId": "b297646e-f357-4b9d-d97c-b6f2886ac7ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114458"
      ]
     },
     "execution_count": 133,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_dataset_df[\"Japanese\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSWSgrfXCmd_"
   },
   "source": [
    "## **Dataset Split**\n",
    "\n",
    "We are splitting our dataset into train 60%, val 20% and test 20%. Hence,\n",
    "\n",
    "For training we have 68674 records<br>\n",
    "For validation we have 22892 records<br>\n",
    "For testing we have 22892 records<br>\n",
    "\n",
    "Creating three different csv files in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "mFq7IkumyPPW"
   },
   "outputs": [],
   "source": [
    "# train, val, test = train_val_test_split(kyoto_lexicon_df, test_size=0.3)\n",
    "train, val, test = np.split(merged_dataset_df.sample(frac=1), [int(.6*len(merged_dataset_df)), int(.8*len(merged_dataset_df))])\n",
    "train.to_csv('train.csv', index=False)\n",
    "val.to_csv('val.csv', index=False) \n",
    "test.to_csv('test.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "dhIBcnjsyR9o"
   },
   "outputs": [],
   "source": [
    "data_fields = [('Japanese', JA_TEXT), ('English', EN_TEXT)]\n",
    "\n",
    "train, val, test = TabularDataset.splits(path='./',\n",
    "                        train='train.csv', \n",
    "                        validation='val.csv',\n",
    "                        test = 'test.csv',\n",
    "                        format='csv',\n",
    "              \n",
    "                        fields = data_fields )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yNu43Yu1Q7d",
    "outputId": "32d8c960-b5ff-4b32-e6ab-92981cea9f2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.TabularDataset at 0x7f24563c8940>"
      ]
     },
     "execution_count": 136,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "kLhoHK6ZyUBK"
   },
   "outputs": [],
   "source": [
    "JA_TEXT.build_vocab(train, val) \n",
    "EN_TEXT.build_vocab(train, val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "8V_0aPGhydkA"
   },
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(\n",
    "    train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.English),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0TlVkISJyeR2",
    "outputId": "89dc46b9-89c5-478c-d797-af1461b8332c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [ 1328,    51, 23909,    38,  1128,  8162,  6668,    21,    13, 23911,\n",
      "            13,    13,   870,   124,  7563,     6,  1009,  5675,   467, 10728],\n",
      "        [   15,  1166,  1313,    23,    22,     9,    41,   436,  1252,     3,\n",
      "           181,    27,    28,    22, 15540,  1899,     7,    22,     9,    19],\n",
      "        [    5,    36,     3,    48,  1247, 11337,  4726,  2059,    42,     1,\n",
      "            25,    51,    26, 18517,    19,    10,  4268,  3288,   606,  2552],\n",
      "        [   90,    66,     1,   543,     3,     9,     4,    42,  1078,     1,\n",
      "            44,   230,   379,     3, 15360, 12131,  2405,     3,    16,     9],\n",
      "        [  528,   337,     1,    44,     1,    22,     3,   140,    16,     1,\n",
      "           233,     7,   199,     1,    18,     3,     4,     1,   129,   268],\n",
      "        [   27,    72,     1,    23,     1,     9,     1,     4,     8,     1,\n",
      "            51,  6895,     4,     1,    15,     1,     3,     1,   450,  1662],\n",
      "        [ 2362,    31,     1,  3203,     1,  2545,     1,     3,  1685,     1,\n",
      "            23,    81,     3,     1, 11812,     1,     1,     1,  1898,    18],\n",
      "        [    7,     4,     1,    25,     1,     9,     1,     1,   656,     1,\n",
      "          1041,     6,     1,     1,     9,     1,     1,     1,     3,     3],\n",
      "        [10988,     3,     1,    44,     1,  1233,     1,     1,     4,     1,\n",
      "             4,    70,     1,     1,  1274,     1,     1,     1,     1,     1],\n",
      "        [  208,     1,     1,   630,     1,     3,     1,     1,     3,     1,\n",
      "             3,     4,     1,     1,    19,     1,     1,     1,     1,     1],\n",
      "        [    4,     1,     1,     7,     1,     1,     1,     1,     1,     1,\n",
      "             1,     3,     1,     1,  1546,     1,     1,     1,     1,     1],\n",
      "        [    3,     1,     1,   336,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,   334,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,  1418,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,    18,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     4,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,    15,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     3,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,   243,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,   122,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     3,     1,     1,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter)) \n",
    "print(batch.English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-zeaEjA1mOl",
    "outputId": "31927b91-54e0-413b-c0af-06810d404498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  453,    45,  6291,  8095,   874,  3480,  2341,    14,    10,  2772,\n",
      "            10,    10,   324,   105,  1581, 12514,  5165,   225, 15769, 30405],\n",
      "        [   40,     3, 33982,   219,  1223,  2412,     3,     3,     3,   482,\n",
      "             9,    20,     8, 28394,  2721,   325,     7,     4, 12198,     1],\n",
      "        [   22,    46,     1,     5,     1,     4,  6599,   113,  1325,  2385,\n",
      "           209,    45,  1044,     1, 15721,     1,   153,    52,    26,     1],\n",
      "        [    2,     5,     1,    85,     1,  2781,     9,     7,     4,     1,\n",
      "             5,    54,    59,     1,   248,     1,     5,     1,     1,     1],\n",
      "        [  296,   130,     1,     8,     1,   243,    56,  2123,   805,     1,\n",
      "             3,    17,    40,     1,     1,     1,    12,     1,     1,     1],\n",
      "        [28696,     8,     1,   725,     1,     1,    28,  1996,    11,     1,\n",
      "            17,   115,    22,     1,     1,     1,     8,     1,     1,     1],\n",
      "        [    7,   237,     1,    14,     1,     1,    22,     6,   588,     1,\n",
      "            45,     3,     2,     1,     1,     1,     2,     1,     1,     1],\n",
      "        [   12,    20,     1,     3,     1,     1,    21,     2,     7,     1,\n",
      "             9,  2675,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    8,   136,     1,   925,     1,     1,    24,     1,  1672,     1,\n",
      "           450,   388,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [ 1243,     6,     1,    66,     1,     1,     2,     1,    12,     1,\n",
      "           130,  3254,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [   41,     2,     1,     7,     1,     1,     1,     1,     6,     1,\n",
      "            42,    12,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    6,     1,     1,  3486,     1,     1,     1,     1,     2,     1,\n",
      "            20,     8,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,     1,     1,   258,     1,     1,     1,     1,     1,     1,\n",
      "           123,     6,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     6,     1,     1,     1,     1,     1,     1,\n",
      "             6,    22,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     2,     1,     1,     1,     1,     1,     1,\n",
      "            22,    16,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            16,     2,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            54,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.Japanese) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "VmbGCTYKygZR"
   },
   "outputs": [],
   "source": [
    "# Referred from from http://nlp.seas.harvard.edu/2018/04/03/attention.html \n",
    "# Batching matters a ton for speed. We want to have very evenly divided batches, \n",
    "# with absolutely minimal padding. To do this we have to hack a bit around the default \n",
    "# torchtext batching. This code patches their default batching to make sure we search \n",
    "# over enough sentences to find tight batches.\n",
    "\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.Japanese))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.English) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGCB6B1lysZD",
    "outputId": "7796fed7-8257-428f-e701-f179ea0a7952"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter = MyIterator(\n",
    "    train,\n",
    "    batch_size=1300,\n",
    "    device=0,\n",
    "    repeat=False,\n",
    "    sort_key=lambda x: (len(x.Japanese), len(x.English)),\n",
    "    batch_size_fn=batch_size_fn,\n",
    "    train=True,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_9zMXJ3EKH1"
   },
   "source": [
    "# **Transformer Model**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "![transformer_model_architecture.png](data:image/png;base64,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)\n",
    "\n",
    "Similar to the Convolutional Sequence-to-Sequence model, the Transformer does not use any recurrence. It also does not use any convolutional layers. Instead the model is entirely made up of linear layers, attention mechanisms and normalization. \n",
    "\n",
    "Recurrent neural networks are very slow to train and without LSTM the model is not very accurate. But with LSTM, the model makes it much slower to train.\n",
    "\n",
    "First for out baseline model we used seq2seq as our baseline model but since doesn't do parallel computing and no GPU is used we have switched to transformer model which is much faster than the seq2seq model \n",
    "\n",
    "In the seq2seq model the words are passed to the encoder sequentially and there is no use of GPU there. So in order to do parallel computing for the language translation we went with transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqXRE2RqKynH"
   },
   "source": [
    "First the tokens are passed through the standard embedding layer. Next, as the model has no recurrent it has no idea about the order of the tokens within the sequence. We solve this by using a second embedding layer called a positional embedding layer. The next function after the Enbedder is the PositionEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "yZxEYQ6Yyux2"
   },
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        # print(\"inside embedder init\")\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"inside embedder forward\")\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N3vBW2zLVRE"
   },
   "source": [
    "As mentioned above Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. We have used the formula (PE(pos,2i)=sin(pos100002i/dmodel), PE(pos,2i+1)=cos(pos100002i/dmodel). with dmodel=512) for Positional Encoder from the following reference. \n",
    "\n",
    "http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "YFGQy8Blyzfe"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        # print(\"inside PositionalEncoder init\")\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"inside PositionalEncoder forward\")\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], \\\n",
    "        requires_grad=False).to(device)\n",
    "        # print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eaFLxfzMLUF"
   },
   "source": [
    "The input mask, input_msk, is simply the same shape as the input sentence but has a value of 1 when the token in the source sentence is not a <pad> token and 0 when it is a <pad> token. This is used in the encoder layers to mask the multi-head attention mechanisms, which are used to calculate and apply attention over the source sentence, so the model does not pay attention to <pad> tokens, which contain no useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "vAhLYjPey3JS"
   },
   "outputs": [],
   "source": [
    "def create_masks(input_seq, target_seq):\n",
    "    # print(\"inside create_masks\")\n",
    "    input_pad = JA_TEXT.vocab.stoi['<pad>']\n",
    "    # creates mask with 0s wherever there is padding in the input\n",
    "    input_msk = (input_seq != input_pad).unsqueeze(1)\n",
    "    \n",
    "    target_pad = EN_TEXT.vocab.stoi['<pad>']\n",
    "    target_msk = (target_seq != target_pad).unsqueeze(1)\n",
    "    size = target_seq.size(1) # get seq_len for matrix\n",
    "    nopeak_mask = np.triu(np.ones((1, size, size)), k=1).astype(np.uint8)\n",
    "    nopeak_mask = torch.autograd.Variable(torch.from_numpy(nopeak_mask) == 0).to(device)\n",
    "    target_msk = target_msk & nopeak_mask\n",
    "    \n",
    "    # print(input_msk)\n",
    "    # print(target_msk)\n",
    "    return input_msk, target_msk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guTFfnAQM_FI"
   },
   "source": [
    "We first pass the input sentence and its mask into the multi-head attention layer, then perform dropout on it,  pass it through a Layer Normalization layer. \n",
    "\n",
    "The mutli head attention layer is used by the encoder layer to attend to the input sentence, i.e. it is calculating and applying attention over itself instead of another sequence.\n",
    "\n",
    "Multi head attention means many attention vectors will be created for each word and the Wz weight will choose which attention vector to take. (Multiple attention vector for one word)\n",
    "And the rest of the things in the Attention model are normal like Feed Forward Neural Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "8knxD6o7y5qm"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        # print(\"inside MultiHeadAttention __init__\")\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # print(\"inside MultiHeadAttention forward\")\n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        # print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "1FnRrci4y_iR"
   },
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    # print(\"inside attention\")\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    # print(scores)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "\n",
    "    output = torch.matmul(scores, v)\n",
    "    # print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "Rbazxn1rzBYc"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        # print(\"inside FeedForward init\")\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) \n",
    "    def forward(self, x):\n",
    "        # print(\"inside FeedForward forward\")\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        # print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "TGV7jl_gzD9f"
   },
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        # print(\"inside Norm init\")\n",
    "        super().__init__()a\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        # print(\"inside Norm forward\")\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        # print(norm)\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD-X9EhzOy-F"
   },
   "source": [
    "The objective of the decoder is to take the encoded representation of the source sentence and convert it into predicted tokens in the target sentence. We then compare  with the actual tokens in the target sentence to calculate our loss, which will be used to calculate the gradients of our parameters and then use Adam optimizer to update our weights in order to improve our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "b62B2RMjzGAD"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        # print(\"inside EncoderLayer init\")\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # print(\"inside EncoderLayer forward\")\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        # print(\"inside EncoderLayer forward : \"+x)\n",
    "        return x\n",
    "    \n",
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        # print(\"inside EncoderLayer __init__\")\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model).to(device)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "            # print(\"inside EncoderLayer forward\")\n",
    "            x2 = self.norm_1(x)\n",
    "            x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "            x2 = self.norm_2(x)\n",
    "            x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
    "            src_mask))\n",
    "            x2 = self.norm_3(x)\n",
    "            x = x + self.dropout_3(self.ff(x2))\n",
    "            # print(x)\n",
    "            return x\n",
    "\n",
    "# We can then build a convenient cloning function that can generate multiple layers:\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kILnOLx3PQOG"
   },
   "source": [
    "The decoder is similar to encoder, however it has two multi-head attention layers. A *masked multi-head attention layer* over the target sequence, and a multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "0oDIYBWczL6G"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        # print(\"inside Encoder __init__\")\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask):\n",
    "        # print(\"inside Encoder forward\")\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        # print(\"inside Decoder __init__\")\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        # print(\"inside Decoder forward\")\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfmczIlBPY8Z"
   },
   "source": [
    "The main Transformer model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "LzR9yM1JzPcv"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        # print(\"inside Transformer __init__\")\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        # print(\"inside Transformer forward\")\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "# we don't perform softmax on the output as this will be handled \n",
    "# automatically by our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "EweXXucIzSZ9"
   },
   "outputs": [],
   "source": [
    "src_vocab = len(JA_TEXT.vocab)\n",
    "trg_vocab = len(EN_TEXT.vocab)\n",
    "# print(JA_TEXT.vocab)\n",
    "model = Transformer(src_vocab, trg_vocab, D_MODEL, N, HEADS)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "D_f3tvW7zUaD"
   },
   "outputs": [],
   "source": [
    "input_pad = JA_TEXT.vocab.stoi['<pad>']\n",
    "target_pad = EN_TEXT.vocab.stoi['<pad>']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhhXS_KAx2zC",
    "outputId": "9666a1aa-1b45-4f14-c2d3-0e05ad4df91d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (out): Linear(in_features=512, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadAttention(HEADS, D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "Wy6rEBtrzXDu"
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs, print_every=50):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    start = datetime.now()\n",
    "    temp = start\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "       \n",
    "        for i, batch in enumerate(train_iter):\n",
    "            src = batch.Japanese.transpose(0, 1)\n",
    "            trg = batch.English.transpose(0, 1)\n",
    "            # the French sentence we input has all words except\n",
    "            # the last, as it is using each word to predict the next\n",
    "            \n",
    "            trg_input = trg[:, :-1]\n",
    "            \n",
    "            # the words we are trying to predict\n",
    "            \n",
    "            targets = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # create function to make masks using mask code above\n",
    "            \n",
    "            src_mask, trg_mask = create_masks(src, trg_input)\n",
    "            \n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            \n",
    "            loss = F.cross_entropy(\n",
    "                preds.view(-1, preds.size(-1)),\n",
    "                targets,\n",
    "                ignore_index=target_pad\n",
    "            )\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            Loss_hist = []\n",
    "            Perplexity_hist = []\n",
    "            if (i + 1) % print_every == 0:\n",
    "                loss_avg = total_loss / print_every\n",
    "                perplexity  = math.exp(loss_avg)\n",
    "                print(\"time = {}, epoch {}, iter = {}, loss = {}, perplexity = {}, {} per {} iters\".format(\n",
    "                    (datetime.now() - start) // 60,\n",
    "                    epoch + 1,\n",
    "                    i + 1,\n",
    "                    loss_avg,\n",
    "                    perplexity,\n",
    "                    datetime.now() - temp,\n",
    "                    print_every\n",
    "                ))\n",
    "                if i+1 == 450:\n",
    "                    Loss_hist.append(loss_avg)\n",
    "                    Perplexity_hist.append(perplexity)\n",
    "                total_loss = 0\n",
    "                temp = datetime.now()\n",
    "    \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z24Ic9Omzb1o",
    "outputId": "24ac3914-1c12-41a0-f09f-4b640fe02516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0:00:00.155824, epoch 1, iter = 50, loss = 9.019476728439331, perplexity = 8262.452443495316, 0:00:09.349480 per 50 iters\n",
      "time = 0:00:00.307910, epoch 1, iter = 100, loss = 7.144762582778931, perplexity = 1267.4503749414623, 0:00:09.125011 per 50 iters\n",
      "time = 0:00:00.462090, epoch 1, iter = 150, loss = 6.4894912147521975, perplexity = 658.1884014617085, 0:00:09.250696 per 50 iters\n",
      "time = 0:00:00.616094, epoch 1, iter = 200, loss = 6.222094306945801, perplexity = 503.7571501195155, 0:00:09.240153 per 50 iters\n",
      "time = 0:00:00.770128, epoch 1, iter = 250, loss = 5.900127840042114, perplexity = 365.08413725363874, 0:00:09.241872 per 50 iters\n",
      "time = 0:00:00.927535, epoch 1, iter = 300, loss = 5.763546733856201, perplexity = 318.47587735535876, 0:00:09.444334 per 50 iters\n",
      "time = 0:00:01.085078, epoch 1, iter = 350, loss = 5.6538427734375, perplexity = 285.3860352615269, 0:00:09.452427 per 50 iters\n",
      "time = 0:00:01.242211, epoch 1, iter = 400, loss = 5.645571060180664, perplexity = 283.03514017959355, 0:00:09.427881 per 50 iters\n",
      "time = 0:00:01.403620, epoch 1, iter = 450, loss = 5.197380542755127, perplexity = 180.79802835005742, 0:00:09.684419 per 50 iters\n",
      "\n",
      "time = 0:00:01.677475, epoch 2, iter = 50, loss = 8.608190865516663, perplexity = 5476.332288878575, 0:00:16.431158 per 50 iters\n",
      "time = 0:00:01.835318, epoch 2, iter = 100, loss = 5.080533738136292, perplexity = 160.85989007802445, 0:00:09.470418 per 50 iters\n",
      "time = 0:00:01.995432, epoch 2, iter = 150, loss = 5.118893871307373, perplexity = 167.15037749888376, 0:00:09.606340 per 50 iters\n",
      "time = 0:00:02.158050, epoch 2, iter = 200, loss = 4.740099482536316, perplexity = 114.44558645106663, 0:00:09.756904 per 50 iters\n",
      "time = 0:00:02.315194, epoch 2, iter = 250, loss = 5.098279943466187, perplexity = 163.74002284466002, 0:00:09.428527 per 50 iters\n",
      "time = 0:00:02.475258, epoch 2, iter = 300, loss = 4.817596497535706, perplexity = 123.66749815421434, 0:00:09.603697 per 50 iters\n",
      "time = 0:00:02.635451, epoch 2, iter = 350, loss = 5.1130406951904295, perplexity = 166.17487458159044, 0:00:09.611488 per 50 iters\n",
      "time = 0:00:02.797178, epoch 2, iter = 400, loss = 4.978182706832886, perplexity = 145.21025212692462, 0:00:09.703402 per 50 iters\n",
      "time = 0:00:02.958647, epoch 2, iter = 450, loss = 4.824313306808472, perplexity = 124.50094507109883, 0:00:09.687975 per 50 iters\n",
      "\n",
      "time = 0:00:03.237062, epoch 3, iter = 50, loss = 7.594953465461731, perplexity = 1988.1373322947627, 0:00:16.704815 per 50 iters\n",
      "time = 0:00:03.397816, epoch 3, iter = 100, loss = 4.700686898231506, perplexity = 110.02272091452208, 0:00:09.645112 per 50 iters\n",
      "time = 0:00:03.560679, epoch 3, iter = 150, loss = 4.631480379104614, perplexity = 102.66593617531147, 0:00:09.771664 per 50 iters\n",
      "time = 0:00:03.728029, epoch 3, iter = 200, loss = 4.369613738059997, perplexity = 79.01310604886739, 0:00:10.040854 per 50 iters\n",
      "time = 0:00:03.891712, epoch 3, iter = 250, loss = 4.479687843322754, perplexity = 88.20713393183617, 0:00:09.820864 per 50 iters\n",
      "time = 0:00:04.056023, epoch 3, iter = 300, loss = 4.405506401062012, perplexity = 81.90060689787335, 0:00:09.858481 per 50 iters\n",
      "time = 0:00:04.216734, epoch 3, iter = 350, loss = 4.681825070381165, perplexity = 107.96694013275894, 0:00:09.642580 per 50 iters\n",
      "time = 0:00:04.380807, epoch 3, iter = 400, loss = 4.39953275680542, perplexity = 81.41282019052936, 0:00:09.844200 per 50 iters\n",
      "time = 0:00:04.542103, epoch 3, iter = 450, loss = 4.5965490770339965, perplexity = 99.1415946243614, 0:00:09.677583 per 50 iters\n",
      "\n",
      "time = 0:00:04.823284, epoch 4, iter = 50, loss = 7.2224214029312135, perplexity = 1369.801890554621, 0:00:16.870743 per 50 iters\n",
      "time = 0:00:04.988146, epoch 4, iter = 100, loss = 4.2258964538574215, perplexity = 68.43582561810972, 0:00:09.891603 per 50 iters\n",
      "time = 0:00:05.154810, epoch 4, iter = 150, loss = 4.107049894332886, perplexity = 60.76718326552113, 0:00:09.999739 per 50 iters\n",
      "time = 0:00:05.319979, epoch 4, iter = 200, loss = 4.241846256256103, perplexity = 69.5361148862283, 0:00:09.910025 per 50 iters\n",
      "time = 0:00:05.482141, epoch 4, iter = 250, loss = 4.342828707695007, perplexity = 76.92482972569339, 0:00:09.729519 per 50 iters\n",
      "time = 0:00:05.643718, epoch 4, iter = 300, loss = 4.322066230773926, perplexity = 75.34414596201772, 0:00:09.694487 per 50 iters\n",
      "time = 0:00:05.809261, epoch 4, iter = 350, loss = 4.229920730590821, perplexity = 68.71178521534304, 0:00:09.932436 per 50 iters\n",
      "time = 0:00:05.973057, epoch 4, iter = 400, loss = 4.209156775474549, perplexity = 67.29976706324933, 0:00:09.827646 per 50 iters\n",
      "time = 0:00:06.140132, epoch 4, iter = 450, loss = 3.9208884859085082, perplexity = 50.44524476216351, 0:00:10.024421 per 50 iters\n",
      "\n",
      "time = 0:00:06.419190, epoch 5, iter = 50, loss = 6.998287644386291, perplexity = 1094.756939319055, 0:00:16.743327 per 50 iters\n",
      "time = 0:00:06.582848, epoch 5, iter = 100, loss = 4.044268851280212, perplexity = 57.069444525075454, 0:00:09.819386 per 50 iters\n",
      "time = 0:00:06.748480, epoch 5, iter = 150, loss = 3.915196738243103, perplexity = 50.15893872185618, 0:00:09.937823 per 50 iters\n",
      "time = 0:00:06.910813, epoch 5, iter = 200, loss = 4.0998414039611815, perplexity = 60.330718625588425, 0:00:09.739491 per 50 iters\n",
      "time = 0:00:07.073533, epoch 5, iter = 250, loss = 4.0504486989974975, perplexity = 57.42321700568337, 0:00:09.763111 per 50 iters\n",
      "time = 0:00:07.238612, epoch 5, iter = 300, loss = 3.9938077974319457, perplexity = 54.261111809705504, 0:00:09.904282 per 50 iters\n",
      "time = 0:00:07.401944, epoch 5, iter = 350, loss = 3.8851067066192626, perplexity = 48.67213582146824, 0:00:09.799801 per 50 iters\n",
      "time = 0:00:07.566032, epoch 5, iter = 400, loss = 3.920732274055481, perplexity = 50.4373652324568, 0:00:09.845119 per 50 iters\n",
      "time = 0:00:07.731863, epoch 5, iter = 450, loss = 3.8972718811035154, perplexity = 49.26785702566756, 0:00:09.949745 per 50 iters\n",
      "\n",
      "time = 0:00:08.013381, epoch 6, iter = 50, loss = 6.264083523750305, perplexity = 525.3598854019683, 0:00:16.890915 per 50 iters\n",
      "time = 0:00:08.175222, epoch 6, iter = 100, loss = 3.806433734893799, perplexity = 44.98970720786464, 0:00:09.710342 per 50 iters\n",
      "time = 0:00:08.341248, epoch 6, iter = 150, loss = 3.532683024406433, perplexity = 34.21564598536555, 0:00:09.961429 per 50 iters\n",
      "time = 0:00:08.505796, epoch 6, iter = 200, loss = 3.6731797075271606, perplexity = 39.3769140802879, 0:00:09.872727 per 50 iters\n",
      "time = 0:00:08.667640, epoch 6, iter = 250, loss = 3.8530475187301634, perplexity = 47.13649391480427, 0:00:09.710534 per 50 iters\n",
      "time = 0:00:08.834523, epoch 6, iter = 300, loss = 3.517721586227417, perplexity = 33.707541180522554, 0:00:10.012808 per 50 iters\n",
      "time = 0:00:09.001066, epoch 6, iter = 350, loss = 3.6551569843292238, perplexity = 38.673591786514066, 0:00:09.992402 per 50 iters\n",
      "time = 0:00:09.166939, epoch 6, iter = 400, loss = 3.817548146247864, perplexity = 45.49253043643081, 0:00:09.952187 per 50 iters\n",
      "time = 0:00:09.328901, epoch 6, iter = 450, loss = 3.88060649394989, perplexity = 48.45359297267331, 0:00:09.717624 per 50 iters\n",
      "\n",
      "time = 0:00:09.612068, epoch 7, iter = 50, loss = 6.05581874370575, perplexity = 426.5880287773529, 0:00:16.989791 per 50 iters\n",
      "time = 0:00:09.774188, epoch 7, iter = 100, loss = 3.642450714111328, perplexity = 38.185303410470304, 0:00:09.727008 per 50 iters\n",
      "time = 0:00:09.940384, epoch 7, iter = 150, loss = 3.449631814956665, perplexity = 31.488796470278803, 0:00:09.971591 per 50 iters\n",
      "time = 0:00:10.100722, epoch 7, iter = 200, loss = 3.710443172454834, perplexity = 40.87191582119194, 0:00:09.620150 per 50 iters\n",
      "time = 0:00:10.266960, epoch 7, iter = 250, loss = 3.3568261575698854, perplexity = 28.697963371149434, 0:00:09.974144 per 50 iters\n",
      "time = 0:00:10.433240, epoch 7, iter = 300, loss = 3.4412259340286253, perplexity = 31.225214766818908, 0:00:09.976642 per 50 iters\n",
      "time = 0:00:10.596499, epoch 7, iter = 350, loss = 3.5372679901123045, perplexity = 34.37288373821186, 0:00:09.795003 per 50 iters\n",
      "time = 0:00:10.763160, epoch 7, iter = 400, loss = 3.3723691272735596, perplexity = 29.147499459262878, 0:00:09.999474 per 50 iters\n",
      "time = 0:00:10.924717, epoch 7, iter = 450, loss = 3.5479052829742432, perplexity = 34.74046976305912, 0:00:09.693245 per 50 iters\n",
      "\n",
      "time = 0:00:11.204289, epoch 8, iter = 50, loss = 5.712195243835449, perplexity = 302.53447678983713, 0:00:16.774235 per 50 iters\n",
      "time = 0:00:11.373044, epoch 8, iter = 100, loss = 3.138072748184204, perplexity = 23.059382769156933, 0:00:10.125162 per 50 iters\n",
      "time = 0:00:11.536704, epoch 8, iter = 150, loss = 3.2354216623306273, perplexity = 25.41708694771818, 0:00:09.819434 per 50 iters\n",
      "time = 0:00:11.700827, epoch 8, iter = 200, loss = 3.3115587759017946, perplexity = 27.42784603279672, 0:00:09.847267 per 50 iters\n",
      "time = 0:00:11.866443, epoch 8, iter = 250, loss = 3.405244216918945, perplexity = 30.12165104254574, 0:00:09.936844 per 50 iters\n",
      "time = 0:00:12.032010, epoch 8, iter = 300, loss = 3.3550048542022703, perplexity = 28.645743242601004, 0:00:09.933852 per 50 iters\n",
      "time = 0:00:12.195188, epoch 8, iter = 350, loss = 3.2865355157852174, perplexity = 26.750027885370184, 0:00:09.790604 per 50 iters\n",
      "time = 0:00:12.361130, epoch 8, iter = 400, loss = 3.316758089065552, perplexity = 27.570823363597285, 0:00:09.956389 per 50 iters\n",
      "time = 0:00:12.523347, epoch 8, iter = 450, loss = 3.3527771711349486, perplexity = 28.582000630957534, 0:00:09.732891 per 50 iters\n",
      "\n",
      "time = 0:00:12.806805, epoch 9, iter = 50, loss = 5.0718183422088625, perplexity = 159.4640240453229, 0:00:17.007361 per 50 iters\n",
      "time = 0:00:12.972690, epoch 9, iter = 100, loss = 3.3264851140975953, perplexity = 27.840313998868254, 0:00:09.952978 per 50 iters\n",
      "time = 0:00:13.134530, epoch 9, iter = 150, loss = 3.126042923927307, perplexity = 22.78364431486571, 0:00:09.710275 per 50 iters\n",
      "time = 0:00:13.295864, epoch 9, iter = 200, loss = 3.0529127955436706, perplexity = 21.176938765993086, 0:00:09.679909 per 50 iters\n",
      "time = 0:00:13.461439, epoch 9, iter = 250, loss = 3.0540497732162475, perplexity = 21.201030165640017, 0:00:09.934336 per 50 iters\n",
      "time = 0:00:13.624321, epoch 9, iter = 300, loss = 3.171034631729126, perplexity = 23.832129082043604, 0:00:09.772830 per 50 iters\n",
      "time = 0:00:13.787964, epoch 9, iter = 350, loss = 3.1782783603668214, perplexity = 24.005389325463053, 0:00:09.818422 per 50 iters\n",
      "time = 0:00:13.954864, epoch 9, iter = 400, loss = 2.9069264101982117, perplexity = 18.300463917300515, 0:00:10.013894 per 50 iters\n",
      "time = 0:00:14.118712, epoch 9, iter = 450, loss = 3.0902501225471495, perplexity = 21.9825756260001, 0:00:09.830752 per 50 iters\n",
      "\n",
      "time = 0:00:14.403613, epoch 10, iter = 50, loss = 4.676666769981384, perplexity = 107.4114481512774, 0:00:17.093956 per 50 iters\n",
      "time = 0:00:14.569827, epoch 10, iter = 100, loss = 2.6912222027778627, perplexity = 14.749692023097792, 0:00:09.972708 per 50 iters\n",
      "time = 0:00:14.729208, epoch 10, iter = 150, loss = 3.2400466203689575, perplexity = 25.53491216662892, 0:00:09.562744 per 50 iters\n",
      "time = 0:00:14.891555, epoch 10, iter = 200, loss = 2.794754273891449, perplexity = 16.358608522026138, 0:00:09.740717 per 50 iters\n",
      "time = 0:00:15.057204, epoch 10, iter = 250, loss = 2.9062410378456116, perplexity = 18.287925582496662, 0:00:09.938780 per 50 iters\n",
      "time = 0:00:15.224201, epoch 10, iter = 300, loss = 2.6356761622428895, perplexity = 13.952743592718798, 0:00:10.019712 per 50 iters\n",
      "time = 0:00:15.387663, epoch 10, iter = 350, loss = 2.7121095132827757, perplexity = 15.061013435217191, 0:00:09.807585 per 50 iters\n",
      "time = 0:00:15.552505, epoch 10, iter = 400, loss = 2.9512429881095885, perplexity = 19.12971696719037, 0:00:09.889776 per 50 iters\n",
      "time = 0:00:15.717191, epoch 10, iter = 450, loss = 2.9477842903137206, perplexity = 19.063667345963747, 0:00:09.881064 per 50 iters\n",
      "\n",
      "time = 0:00:15.998926, epoch 11, iter = 50, loss = 4.658877308368683, perplexity = 105.51755196013693, 0:00:16.903994 per 50 iters\n",
      "time = 0:00:16.163920, epoch 11, iter = 100, loss = 2.7592075419425965, perplexity = 15.78732719517001, 0:00:09.899467 per 50 iters\n",
      "time = 0:00:16.326321, epoch 11, iter = 150, loss = 2.718162088394165, perplexity = 15.152447777769424, 0:00:09.743670 per 50 iters\n",
      "time = 0:00:16.490242, epoch 11, iter = 200, loss = 2.5611398220062256, perplexity = 12.950570251050877, 0:00:09.835106 per 50 iters\n",
      "time = 0:00:16.656876, epoch 11, iter = 250, loss = 2.5214633083343507, perplexity = 12.44679684565107, 0:00:09.997958 per 50 iters\n",
      "time = 0:00:16.818635, epoch 11, iter = 300, loss = 2.8183427095413207, perplexity = 16.749069584618542, 0:00:09.705402 per 50 iters\n",
      "time = 0:00:16.983136, epoch 11, iter = 350, loss = 2.678542232513428, perplexity = 14.563847110846808, 0:00:09.869921 per 50 iters\n",
      "time = 0:00:17.150885, epoch 11, iter = 400, loss = 2.43326758146286, perplexity = 11.396058858522746, 0:00:10.064385 per 50 iters\n",
      "time = 0:00:17.314186, epoch 11, iter = 450, loss = 2.8725065279006956, perplexity = 17.681281328249195, 0:00:09.797978 per 50 iters\n",
      "\n",
      "time = 0:00:17.593785, epoch 12, iter = 50, loss = 4.267993915081024, perplexity = 71.37830096343806, 0:00:16.775758 per 50 iters\n",
      "time = 0:00:17.759529, epoch 12, iter = 100, loss = 2.3430373907089233, perplexity = 10.412816370042352, 0:00:09.944493 per 50 iters\n",
      "time = 0:00:17.923289, epoch 12, iter = 150, loss = 2.552522268295288, perplexity = 12.839447507116752, 0:00:09.825494 per 50 iters\n",
      "time = 0:00:18.086580, epoch 12, iter = 200, loss = 2.4622543478012084, perplexity = 11.731228020919866, 0:00:09.797350 per 50 iters\n",
      "time = 0:00:18.251041, epoch 12, iter = 250, loss = 2.461192715167999, perplexity = 11.7187803750079, 0:00:09.867482 per 50 iters\n",
      "time = 0:00:18.413188, epoch 12, iter = 300, loss = 2.5145255732536316, perplexity = 12.360743120668152, 0:00:09.728728 per 50 iters\n",
      "time = 0:00:18.579728, epoch 12, iter = 350, loss = 2.2904038214683533, perplexity = 9.878926198277707, 0:00:09.992199 per 50 iters\n",
      "time = 0:00:18.744718, epoch 12, iter = 400, loss = 2.4818086934089663, perplexity = 11.962882048035734, 0:00:09.899306 per 50 iters\n",
      "time = 0:00:18.912628, epoch 12, iter = 450, loss = 2.485487198829651, perplexity = 12.006968611114337, 0:00:10.074410 per 50 iters\n",
      "\n",
      "time = 0:00:19.192589, epoch 13, iter = 50, loss = 3.976792380809784, perplexity = 53.345646980957774, 0:00:16.797561 per 50 iters\n",
      "time = 0:00:19.355293, epoch 13, iter = 100, loss = 2.2582208704948425, perplexity = 9.566054767771409, 0:00:09.762087 per 50 iters\n",
      "time = 0:00:19.517887, epoch 13, iter = 150, loss = 2.3166973328590394, perplexity = 10.142122876005514, 0:00:09.755471 per 50 iters\n",
      "time = 0:00:19.682890, epoch 13, iter = 200, loss = 2.255416238307953, perplexity = 9.53926309063005, 0:00:09.900085 per 50 iters\n",
      "time = 0:00:19.848576, epoch 13, iter = 250, loss = 2.184635775089264, perplexity = 8.88741094733981, 0:00:09.940997 per 50 iters\n",
      "time = 0:00:20.011005, epoch 13, iter = 300, loss = 2.503273208141327, perplexity = 12.222435131339322, 0:00:09.745659 per 50 iters\n",
      "time = 0:00:20.178215, epoch 13, iter = 350, loss = 2.14323410987854, perplexity = 8.526970241833464, 0:00:10.032447 per 50 iters\n",
      "time = 0:00:20.346143, epoch 13, iter = 400, loss = 2.2238019776344298, perplexity = 9.242403562463045, 0:00:10.075545 per 50 iters\n",
      "time = 0:00:20.509395, epoch 13, iter = 450, loss = 2.2766413784027097, perplexity = 9.743899320008424, 0:00:09.795038 per 50 iters\n",
      "\n",
      "time = 0:00:20.789996, epoch 14, iter = 50, loss = 3.793403675556183, perplexity = 44.407291350265155, 0:00:16.835851 per 50 iters\n",
      "time = 0:00:20.955232, epoch 14, iter = 100, loss = 2.0327354645729065, perplexity = 7.634942935477396, 0:00:09.914055 per 50 iters\n",
      "time = 0:00:21.114407, epoch 14, iter = 150, loss = 2.342634611129761, perplexity = 10.408623144776374, 0:00:09.550377 per 50 iters\n",
      "time = 0:00:21.281078, epoch 14, iter = 200, loss = 2.024793403148651, perplexity = 7.574545905641846, 0:00:10.000176 per 50 iters\n",
      "time = 0:00:21.442598, epoch 14, iter = 250, loss = 2.2194415879249574, perplexity = 9.202190816414241, 0:00:09.691066 per 50 iters\n",
      "time = 0:00:21.608028, epoch 14, iter = 300, loss = 2.0423605275154113, perplexity = 7.708784536756257, 0:00:09.925687 per 50 iters\n",
      "time = 0:00:21.770737, epoch 14, iter = 350, loss = 1.9796653628349303, perplexity = 7.240319699664438, 0:00:09.762329 per 50 iters\n",
      "time = 0:00:21.938249, epoch 14, iter = 400, loss = 2.0095885968208314, perplexity = 7.460247546341601, 0:00:10.050596 per 50 iters\n",
      "time = 0:00:22.103638, epoch 14, iter = 450, loss = 2.298941411972046, perplexity = 9.963629491285328, 0:00:09.923164 per 50 iters\n",
      "\n",
      "time = 0:00:22.387887, epoch 15, iter = 50, loss = 3.316244451999664, perplexity = 27.55666560306725, 0:00:17.054829 per 50 iters\n",
      "time = 0:00:22.551356, epoch 15, iter = 100, loss = 2.030653953552246, perplexity = 7.619067246058363, 0:00:09.808001 per 50 iters\n",
      "time = 0:00:22.717807, epoch 15, iter = 150, loss = 1.8688245153427123, perplexity = 6.480673987309278, 0:00:09.986897 per 50 iters\n",
      "time = 0:00:22.882443, epoch 15, iter = 200, loss = 1.8790451383590698, perplexity = 6.547250158448166, 0:00:09.877960 per 50 iters\n",
      "time = 0:00:23.048244, epoch 15, iter = 250, loss = 1.8796174716949463, perplexity = 6.550998440503236, 0:00:09.947958 per 50 iters\n",
      "time = 0:00:23.212212, epoch 15, iter = 300, loss = 1.9163423418998717, perplexity = 6.79605530668595, 0:00:09.837920 per 50 iters\n",
      "time = 0:00:23.377819, epoch 15, iter = 350, loss = 1.9584216332435609, perplexity = 7.088130561742722, 0:00:09.936299 per 50 iters\n",
      "time = 0:00:23.543416, epoch 15, iter = 400, loss = 1.9627770471572876, perplexity = 7.119069631532624, 0:00:09.935668 per 50 iters\n",
      "time = 0:00:23.705621, epoch 15, iter = 450, loss = 2.1264015197753907, perplexity = 8.38464049664133, 0:00:09.732119 per 50 iters\n",
      "\n",
      "time = 0:00:23.981017, epoch 16, iter = 50, loss = 3.4048627328872683, perplexity = 30.110162305189515, 0:00:16.523634 per 50 iters\n",
      "time = 0:00:24.143916, epoch 16, iter = 100, loss = 1.703923327922821, perplexity = 5.495465666399486, 0:00:09.773526 per 50 iters\n",
      "time = 0:00:24.307294, epoch 16, iter = 150, loss = 1.8243945109844208, perplexity = 6.199040431650347, 0:00:09.802538 per 50 iters\n",
      "time = 0:00:24.473555, epoch 16, iter = 200, loss = 1.6511094319820403, perplexity = 5.212759822793101, 0:00:09.975565 per 50 iters\n",
      "time = 0:00:24.637264, epoch 16, iter = 250, loss = 1.8255766570568084, perplexity = 6.206372936140974, 0:00:09.822332 per 50 iters\n",
      "time = 0:00:24.800677, epoch 16, iter = 300, loss = 1.842552617788315, perplexity = 6.312631447735482, 0:00:09.804643 per 50 iters\n",
      "time = 0:00:24.967627, epoch 16, iter = 350, loss = 1.6460588371753693, perplexity = 5.186498658178061, 0:00:10.016897 per 50 iters\n",
      "time = 0:00:25.131762, epoch 16, iter = 400, loss = 1.9031893301010132, perplexity = 6.707252006734044, 0:00:09.847997 per 50 iters\n",
      "time = 0:00:25.298064, epoch 16, iter = 450, loss = 1.8740643095970153, perplexity = 6.5147205061119715, 0:00:09.978012 per 50 iters\n",
      "\n",
      "time = 0:00:25.577974, epoch 17, iter = 50, loss = 3.037814732789993, perplexity = 20.859609571568065, 0:00:16.794421 per 50 iters\n",
      "time = 0:00:25.738173, epoch 17, iter = 100, loss = 1.885938686132431, perplexity = 6.592539864223907, 0:00:09.611841 per 50 iters\n",
      "time = 0:00:25.902217, epoch 17, iter = 150, loss = 1.6973667681217193, perplexity = 5.45955218023809, 0:00:09.842460 per 50 iters\n",
      "time = 0:00:26.067740, epoch 17, iter = 200, loss = 1.5513398778438567, perplexity = 4.717787208176632, 0:00:09.931227 per 50 iters\n",
      "time = 0:00:26.230898, epoch 17, iter = 250, loss = 1.7125848245620727, perplexity = 5.543271359353603, 0:00:09.789344 per 50 iters\n",
      "time = 0:00:26.395069, epoch 17, iter = 300, loss = 1.6256028068065644, perplexity = 5.081481265615624, 0:00:09.850183 per 50 iters\n",
      "time = 0:00:26.561910, epoch 17, iter = 350, loss = 1.7236846375465393, perplexity = 5.605143383464582, 0:00:10.010309 per 50 iters\n",
      "time = 0:00:26.729945, epoch 17, iter = 400, loss = 1.5712905037403107, perplexity = 4.812855195465623, 0:00:10.081970 per 50 iters\n",
      "time = 0:00:26.895971, epoch 17, iter = 450, loss = 1.658727123737335, perplexity = 5.252620651312339, 0:00:09.961409 per 50 iters\n",
      "\n",
      "time = 0:00:27.178339, epoch 18, iter = 50, loss = 2.5782424664497374, perplexity = 13.173964117119175, 0:00:16.941952 per 50 iters\n",
      "time = 0:00:27.347312, epoch 18, iter = 100, loss = 1.3501808905601502, perplexity = 3.858123365675937, 0:00:10.138241 per 50 iters\n",
      "time = 0:00:27.510203, epoch 18, iter = 150, loss = 1.6177018618583678, perplexity = 5.041490950552096, 0:00:09.773347 per 50 iters\n",
      "time = 0:00:27.673820, epoch 18, iter = 200, loss = 1.6242194032669068, perplexity = 5.074456386687609, 0:00:09.816836 per 50 iters\n",
      "time = 0:00:27.842453, epoch 18, iter = 250, loss = 1.4178927111625672, perplexity = 4.12841151186511, 0:00:10.117892 per 50 iters\n",
      "time = 0:00:28.010144, epoch 18, iter = 300, loss = 1.5721210765838622, perplexity = 4.816854282827706, 0:00:10.061299 per 50 iters\n",
      "time = 0:00:28.173379, epoch 18, iter = 350, loss = 1.502437218427658, perplexity = 4.492625247039581, 0:00:09.793989 per 50 iters\n",
      "time = 0:00:28.334368, epoch 18, iter = 400, loss = 1.8021386122703553, perplexity = 6.0625991591018655, 0:00:09.659256 per 50 iters\n",
      "time = 0:00:28.496407, epoch 18, iter = 450, loss = 1.7439177739620209, perplexity = 5.719708107670103, 0:00:09.722188 per 50 iters\n",
      "\n",
      "time = 0:00:28.777040, epoch 19, iter = 50, loss = 2.6301922821998596, perplexity = 13.876437837617065, 0:00:16.837844 per 50 iters\n",
      "time = 0:00:28.943936, epoch 19, iter = 100, loss = 1.2248853874206542, perplexity = 3.4037759448933316, 0:00:10.013633 per 50 iters\n",
      "time = 0:00:29.110482, epoch 19, iter = 150, loss = 1.3003202366828919, perplexity = 3.6704718991787133, 0:00:09.992656 per 50 iters\n",
      "time = 0:00:29.277781, epoch 19, iter = 200, loss = 1.2123524749279022, perplexity = 3.361382927719895, 0:00:10.037774 per 50 iters\n",
      "time = 0:00:29.440190, epoch 19, iter = 250, loss = 1.5159830284118652, perplexity = 4.553895535946192, 0:00:09.744369 per 50 iters\n",
      "time = 0:00:29.603184, epoch 19, iter = 300, loss = 1.5439331078529357, perplexity = 4.6829727340214955, 0:00:09.779511 per 50 iters\n",
      "time = 0:00:29.764913, epoch 19, iter = 350, loss = 1.5827825260162354, perplexity = 4.868483664163002, 0:00:09.703610 per 50 iters\n",
      "time = 0:00:29.930269, epoch 19, iter = 400, loss = 1.498532143831253, perplexity = 4.475115421155815, 0:00:09.920817 per 50 iters\n",
      "time = 0:00:30.092471, epoch 19, iter = 450, loss = 1.5588864743709565, perplexity = 4.753525124947378, 0:00:09.731967 per 50 iters\n",
      "\n",
      "time = 0:00:30.375774, epoch 20, iter = 50, loss = 2.1731172800064087, perplexity = 8.785628662749925, 0:00:16.998064 per 50 iters\n",
      "time = 0:00:30.539234, epoch 20, iter = 100, loss = 1.22391068816185, perplexity = 3.4004599033368623, 0:00:09.807404 per 50 iters\n",
      "time = 0:00:30.702122, epoch 20, iter = 150, loss = 1.3083692944049836, perplexity = 3.700134959054062, 0:00:09.773141 per 50 iters\n",
      "time = 0:00:30.867793, epoch 20, iter = 200, loss = 1.3094602119922638, perplexity = 3.7041737039245666, 0:00:09.940130 per 50 iters\n",
      "time = 0:00:31.035280, epoch 20, iter = 250, loss = 1.199340237379074, perplexity = 3.3179271561363293, 0:00:10.049139 per 50 iters\n",
      "time = 0:00:31.196656, epoch 20, iter = 300, loss = 1.4461035215854645, perplexity = 4.246535701951397, 0:00:09.682403 per 50 iters\n",
      "time = 0:00:31.358954, epoch 20, iter = 350, loss = 1.5263366270065308, perplexity = 4.601289668906628, 0:00:09.737742 per 50 iters\n",
      "time = 0:00:31.519286, epoch 20, iter = 400, loss = 1.5977213728427886, perplexity = 4.941759158864532, 0:00:09.619794 per 50 iters\n",
      "time = 0:00:31.688993, epoch 20, iter = 450, loss = 1.1187182581424713, perplexity = 3.060928367864683, 0:00:10.182262 per 50 iters\n",
      "\n",
      "time = 0:00:31.972482, epoch 21, iter = 50, loss = 2.224641451239586, perplexity = 9.250165573847939, 0:00:17.009270 per 50 iters\n",
      "time = 0:00:32.139394, epoch 21, iter = 100, loss = 1.0789875531196593, perplexity = 2.94169972815119, 0:00:10.014586 per 50 iters\n",
      "time = 0:00:32.305208, epoch 21, iter = 150, loss = 1.2224962210655212, perplexity = 3.3956534647671712, 0:00:09.948688 per 50 iters\n",
      "time = 0:00:32.471776, epoch 21, iter = 200, loss = 1.1811939895153045, perplexity = 3.258262212226091, 0:00:09.993928 per 50 iters\n",
      "time = 0:00:32.633562, epoch 21, iter = 250, loss = 1.2459050989151002, perplexity = 3.47607957180631, 0:00:09.707028 per 50 iters\n",
      "time = 0:00:32.798097, epoch 21, iter = 300, loss = 1.174913890361786, perplexity = 3.237864120525441, 0:00:09.871955 per 50 iters\n",
      "time = 0:00:32.963356, epoch 21, iter = 350, loss = 1.3005822670459748, perplexity = 3.6714338002812847, 0:00:09.915440 per 50 iters\n",
      "time = 0:00:33.126074, epoch 21, iter = 400, loss = 1.4144325506687165, perplexity = 4.114151231107299, 0:00:09.762969 per 50 iters\n",
      "time = 0:00:33.288809, epoch 21, iter = 450, loss = 1.3964791524410247, perplexity = 4.040947331342118, 0:00:09.763944 per 50 iters\n",
      "\n",
      "time = 0:00:33.574050, epoch 22, iter = 50, loss = 1.7616341090202332, perplexity = 5.821943315518729, 0:00:17.114323 per 50 iters\n",
      "time = 0:00:33.740308, epoch 22, iter = 100, loss = 0.9439473450183868, perplexity = 2.5701065187133123, 0:00:09.975286 per 50 iters\n",
      "time = 0:00:33.905214, epoch 22, iter = 150, loss = 1.097706503868103, perplexity = 2.9972838758976463, 0:00:09.894247 per 50 iters\n",
      "time = 0:00:34.067453, epoch 22, iter = 200, loss = 1.283141267299652, perplexity = 3.6079554959526607, 0:00:09.734213 per 50 iters\n",
      "time = 0:00:34.233406, epoch 22, iter = 250, loss = 1.187112982273102, perplexity = 3.277605031193862, 0:00:09.957048 per 50 iters\n",
      "time = 0:00:34.395575, epoch 22, iter = 300, loss = 1.0325696498155594, perplexity = 2.808272848790079, 0:00:09.729985 per 50 iters\n",
      "time = 0:00:34.559311, epoch 22, iter = 350, loss = 1.2693596577644348, perplexity = 3.5585731279521724, 0:00:09.824065 per 50 iters\n",
      "time = 0:00:34.724466, epoch 22, iter = 400, loss = 1.1136601281166076, perplexity = 3.0454848846795364, 0:00:09.909133 per 50 iters\n",
      "time = 0:00:34.889942, epoch 22, iter = 450, loss = 1.2852794635295868, perplexity = 3.6156782662487448, 0:00:09.928401 per 50 iters\n",
      "\n",
      "time = 0:00:35.169300, epoch 23, iter = 50, loss = 1.9935022401809692, perplexity = 7.341199436040149, 0:00:16.761382 per 50 iters\n",
      "time = 0:00:35.333924, epoch 23, iter = 100, loss = 1.0359348791837693, perplexity = 2.817739250427442, 0:00:09.877260 per 50 iters\n",
      "time = 0:00:35.493510, epoch 23, iter = 150, loss = 1.2038387799263, perplexity = 3.3328866152707755, 0:00:09.575041 per 50 iters\n",
      "time = 0:00:35.661435, epoch 23, iter = 200, loss = 0.9871290349960327, perplexity = 2.683519112924576, 0:00:10.075383 per 50 iters\n",
      "time = 0:00:35.829527, epoch 23, iter = 250, loss = 0.9155879408121109, perplexity = 2.4982436395943783, 0:00:10.085384 per 50 iters\n",
      "time = 0:00:35.994242, epoch 23, iter = 300, loss = 1.0295461070537568, perplexity = 2.7997947391672167, 0:00:09.882778 per 50 iters\n",
      "time = 0:00:36.160942, epoch 23, iter = 350, loss = 1.0415419244766235, perplexity = 2.8335828183206817, 0:00:10.001821 per 50 iters\n",
      "time = 0:00:36.323315, epoch 23, iter = 400, loss = 1.1991099947690964, perplexity = 3.317163315865848, 0:00:09.742258 per 50 iters\n",
      "time = 0:00:36.488035, epoch 23, iter = 450, loss = 1.1306360107660294, perplexity = 3.0976259972310114, 0:00:09.883064 per 50 iters\n",
      "\n",
      "time = 0:00:36.773477, epoch 24, iter = 50, loss = 1.6178467041254043, perplexity = 5.042221224416602, 0:00:17.126386 per 50 iters\n",
      "time = 0:00:36.935404, epoch 24, iter = 100, loss = 1.0590340667963027, perplexity = 2.8835842937887537, 0:00:09.715447 per 50 iters\n",
      "time = 0:00:37.101465, epoch 24, iter = 150, loss = 1.0605396825075148, perplexity = 2.88792913361487, 0:00:09.963551 per 50 iters\n",
      "time = 0:00:37.265630, epoch 24, iter = 200, loss = 0.9570157700777053, perplexity = 2.6039141884984556, 0:00:09.849671 per 50 iters\n",
      "time = 0:00:37.431483, epoch 24, iter = 250, loss = 0.9690260869264603, perplexity = 2.6353765814058394, 0:00:09.951058 per 50 iters\n",
      "time = 0:00:37.593748, epoch 24, iter = 300, loss = 1.0431954658031464, perplexity = 2.838272140538479, 0:00:09.735748 per 50 iters\n",
      "time = 0:00:37.758007, epoch 24, iter = 350, loss = 0.9626503378152848, perplexity = 2.6186275321150054, 0:00:09.855407 per 50 iters\n",
      "time = 0:00:37.920942, epoch 24, iter = 400, loss = 0.9500834023952485, perplexity = 2.585925322688146, 0:00:09.775863 per 50 iters\n",
      "time = 0:00:38.086675, epoch 24, iter = 450, loss = 0.901633774638176, perplexity = 2.4636248327387262, 0:00:09.943815 per 50 iters\n",
      "\n",
      "time = 0:00:38.370214, epoch 25, iter = 50, loss = 1.5476422423124314, perplexity = 4.7003747628435635, 0:00:17.012250 per 50 iters\n",
      "time = 0:00:38.532765, epoch 25, iter = 100, loss = 0.9181546413898468, perplexity = 2.5046641191869727, 0:00:09.752863 per 50 iters\n",
      "time = 0:00:38.696936, epoch 25, iter = 150, loss = 0.998319360613823, perplexity = 2.713717213765441, 0:00:09.850149 per 50 iters\n",
      "time = 0:00:38.860992, epoch 25, iter = 200, loss = 0.8487761831283569, perplexity = 2.336785303997386, 0:00:09.843249 per 50 iters\n",
      "time = 0:00:39.025295, epoch 25, iter = 250, loss = 1.0136112761497498, perplexity = 2.7555340631741294, 0:00:09.858079 per 50 iters\n",
      "time = 0:00:39.191454, epoch 25, iter = 300, loss = 0.8600742328166961, perplexity = 2.363336124291667, 0:00:09.969399 per 50 iters\n",
      "time = 0:00:39.356815, epoch 25, iter = 350, loss = 0.9120561164617539, perplexity = 2.4894358448225, 0:00:09.921502 per 50 iters\n",
      "time = 0:00:39.519101, epoch 25, iter = 400, loss = 0.9578649169206619, perplexity = 2.6061262330533577, 0:00:09.737039 per 50 iters\n",
      "time = 0:00:39.682086, epoch 25, iter = 450, loss = 0.9289724081754684, perplexity = 2.5319060744005877, 0:00:09.778952 per 50 iters\n",
      "\n",
      "time = 0:00:39.968052, epoch 26, iter = 50, loss = 1.3731684291362762, perplexity = 3.9478393495354336, 0:00:17.157839 per 50 iters\n",
      "time = 0:00:40.135195, epoch 26, iter = 100, loss = 0.6966273206472396, perplexity = 2.006972405611609, 0:00:10.028312 per 50 iters\n",
      "time = 0:00:40.302037, epoch 26, iter = 150, loss = 0.7826702588796616, perplexity = 2.187305145369679, 0:00:10.010376 per 50 iters\n",
      "time = 0:00:40.467814, epoch 26, iter = 200, loss = 0.9478708899021149, perplexity = 2.5802102552657953, 0:00:09.946514 per 50 iters\n",
      "time = 0:00:40.631860, epoch 26, iter = 250, loss = 0.8412184113264084, perplexity = 2.3191909845939134, 0:00:09.842385 per 50 iters\n",
      "time = 0:00:40.788628, epoch 26, iter = 300, loss = 1.0674571681022644, perplexity = 2.9079755974197115, 0:00:09.405930 per 50 iters\n",
      "time = 0:00:40.951006, epoch 26, iter = 350, loss = 0.8075988095998764, perplexity = 2.242516806813503, 0:00:09.742583 per 50 iters\n",
      "time = 0:00:41.115008, epoch 26, iter = 400, loss = 0.8746675759553909, perplexity = 2.39807798266943, 0:00:09.839575 per 50 iters\n",
      "time = 0:00:41.281285, epoch 26, iter = 450, loss = 0.8786779820919037, perplexity = 2.4077145597050973, 0:00:09.976466 per 50 iters\n",
      "\n",
      "time = 0:00:41.564408, epoch 27, iter = 50, loss = 1.4224023681879043, perplexity = 4.147071274784913, 0:00:16.987262 per 50 iters\n",
      "time = 0:00:41.727890, epoch 27, iter = 100, loss = 0.738137710094452, perplexity = 2.092035907169747, 0:00:09.808766 per 50 iters\n",
      "time = 0:00:41.890579, epoch 27, iter = 150, loss = 0.7465444070100784, perplexity = 2.1096971513276794, 0:00:09.761213 per 50 iters\n",
      "time = 0:00:42.057267, epoch 27, iter = 200, loss = 0.671927889585495, perplexity = 1.9580085084627978, 0:00:10.001120 per 50 iters\n",
      "time = 0:00:42.223500, epoch 27, iter = 250, loss = 0.6706748914718628, perplexity = 1.9555566638943116, 0:00:09.973897 per 50 iters\n",
      "time = 0:00:42.389032, epoch 27, iter = 300, loss = 0.7992935788631439, perplexity = 2.2239693145156654, 0:00:09.931777 per 50 iters\n",
      "time = 0:00:42.553591, epoch 27, iter = 350, loss = 0.7733946090936661, perplexity = 2.167110273936681, 0:00:09.873396 per 50 iters\n",
      "time = 0:00:42.716457, epoch 27, iter = 400, loss = 0.9395396143198014, perplexity = 2.558803110801467, 0:00:09.771872 per 50 iters\n",
      "time = 0:00:42.880863, epoch 27, iter = 450, loss = 0.925506586432457, perplexity = 2.5231461282512297, 0:00:09.864219 per 50 iters\n",
      "\n",
      "time = 0:00:43.156704, epoch 28, iter = 50, loss = 1.4832802826166154, perplexity = 4.407379444741312, 0:00:16.550325 per 50 iters\n",
      "time = 0:00:43.321236, epoch 28, iter = 100, loss = 0.660834339261055, perplexity = 1.9364072812216961, 0:00:09.871848 per 50 iters\n",
      "time = 0:00:43.484838, epoch 28, iter = 150, loss = 0.7650587970018387, perplexity = 2.1491207327961215, 0:00:09.815556 per 50 iters\n",
      "time = 0:00:43.653203, epoch 28, iter = 200, loss = 0.6366564446687698, perplexity = 1.8901504794689203, 0:00:10.101771 per 50 iters\n",
      "time = 0:00:43.818692, epoch 28, iter = 250, loss = 0.7392005962133408, perplexity = 2.0942606852291035, 0:00:09.929188 per 50 iters\n",
      "time = 0:00:43.981893, epoch 28, iter = 300, loss = 0.83626624584198, perplexity = 2.3077343580170226, 0:00:09.791971 per 50 iters\n",
      "time = 0:00:44.148898, epoch 28, iter = 350, loss = 0.7218257611989976, perplexity = 2.0581875412985404, 0:00:10.020165 per 50 iters\n",
      "time = 0:00:44.314842, epoch 28, iter = 400, loss = 0.7297678732872009, perplexity = 2.0745989819351327, 0:00:09.956531 per 50 iters\n",
      "time = 0:00:44.478419, epoch 28, iter = 450, loss = 0.7269717067480087, perplexity = 2.068806160300901, 0:00:09.814471 per 50 iters\n",
      "\n",
      "time = 0:00:44.755435, epoch 29, iter = 50, loss = 1.1300251019001006, perplexity = 3.0957342079602266, 0:00:16.620830 per 50 iters\n",
      "time = 0:00:44.921038, epoch 29, iter = 100, loss = 0.7090903103351593, perplexity = 2.0321417991650126, 0:00:09.936097 per 50 iters\n",
      "time = 0:00:45.088263, epoch 29, iter = 150, loss = 0.5488301289081573, perplexity = 1.7312265208681934, 0:00:10.033335 per 50 iters\n",
      "time = 0:00:45.255209, epoch 29, iter = 200, loss = 0.7111891847848892, perplexity = 2.036411488869393, 0:00:10.016637 per 50 iters\n",
      "time = 0:00:45.418455, epoch 29, iter = 250, loss = 0.6859235374629498, perplexity = 1.9856047694064918, 0:00:09.794664 per 50 iters\n",
      "time = 0:00:45.583265, epoch 29, iter = 300, loss = 0.7167628672719002, perplexity = 2.0477934902862054, 0:00:09.888454 per 50 iters\n",
      "time = 0:00:45.747775, epoch 29, iter = 350, loss = 0.8015133172273636, perplexity = 2.2289114275936956, 0:00:09.870482 per 50 iters\n",
      "time = 0:00:45.912821, epoch 29, iter = 400, loss = 0.6637323915958404, perplexity = 1.9420272303841735, 0:00:09.902592 per 50 iters\n",
      "time = 0:00:46.076661, epoch 29, iter = 450, loss = 0.7020456308126449, perplexity = 2.0178763183126507, 0:00:09.830272 per 50 iters\n",
      "\n",
      "time = 0:00:46.356483, epoch 30, iter = 50, loss = 1.2678362876176834, perplexity = 3.5531562309012092, 0:00:16.789131 per 50 iters\n",
      "time = 0:00:46.521399, epoch 30, iter = 100, loss = 0.5327900084853172, perplexity = 1.7036789626991027, 0:00:09.894872 per 50 iters\n",
      "time = 0:00:46.689578, epoch 30, iter = 150, loss = 0.4737754933163524, perplexity = 1.6060463784032524, 0:00:10.090579 per 50 iters\n",
      "time = 0:00:46.852292, epoch 30, iter = 200, loss = 0.6294390049576759, perplexity = 1.876557544439236, 0:00:09.762763 per 50 iters\n",
      "time = 0:00:47.017786, epoch 30, iter = 250, loss = 0.5427888286113739, perplexity = 1.7207991906046862, 0:00:09.929479 per 50 iters\n",
      "time = 0:00:47.185963, epoch 30, iter = 300, loss = 0.6024900639057159, perplexity = 1.8266616462882923, 0:00:10.090488 per 50 iters\n",
      "time = 0:00:47.346531, epoch 30, iter = 350, loss = 0.8823468071222306, perplexity = 2.4165642672352505, 0:00:09.633953 per 50 iters\n",
      "time = 0:00:47.510632, epoch 30, iter = 400, loss = 0.7093751952052116, perplexity = 2.0327208080889205, 0:00:09.845868 per 50 iters\n",
      "time = 0:00:47.671222, epoch 30, iter = 450, loss = 0.8115398889780044, perplexity = 2.25137218196225, 0:00:09.635250 per 50 iters\n",
      "\n",
      "time = 0:00:47.956783, epoch 31, iter = 50, loss = 0.8663785222172737, perplexity = 2.378282342310035, 0:00:17.133521 per 50 iters\n",
      "time = 0:00:48.117804, epoch 31, iter = 100, loss = 0.6779203853011131, perplexity = 1.9697770924379587, 0:00:09.661156 per 50 iters\n",
      "time = 0:00:48.283280, epoch 31, iter = 150, loss = 0.5746656581759453, perplexity = 1.7765364571688016, 0:00:09.928390 per 50 iters\n",
      "time = 0:00:48.447959, epoch 31, iter = 200, loss = 0.5871965882182121, perplexity = 1.7989381752787825, 0:00:09.880651 per 50 iters\n",
      "time = 0:00:48.612960, epoch 31, iter = 250, loss = 0.5392032581567764, perplexity = 1.7146401922283636, 0:00:09.899909 per 50 iters\n",
      "time = 0:00:48.775302, epoch 31, iter = 300, loss = 0.6140931969881058, perplexity = 1.8479800856316988, 0:00:09.740417 per 50 iters\n",
      "time = 0:00:48.938094, epoch 31, iter = 350, loss = 0.7728333592414856, perplexity = 2.1658943248733102, 0:00:09.767405 per 50 iters\n",
      "time = 0:00:49.102895, epoch 31, iter = 400, loss = 0.700034077167511, perplexity = 2.0138213316280664, 0:00:09.887920 per 50 iters\n",
      "time = 0:00:49.268413, epoch 31, iter = 450, loss = 0.582347283065319, perplexity = 1.7902356926663394, 0:00:09.930960 per 50 iters\n",
      "\n",
      "time = 0:00:49.552697, epoch 32, iter = 50, loss = 0.9111080187559127, perplexity = 2.4870767349192433, 0:00:17.056933 per 50 iters\n",
      "time = 0:00:49.718793, epoch 32, iter = 100, loss = 0.4981271755695343, perplexity = 1.6456363948424442, 0:00:09.965536 per 50 iters\n",
      "time = 0:00:49.883088, epoch 32, iter = 150, loss = 0.5895064976811409, perplexity = 1.8030983625712942, 0:00:09.857589 per 50 iters\n",
      "time = 0:00:50.049392, epoch 32, iter = 200, loss = 0.5124143365025521, perplexity = 1.6693166256820453, 0:00:09.978075 per 50 iters\n",
      "time = 0:00:50.215248, epoch 32, iter = 250, loss = 0.48592724710702895, perplexity = 1.6256817188607868, 0:00:09.951237 per 50 iters\n",
      "time = 0:00:50.379623, epoch 32, iter = 300, loss = 0.702148494720459, perplexity = 2.018083895632162, 0:00:09.862357 per 50 iters\n",
      "time = 0:00:50.540222, epoch 32, iter = 350, loss = 0.6486646285653115, perplexity = 1.912984577661232, 0:00:09.635870 per 50 iters\n",
      "time = 0:00:50.706811, epoch 32, iter = 400, loss = 0.5006432136893273, perplexity = 1.6497820919221196, 0:00:09.995137 per 50 iters\n",
      "time = 0:00:50.867630, epoch 32, iter = 450, loss = 0.6763853441178799, perplexity = 1.9667557230357922, 0:00:09.649008 per 50 iters\n",
      "\n",
      "time = 0:00:51.149182, epoch 33, iter = 50, loss = 0.8822940769791603, perplexity = 2.4164368448152316, 0:00:16.893020 per 50 iters\n",
      "time = 0:00:51.314134, epoch 33, iter = 100, loss = 0.5724401345849037, perplexity = 1.7725871296638627, 0:00:09.896579 per 50 iters\n",
      "time = 0:00:51.480069, epoch 33, iter = 150, loss = 0.4718401825428009, perplexity = 1.6029411852703568, 0:00:09.955990 per 50 iters\n",
      "time = 0:00:51.643662, epoch 33, iter = 200, loss = 0.4961689078807831, perplexity = 1.6424169515577014, 0:00:09.815427 per 50 iters\n",
      "time = 0:00:51.806383, epoch 33, iter = 250, loss = 0.5795104128122329, perplexity = 1.78516422323538, 0:00:09.763027 per 50 iters\n",
      "time = 0:00:51.969283, epoch 33, iter = 300, loss = 0.6524630197882653, perplexity = 1.9202646590363186, 0:00:09.773831 per 50 iters\n",
      "time = 0:00:52.134022, epoch 33, iter = 350, loss = 0.5576718947291375, perplexity = 1.7466014911283223, 0:00:09.884168 per 50 iters\n",
      "time = 0:00:52.300547, epoch 33, iter = 400, loss = 0.5312170270085335, perplexity = 1.7010012138252029, 0:00:09.991384 per 50 iters\n",
      "time = 0:00:52.464407, epoch 33, iter = 450, loss = 0.5687957906723022, perplexity = 1.7661389693418552, 0:00:09.831446 per 50 iters\n",
      "\n",
      "time = 0:00:52.748390, epoch 34, iter = 50, loss = 0.8245416915416718, perplexity = 2.280835199687747, 0:00:17.038843 per 50 iters\n",
      "time = 0:00:52.909085, epoch 34, iter = 100, loss = 0.5013227051496506, perplexity = 1.65090348571058, 0:00:09.641570 per 50 iters\n",
      "time = 0:00:53.072894, epoch 34, iter = 150, loss = 0.5921358779072762, perplexity = 1.8078456322049914, 0:00:09.828427 per 50 iters\n",
      "time = 0:00:53.238860, epoch 34, iter = 200, loss = 0.43672429129481316, perplexity = 1.5476293233523373, 0:00:09.957812 per 50 iters\n",
      "time = 0:00:53.404185, epoch 34, iter = 250, loss = 0.5245362702012062, perplexity = 1.6896751139720076, 0:00:09.919356 per 50 iters\n",
      "time = 0:00:53.568811, epoch 34, iter = 300, loss = 0.5362611734867095, perplexity = 1.7096029891765117, 0:00:09.877405 per 50 iters\n",
      "time = 0:00:53.733522, epoch 34, iter = 350, loss = 0.4864529100060463, perplexity = 1.6265365040707553, 0:00:09.882499 per 50 iters\n",
      "time = 0:00:53.900331, epoch 34, iter = 400, loss = 0.4616564291715622, perplexity = 1.5867000657110482, 0:00:10.008432 per 50 iters\n",
      "time = 0:00:54.063957, epoch 34, iter = 450, loss = 0.5489421799778939, perplexity = 1.731420517520376, 0:00:09.817437 per 50 iters\n",
      "\n",
      "time = 0:00:54.343386, epoch 35, iter = 50, loss = 0.863366816341877, perplexity = 2.3711304305395715, 0:00:16.765593 per 50 iters\n",
      "time = 0:00:54.503577, epoch 35, iter = 100, loss = 0.47165375351905825, perplexity = 1.6026423783640753, 0:00:09.611360 per 50 iters\n",
      "time = 0:00:54.667995, epoch 35, iter = 150, loss = 0.4795661550760269, perplexity = 1.6153734285846548, 0:00:09.864978 per 50 iters\n",
      "time = 0:00:54.832831, epoch 35, iter = 200, loss = 0.40635079562664034, perplexity = 1.5013291197832304, 0:00:09.890013 per 50 iters\n",
      "time = 0:00:54.993199, epoch 35, iter = 250, loss = 0.5327219241857528, perplexity = 1.703562972858833, 0:00:09.621972 per 50 iters\n",
      "time = 0:00:55.156930, epoch 35, iter = 300, loss = 0.523217271566391, perplexity = 1.6874479039697738, 0:00:09.823732 per 50 iters\n",
      "time = 0:00:55.325914, epoch 35, iter = 350, loss = 0.4067422431707382, perplexity = 1.5019169264202785, 0:00:10.138936 per 50 iters\n",
      "time = 0:00:55.492161, epoch 35, iter = 400, loss = 0.5108413624763489, perplexity = 1.6666928980570206, 0:00:09.974648 per 50 iters\n",
      "time = 0:00:55.656152, epoch 35, iter = 450, loss = 0.5525141367316246, perplexity = 1.7376161353835913, 0:00:09.839327 per 50 iters\n",
      "\n",
      "time = 0:00:55.935943, epoch 36, iter = 50, loss = 0.8391428193449975, perplexity = 2.3143822825609766, 0:00:16.787316 per 50 iters\n",
      "time = 0:00:56.097970, epoch 36, iter = 100, loss = 0.4913878181576729, perplexity = 1.634583150724489, 0:00:09.721465 per 50 iters\n",
      "time = 0:00:56.263775, epoch 36, iter = 150, loss = 0.3678684037923813, perplexity = 1.4446519157509639, 0:00:09.948225 per 50 iters\n",
      "time = 0:00:56.429550, epoch 36, iter = 200, loss = 0.44362907275557517, perplexity = 1.5583523430636754, 0:00:09.946319 per 50 iters\n",
      "time = 0:00:56.595779, epoch 36, iter = 250, loss = 0.42815814435482025, perplexity = 1.5344287230212583, 0:00:09.973233 per 50 iters\n",
      "time = 0:00:56.763372, epoch 36, iter = 300, loss = 0.3973254507780075, perplexity = 1.4878400699735, 0:00:10.055441 per 50 iters\n",
      "time = 0:00:56.923293, epoch 36, iter = 350, loss = 0.5559041604399682, perplexity = 1.7435166911396018, 0:00:09.595102 per 50 iters\n",
      "time = 0:00:57.083688, epoch 36, iter = 400, loss = 0.5466585701704025, perplexity = 1.7274711397808367, 0:00:09.623605 per 50 iters\n",
      "time = 0:00:57.249877, epoch 36, iter = 450, loss = 0.4588515868782997, perplexity = 1.5822558578247394, 0:00:09.970823 per 50 iters\n",
      "\n",
      "time = 0:00:57.532183, epoch 37, iter = 50, loss = 0.6434278482198715, perplexity = 1.9029928826141598, 0:00:16.938264 per 50 iters\n",
      "time = 0:00:57.699518, epoch 37, iter = 100, loss = 0.42057536959648134, perplexity = 1.5228374979958912, 0:00:10.039949 per 50 iters\n",
      "time = 0:00:57.866017, epoch 37, iter = 150, loss = 0.3317172765731812, perplexity = 1.3933588576288973, 0:00:09.989819 per 50 iters\n",
      "time = 0:00:58.028243, epoch 37, iter = 200, loss = 0.46095735877752303, perplexity = 1.5855912382902575, 0:00:09.733418 per 50 iters\n",
      "time = 0:00:58.194338, epoch 37, iter = 250, loss = 0.38293834567070006, perplexity = 1.466587605805708, 0:00:09.965558 per 50 iters\n",
      "time = 0:00:58.355837, epoch 37, iter = 300, loss = 0.5270707684755326, perplexity = 1.6939630241877157, 0:00:09.689869 per 50 iters\n",
      "time = 0:00:58.521562, epoch 37, iter = 350, loss = 0.499954017996788, perplexity = 1.6486454609363188, 0:00:09.943301 per 50 iters\n",
      "time = 0:00:58.684820, epoch 37, iter = 400, loss = 0.49771391779184343, perplexity = 1.6449564633061988, 0:00:09.795135 per 50 iters\n",
      "time = 0:00:58.847571, epoch 37, iter = 450, loss = 0.4717161911725998, perplexity = 1.6027424467176308, 0:00:09.764925 per 50 iters\n",
      "\n",
      "time = 0:00:59.127584, epoch 38, iter = 50, loss = 0.7326765914261341, perplexity = 2.0806421903590953, 0:00:16.800648 per 50 iters\n",
      "time = 0:00:59.288856, epoch 38, iter = 100, loss = 0.38226699382066726, perplexity = 1.4656033399346402, 0:00:09.676193 per 50 iters\n",
      "time = 0:00:59.451120, epoch 38, iter = 150, loss = 0.4657323431968689, perplexity = 1.5931805166716329, 0:00:09.735675 per 50 iters\n",
      "time = 0:00:59.616097, epoch 38, iter = 200, loss = 0.3765167686343193, perplexity = 1.45719997447284, 0:00:09.898465 per 50 iters\n",
      "time = 0:00:59.777364, epoch 38, iter = 250, loss = 0.44347570449113843, perplexity = 1.5581133595961472, 0:00:09.675947 per 50 iters\n",
      "time = 0:00:59.945462, epoch 38, iter = 300, loss = 0.3114240449666977, perplexity = 1.3653680759075504, 0:00:10.085674 per 50 iters\n",
      "time = 0:01:00.108486, epoch 38, iter = 350, loss = 0.5506323301792144, perplexity = 1.7343493526450604, 0:00:09.781320 per 50 iters\n",
      "time = 0:01:00.277145, epoch 38, iter = 400, loss = 0.3481911861896515, perplexity = 1.4165030396723357, 0:00:10.119430 per 50 iters\n",
      "time = 0:01:00.440168, epoch 38, iter = 450, loss = 0.500645999610424, perplexity = 1.6497866880912568, 0:00:09.781239 per 50 iters\n",
      "\n",
      "time = 0:01:00.724275, epoch 39, iter = 50, loss = 0.5911526885628701, perplexity = 1.8060690511439734, 0:00:17.046321 per 50 iters\n",
      "time = 0:01:00.892124, epoch 39, iter = 100, loss = 0.2932834538817406, perplexity = 1.3408227979986462, 0:00:10.070757 per 50 iters\n",
      "time = 0:01:01.058240, epoch 39, iter = 150, loss = 0.410621365904808, perplexity = 1.5077543612573887, 0:00:09.966823 per 50 iters\n",
      "time = 0:01:01.220356, epoch 39, iter = 200, loss = 0.34656544037163256, perplexity = 1.414202036709155, 0:00:09.726862 per 50 iters\n",
      "time = 0:01:01.385784, epoch 39, iter = 250, loss = 0.4154938109219074, perplexity = 1.515118738173082, 0:00:09.925534 per 50 iters\n",
      "time = 0:01:01.545298, epoch 39, iter = 300, loss = 0.5385773807764054, perplexity = 1.713567373478088, 0:00:09.570696 per 50 iters\n",
      "time = 0:01:01.707421, epoch 39, iter = 350, loss = 0.41985094964504244, perplexity = 1.5217347236136525, 0:00:09.727230 per 50 iters\n",
      "time = 0:01:01.869334, epoch 39, iter = 400, loss = 0.5025183272361755, perplexity = 1.6528785228445477, 0:00:09.714654 per 50 iters\n",
      "time = 0:01:02.034208, epoch 39, iter = 450, loss = 0.4068842324614525, perplexity = 1.5021301976801318, 0:00:09.892321 per 50 iters\n",
      "\n",
      "time = 0:01:02.312604, epoch 40, iter = 50, loss = 0.6724650013446808, perplexity = 1.9590604603398512, 0:00:16.703666 per 50 iters\n",
      "time = 0:01:02.479623, epoch 40, iter = 100, loss = 0.3289030672609806, perplexity = 1.389443166527983, 0:00:10.020987 per 50 iters\n",
      "time = 0:01:02.647114, epoch 40, iter = 150, loss = 0.28225077867507936, perplexity = 1.3261112387101985, 0:00:10.049346 per 50 iters\n",
      "time = 0:01:02.805997, epoch 40, iter = 200, loss = 0.43821630381047727, perplexity = 1.5499401291192765, 0:00:09.532845 per 50 iters\n",
      "time = 0:01:02.971517, epoch 40, iter = 250, loss = 0.35464602053165434, perplexity = 1.4256759049524328, 0:00:09.931049 per 50 iters\n",
      "time = 0:01:03.132985, epoch 40, iter = 300, loss = 0.462739392220974, perplexity = 1.5884193340367159, 0:00:09.687953 per 50 iters\n",
      "time = 0:01:03.299570, epoch 40, iter = 350, loss = 0.3128557774424553, perplexity = 1.367324317796543, 0:00:09.994985 per 50 iters\n",
      "time = 0:01:03.462294, epoch 40, iter = 400, loss = 0.3732233703136444, perplexity = 1.4524087285952627, 0:00:09.763250 per 50 iters\n",
      "time = 0:01:03.626332, epoch 40, iter = 450, loss = 0.4307651424407959, perplexity = 1.5384341946250646, 0:00:09.842152 per 50 iters\n",
      "\n",
      "time = 0:01:03.906701, epoch 41, iter = 50, loss = 0.5877191512286664, perplexity = 1.7998784794899794, 0:00:16.822042 per 50 iters\n",
      "time = 0:01:04.070320, epoch 41, iter = 100, loss = 0.2909645157307386, perplexity = 1.337717115193532, 0:00:09.816914 per 50 iters\n",
      "time = 0:01:04.234211, epoch 41, iter = 150, loss = 0.35414459511637686, perplexity = 1.4249612140168149, 0:00:09.833399 per 50 iters\n",
      "time = 0:01:04.396309, epoch 41, iter = 200, loss = 0.4021263389289379, perplexity = 1.495000197469014, 0:00:09.725706 per 50 iters\n",
      "time = 0:01:04.560842, epoch 41, iter = 250, loss = 0.35971766903996466, perplexity = 1.4329247984112012, 0:00:09.871857 per 50 iters\n",
      "time = 0:01:04.723837, epoch 41, iter = 300, loss = 0.38859421014785767, perplexity = 1.4749059280251804, 0:00:09.778849 per 50 iters\n",
      "time = 0:01:04.886911, epoch 41, iter = 350, loss = 0.41703303918242457, perplexity = 1.5174526475017192, 0:00:09.784230 per 50 iters\n",
      "time = 0:01:05.054035, epoch 41, iter = 400, loss = 0.3645761288702488, perplexity = 1.4399035452371025, 0:00:10.027298 per 50 iters\n",
      "time = 0:01:05.217782, epoch 41, iter = 450, loss = 0.4206023275852203, perplexity = 1.5228785511853666, 0:00:09.824699 per 50 iters\n",
      "\n",
      "time = 0:01:05.499435, epoch 42, iter = 50, loss = 0.5864058205485344, perplexity = 1.7975161954319665, 0:00:16.899024 per 50 iters\n",
      "time = 0:01:05.664291, epoch 42, iter = 100, loss = 0.2794696904718876, perplexity = 1.3224283300088457, 0:00:09.891261 per 50 iters\n",
      "time = 0:01:05.828677, epoch 42, iter = 150, loss = 0.3753423485159874, perplexity = 1.4554896140438214, 0:00:09.862963 per 50 iters\n",
      "time = 0:01:05.992373, epoch 42, iter = 200, loss = 0.3693248370289803, perplexity = 1.446757487756711, 0:00:09.821613 per 50 iters\n",
      "time = 0:01:06.155904, epoch 42, iter = 250, loss = 0.3257836530730128, perplexity = 1.3851156709356611, 0:00:09.811734 per 50 iters\n",
      "time = 0:01:06.321659, epoch 42, iter = 300, loss = 0.3204621773958206, perplexity = 1.3777643887651683, 0:00:09.945083 per 50 iters\n",
      "time = 0:01:06.485838, epoch 42, iter = 350, loss = 0.3307159358263016, perplexity = 1.3919643289454473, 0:00:09.850611 per 50 iters\n",
      "time = 0:01:06.650482, epoch 42, iter = 400, loss = 0.3792867359519005, perplexity = 1.4612419662854974, 0:00:09.878542 per 50 iters\n",
      "time = 0:01:06.812748, epoch 42, iter = 450, loss = 0.39831755071878433, perplexity = 1.4893168884734478, 0:00:09.735819 per 50 iters\n",
      "\n",
      "time = 0:01:07.091272, epoch 43, iter = 50, loss = 0.5765718176960946, perplexity = 1.7799260485741655, 0:00:16.711337 per 50 iters\n",
      "time = 0:01:07.254402, epoch 43, iter = 100, loss = 0.30442160084843634, perplexity = 1.3558405591381983, 0:00:09.787285 per 50 iters\n",
      "time = 0:01:07.420589, epoch 43, iter = 150, loss = 0.2647410754859447, perplexity = 1.3030935292348371, 0:00:09.971075 per 50 iters\n",
      "time = 0:01:07.585252, epoch 43, iter = 200, loss = 0.3630810564756393, perplexity = 1.4377523936600831, 0:00:09.879663 per 50 iters\n",
      "time = 0:01:07.747759, epoch 43, iter = 250, loss = 0.36796636566519736, perplexity = 1.4447934434902463, 0:00:09.750296 per 50 iters\n",
      "time = 0:01:07.912087, epoch 43, iter = 300, loss = 0.33289693780243396, perplexity = 1.3950035189325398, 0:00:09.859544 per 50 iters\n",
      "time = 0:01:08.077066, epoch 43, iter = 350, loss = 0.3230909903347492, perplexity = 1.3813910384203743, 0:00:09.898603 per 50 iters\n",
      "time = 0:01:08.241752, epoch 43, iter = 400, loss = 0.359186971783638, perplexity = 1.4321645509006389, 0:00:09.881089 per 50 iters\n",
      "time = 0:01:08.403988, epoch 43, iter = 450, loss = 0.3885431095957756, perplexity = 1.4748305614436426, 0:00:09.734040 per 50 iters\n",
      "\n",
      "time = 0:01:08.686562, epoch 44, iter = 50, loss = 0.5478634952008724, perplexity = 1.7295538675099762, 0:00:16.954288 per 50 iters\n",
      "time = 0:01:08.851792, epoch 44, iter = 100, loss = 0.25287977486848834, perplexity = 1.2877284502055137, 0:00:09.913696 per 50 iters\n",
      "time = 0:01:09.016993, epoch 44, iter = 150, loss = 0.2733805724978447, perplexity = 1.3144003742845534, 0:00:09.911957 per 50 iters\n",
      "time = 0:01:09.181736, epoch 44, iter = 200, loss = 0.33273336708545687, perplexity = 1.3947753558676692, 0:00:09.884447 per 50 iters\n",
      "time = 0:01:09.346856, epoch 44, iter = 250, loss = 0.29139530286192894, perplexity = 1.33829351065489, 0:00:09.907048 per 50 iters\n",
      "time = 0:01:09.508978, epoch 44, iter = 300, loss = 0.3633721396327019, perplexity = 1.4381709600817663, 0:00:09.727173 per 50 iters\n",
      "time = 0:01:09.675730, epoch 44, iter = 350, loss = 0.3007013885676861, perplexity = 1.3508059152180363, 0:00:10.005037 per 50 iters\n",
      "time = 0:01:09.838725, epoch 44, iter = 400, loss = 0.3725255545973778, perplexity = 1.4513955684986384, 0:00:09.779545 per 50 iters\n",
      "time = 0:01:09.999936, epoch 44, iter = 450, loss = 0.3786311176046729, perplexity = 1.4602842632208166, 0:00:09.672554 per 50 iters\n",
      "\n",
      "time = 0:01:10.279014, epoch 45, iter = 50, loss = 0.5058750279201195, perplexity = 1.658436063594805, 0:00:16.744591 per 50 iters\n",
      "time = 0:01:10.441721, epoch 45, iter = 100, loss = 0.3192651791870594, perplexity = 1.3761161938996673, 0:00:09.761978 per 50 iters\n",
      "time = 0:01:10.607053, epoch 45, iter = 150, loss = 0.2805623410642147, perplexity = 1.3238740718084194, 0:00:09.919810 per 50 iters\n",
      "time = 0:01:10.770762, epoch 45, iter = 200, loss = 0.24173747017979622, perplexity = 1.2734598277480993, 0:00:09.822389 per 50 iters\n",
      "time = 0:01:10.933502, epoch 45, iter = 250, loss = 0.31634522661566733, perplexity = 1.3721038606429643, 0:00:09.764310 per 50 iters\n",
      "time = 0:01:11.095554, epoch 45, iter = 300, loss = 0.3350392724573612, perplexity = 1.3979952868546932, 0:00:09.723000 per 50 iters\n",
      "time = 0:01:11.261702, epoch 45, iter = 350, loss = 0.266854277998209, perplexity = 1.3058501413685417, 0:00:09.968712 per 50 iters\n",
      "time = 0:01:11.428536, epoch 45, iter = 400, loss = 0.3600073139369488, perplexity = 1.4333398978796423, 0:00:10.009927 per 50 iters\n",
      "time = 0:01:11.592543, epoch 45, iter = 450, loss = 0.36122072383761406, perplexity = 1.4350801823284105, 0:00:09.840275 per 50 iters\n",
      "\n",
      "time = 0:01:11.877421, epoch 46, iter = 50, loss = 0.49407790809869767, perplexity = 1.6389862461220603, 0:00:17.092557 per 50 iters\n",
      "time = 0:01:12.040187, epoch 46, iter = 100, loss = 0.2706474770605564, perplexity = 1.3108128973104485, 0:00:09.765844 per 50 iters\n",
      "time = 0:01:12.205927, epoch 46, iter = 150, loss = 0.2397014857828617, perplexity = 1.270869721011848, 0:00:09.944305 per 50 iters\n",
      "time = 0:01:12.372055, epoch 46, iter = 200, loss = 0.23899148568511008, perplexity = 1.2699677236327052, 0:00:09.967522 per 50 iters\n",
      "time = 0:01:12.533612, epoch 46, iter = 250, loss = 0.38373753637075425, perplexity = 1.4677601574648635, 0:00:09.693316 per 50 iters\n",
      "time = 0:01:12.696848, epoch 46, iter = 300, loss = 0.29591824024915697, perplexity = 1.3443602377805526, 0:00:09.794022 per 50 iters\n",
      "time = 0:01:12.857352, epoch 46, iter = 350, loss = 0.3266971269249916, perplexity = 1.3863815159531667, 0:00:09.630136 per 50 iters\n",
      "time = 0:01:13.024206, epoch 46, iter = 400, loss = 0.31701115518808365, perplexity = 1.3730178881128152, 0:00:10.011110 per 50 iters\n",
      "time = 0:01:13.186422, epoch 46, iter = 450, loss = 0.35808743461966513, perplexity = 1.430590698165349, 0:00:09.732827 per 50 iters\n",
      "\n",
      "time = 0:01:13.466619, epoch 47, iter = 50, loss = 0.4198291577026248, perplexity = 1.5217015624195052, 0:00:16.811747 per 50 iters\n",
      "time = 0:01:13.630702, epoch 47, iter = 100, loss = 0.2711544065177441, perplexity = 1.311477555434114, 0:00:09.844807 per 50 iters\n",
      "time = 0:01:13.791654, epoch 47, iter = 150, loss = 0.29815624594688417, perplexity = 1.3473722928935568, 0:00:09.657116 per 50 iters\n",
      "time = 0:01:13.953786, epoch 47, iter = 200, loss = 0.3036485874652863, perplexity = 1.35479288122726, 0:00:09.727687 per 50 iters\n",
      "time = 0:01:14.120425, epoch 47, iter = 250, loss = 0.21793712824583053, perplexity = 1.243508883576468, 0:00:09.997963 per 50 iters\n",
      "time = 0:01:14.284140, epoch 47, iter = 300, loss = 0.36871819481253626, perplexity = 1.4458800897479258, 0:00:09.822760 per 50 iters\n",
      "time = 0:01:14.448395, epoch 47, iter = 350, loss = 0.2618156035244465, perplexity = 1.2992869364070014, 0:00:09.855159 per 50 iters\n",
      "time = 0:01:14.612605, epoch 47, iter = 400, loss = 0.2929697611927986, perplexity = 1.3404022576534116, 0:00:09.852522 per 50 iters\n",
      "time = 0:01:14.778912, epoch 47, iter = 450, loss = 0.2753952504927292, perplexity = 1.3170511371152605, 0:00:09.978204 per 50 iters\n",
      "\n",
      "time = 0:01:15.063065, epoch 48, iter = 50, loss = 0.43837704733479765, perplexity = 1.5501892919832385, 0:00:17.049042 per 50 iters\n",
      "time = 0:01:15.226687, epoch 48, iter = 100, loss = 0.2239739017188549, perplexity = 1.251038369051226, 0:00:09.817245 per 50 iters\n",
      "time = 0:01:15.392456, epoch 48, iter = 150, loss = 0.24591414958238603, perplexity = 1.2787897841920615, 0:00:09.946016 per 50 iters\n",
      "time = 0:01:15.553482, epoch 48, iter = 200, loss = 0.2760086816549301, perplexity = 1.317859305177373, 0:00:09.661433 per 50 iters\n",
      "time = 0:01:15.714215, epoch 48, iter = 250, loss = 0.21213897347450256, perplexity = 1.2363196887828942, 0:00:09.643828 per 50 iters\n",
      "time = 0:01:15.874777, epoch 48, iter = 300, loss = 0.308223527520895, perplexity = 1.3610051770518277, 0:00:09.633642 per 50 iters\n",
      "time = 0:01:16.037408, epoch 48, iter = 350, loss = 0.31923015400767324, perplexity = 1.3760679960271942, 0:00:09.757694 per 50 iters\n",
      "time = 0:01:16.201968, epoch 48, iter = 400, loss = 0.345164056122303, perplexity = 1.4122215842610886, 0:00:09.873501 per 50 iters\n",
      "time = 0:01:16.367436, epoch 48, iter = 450, loss = 0.36930016487836836, perplexity = 1.4467217935784022, 0:00:09.927950 per 50 iters\n",
      "\n",
      "time = 0:01:16.647320, epoch 49, iter = 50, loss = 0.48525819450616836, perplexity = 1.6245944160505572, 0:00:16.792936 per 50 iters\n",
      "time = 0:01:16.813086, epoch 49, iter = 100, loss = 0.19454384103417396, perplexity = 1.214756737909317, 0:00:09.945873 per 50 iters\n",
      "time = 0:01:16.976541, epoch 49, iter = 150, loss = 0.24288722544908523, perplexity = 1.274924836934909, 0:00:09.807175 per 50 iters\n",
      "time = 0:01:17.142190, epoch 49, iter = 200, loss = 0.20922880470752717, perplexity = 1.2327270200103777, 0:00:09.938695 per 50 iters\n",
      "time = 0:01:17.304315, epoch 49, iter = 250, loss = 0.3052404834330082, perplexity = 1.3569512880757562, 0:00:09.727159 per 50 iters\n",
      "time = 0:01:17.469261, epoch 49, iter = 300, loss = 0.2555613824725151, perplexity = 1.291186266791029, 0:00:09.896468 per 50 iters\n",
      "time = 0:01:17.638113, epoch 49, iter = 350, loss = 0.23410158291459082, perplexity = 1.2637728634183534, 0:00:10.130999 per 50 iters\n",
      "time = 0:01:17.800117, epoch 49, iter = 400, loss = 0.4063601863384247, perplexity = 1.5013432183984856, 0:00:09.720061 per 50 iters\n",
      "time = 0:01:17.966027, epoch 49, iter = 450, loss = 0.26074681505560876, perplexity = 1.2978990153407932, 0:00:09.954075 per 50 iters\n",
      "\n",
      "time = 0:01:18.249849, epoch 50, iter = 50, loss = 0.42906623765826224, perplexity = 1.5358227603314185, 0:00:17.029214 per 50 iters\n",
      "time = 0:01:18.413344, epoch 50, iter = 100, loss = 0.2281494230031967, perplexity = 1.2562730274970955, 0:00:09.809484 per 50 iters\n",
      "time = 0:01:18.580488, epoch 50, iter = 150, loss = 0.20475891962647438, perplexity = 1.2272291684308607, 0:00:10.028524 per 50 iters\n",
      "time = 0:01:18.742002, epoch 50, iter = 200, loss = 0.25041804417967795, perplexity = 1.2845623082541247, 0:00:09.690712 per 50 iters\n",
      "time = 0:01:18.908607, epoch 50, iter = 250, loss = 0.2488713827729225, perplexity = 1.2825770609558176, 0:00:09.996089 per 50 iters\n",
      "time = 0:01:19.073936, epoch 50, iter = 300, loss = 0.2705660681426525, perplexity = 1.3107061897944345, 0:00:09.919640 per 50 iters\n",
      "time = 0:01:19.240040, epoch 50, iter = 350, loss = 0.20176713705062865, perplexity = 1.2235630524337044, 0:00:09.966109 per 50 iters\n",
      "time = 0:01:19.403267, epoch 50, iter = 400, loss = 0.2959520423412323, perplexity = 1.3444056807371216, 0:00:09.793417 per 50 iters\n",
      "time = 0:01:19.563896, epoch 50, iter = 450, loss = 0.39904762849211695, perplexity = 1.4904046026408257, 0:00:09.637615 per 50 iters\n",
      "\n",
      "time = 0:01:19.840932, epoch 51, iter = 50, loss = 0.43678998462855817, perplexity = 1.5477309956215484, 0:00:16.622007 per 50 iters\n",
      "time = 0:01:20.006381, epoch 51, iter = 100, loss = 0.2201504784077406, perplexity = 1.2462642523383556, 0:00:09.926358 per 50 iters\n",
      "time = 0:01:20.169733, epoch 51, iter = 150, loss = 0.2334153139591217, perplexity = 1.262905872863803, 0:00:09.800981 per 50 iters\n",
      "time = 0:01:20.335464, epoch 51, iter = 200, loss = 0.25872163504362106, perplexity = 1.2952731959732469, 0:00:09.943665 per 50 iters\n",
      "time = 0:01:20.499823, epoch 51, iter = 250, loss = 0.23379083842039108, perplexity = 1.263380213969114, 0:00:09.861415 per 50 iters\n",
      "time = 0:01:20.665702, epoch 51, iter = 300, loss = 0.2107030676305294, perplexity = 1.2345457240447195, 0:00:09.952573 per 50 iters\n",
      "time = 0:01:20.828612, epoch 51, iter = 350, loss = 0.2976713810861111, perplexity = 1.3467191577682849, 0:00:09.774502 per 50 iters\n",
      "time = 0:01:20.993728, epoch 51, iter = 400, loss = 0.21450819969177246, perplexity = 1.2392522824195724, 0:00:09.906787 per 50 iters\n",
      "time = 0:01:21.156471, epoch 51, iter = 450, loss = 0.3321419830620289, perplexity = 1.393950751858789, 0:00:09.764449 per 50 iters\n",
      "\n",
      "time = 0:01:21.436654, epoch 52, iter = 50, loss = 0.4388424728810787, perplexity = 1.5509109576090807, 0:00:16.810897 per 50 iters\n",
      "time = 0:01:21.600736, epoch 52, iter = 100, loss = 0.1691858943551779, perplexity = 1.1843402806345644, 0:00:09.844745 per 50 iters\n",
      "time = 0:01:21.766430, epoch 52, iter = 150, loss = 0.22748275578022004, perplexity = 1.2554357905567752, 0:00:09.941519 per 50 iters\n",
      "time = 0:01:21.932709, epoch 52, iter = 200, loss = 0.19206003218889237, perplexity = 1.2117432583816417, 0:00:09.976587 per 50 iters\n",
      "time = 0:01:22.098235, epoch 52, iter = 250, loss = 0.2513007064163685, perplexity = 1.285696643438066, 0:00:09.931479 per 50 iters\n",
      "time = 0:01:22.262960, epoch 52, iter = 300, loss = 0.2311829487979412, perplexity = 1.2600897502678594, 0:00:09.883285 per 50 iters\n",
      "time = 0:01:22.424293, epoch 52, iter = 350, loss = 0.26743863806128504, perplexity = 1.3066134510416432, 0:00:09.679851 per 50 iters\n",
      "time = 0:01:22.590534, epoch 52, iter = 400, loss = 0.24134899243712427, perplexity = 1.2729652130284574, 0:00:09.974294 per 50 iters\n",
      "time = 0:01:22.754589, epoch 52, iter = 450, loss = 0.25069870479404927, perplexity = 1.2849228848982217, 0:00:09.843187 per 50 iters\n",
      "\n",
      "time = 0:01:23.034748, epoch 53, iter = 50, loss = 0.48378074370324614, perplexity = 1.6221959299850188, 0:00:16.809400 per 50 iters\n",
      "time = 0:01:23.202258, epoch 53, iter = 100, loss = 0.15807438462972642, perplexity = 1.1712533147115654, 0:00:10.050025 per 50 iters\n",
      "time = 0:01:23.364270, epoch 53, iter = 150, loss = 0.24777958407998085, perplexity = 1.2811775091505098, 0:00:09.720612 per 50 iters\n",
      "time = 0:01:23.528065, epoch 53, iter = 200, loss = 0.24007805496454238, perplexity = 1.2713483815014504, 0:00:09.827523 per 50 iters\n",
      "time = 0:01:23.690222, epoch 53, iter = 250, loss = 0.2697279594838619, perplexity = 1.3096081357961944, 0:00:09.729288 per 50 iters\n",
      "time = 0:01:23.854338, epoch 53, iter = 300, loss = 0.23199613563716412, perplexity = 1.261114855413534, 0:00:09.846794 per 50 iters\n",
      "time = 0:01:24.019942, epoch 53, iter = 350, loss = 0.20495099280029536, perplexity = 1.2274649088712295, 0:00:09.936101 per 50 iters\n",
      "time = 0:01:24.182721, epoch 53, iter = 400, loss = 0.24535539954900742, perplexity = 1.278075459940394, 0:00:09.766615 per 50 iters\n",
      "time = 0:01:24.348753, epoch 53, iter = 450, loss = 0.22948014125227928, perplexity = 1.257945885745256, 0:00:09.961793 per 50 iters\n",
      "\n",
      "time = 0:01:24.631180, epoch 54, iter = 50, loss = 0.35552206352353094, perplexity = 1.4269254055658707, 0:00:16.945492 per 50 iters\n",
      "time = 0:01:24.797586, epoch 54, iter = 100, loss = 0.18847481556236745, perplexity = 1.2074066747456123, 0:00:09.984261 per 50 iters\n",
      "time = 0:01:24.962221, epoch 54, iter = 150, loss = 0.21854376316070556, perplexity = 1.244263468337793, 0:00:09.877972 per 50 iters\n",
      "time = 0:01:25.125003, epoch 54, iter = 200, loss = 0.23238249689340593, perplexity = 1.2616021954719332, 0:00:09.766804 per 50 iters\n",
      "time = 0:01:25.289306, epoch 54, iter = 250, loss = 0.2537489439547062, perplexity = 1.2888481905172144, 0:00:09.858064 per 50 iters\n",
      "time = 0:01:25.451194, epoch 54, iter = 300, loss = 0.2521892447769642, perplexity = 1.2868395419052217, 0:00:09.713049 per 50 iters\n",
      "time = 0:01:25.617172, epoch 54, iter = 350, loss = 0.2155470685660839, perplexity = 1.2405403720054393, 0:00:09.958568 per 50 iters\n",
      "time = 0:01:25.782112, epoch 54, iter = 400, loss = 0.22337024047970772, perplexity = 1.2502833935767856, 0:00:09.896205 per 50 iters\n",
      "time = 0:01:25.944633, epoch 54, iter = 450, loss = 0.20803702101111413, perplexity = 1.2312587511490347, 0:00:09.751154 per 50 iters\n",
      "\n",
      "time = 0:01:26.225844, epoch 55, iter = 50, loss = 0.3952384672313929, perplexity = 1.4847382101185576, 0:00:16.872533 per 50 iters\n",
      "time = 0:01:26.387830, epoch 55, iter = 100, loss = 0.20918077163398266, perplexity = 1.2326678097648016, 0:00:09.719076 per 50 iters\n",
      "time = 0:01:26.552579, epoch 55, iter = 150, loss = 0.17370478361845015, perplexity = 1.1897042937755602, 0:00:09.884812 per 50 iters\n",
      "time = 0:01:26.721761, epoch 55, iter = 200, loss = 0.1621453246474266, perplexity = 1.176031135213239, 0:00:10.150758 per 50 iters\n",
      "time = 0:01:26.882695, epoch 55, iter = 250, loss = 0.2233770851790905, perplexity = 1.250291951420046, 0:00:09.655943 per 50 iters\n",
      "time = 0:01:27.048610, epoch 55, iter = 300, loss = 0.22329599410295486, perplexity = 1.2501905680109218, 0:00:09.954803 per 50 iters\n",
      "time = 0:01:27.209807, epoch 55, iter = 350, loss = 0.24691603533923626, perplexity = 1.280071627485895, 0:00:09.671667 per 50 iters\n",
      "time = 0:01:27.372336, epoch 55, iter = 400, loss = 0.2715306729078293, perplexity = 1.3119711132083531, 0:00:09.751610 per 50 iters\n",
      "time = 0:01:27.535838, epoch 55, iter = 450, loss = 0.2541386595368385, perplexity = 1.2893505726267718, 0:00:09.809976 per 50 iters\n",
      "\n",
      "time = 0:01:27.817386, epoch 56, iter = 50, loss = 0.31377623841166496, perplexity = 1.3685834658727545, 0:00:16.892766 per 50 iters\n",
      "time = 0:01:27.980992, epoch 56, iter = 100, loss = 0.2038730089366436, perplexity = 1.2261424344373824, 0:00:09.816202 per 50 iters\n",
      "time = 0:01:28.144249, epoch 56, iter = 150, loss = 0.17655914075672627, perplexity = 1.193104985804834, 0:00:09.795265 per 50 iters\n",
      "time = 0:01:28.308304, epoch 56, iter = 200, loss = 0.19678102649748325, perplexity = 1.2174774162210602, 0:00:09.843194 per 50 iters\n",
      "time = 0:01:28.472609, epoch 56, iter = 250, loss = 0.2351932568848133, perplexity = 1.2651532446836795, 0:00:09.858191 per 50 iters\n",
      "time = 0:01:28.632425, epoch 56, iter = 300, loss = 0.2964483754336834, perplexity = 1.3450731193883938, 0:00:09.588868 per 50 iters\n",
      "time = 0:01:28.797694, epoch 56, iter = 350, loss = 0.18821397952735425, perplexity = 1.2070917806455725, 0:00:09.915999 per 50 iters\n",
      "time = 0:01:28.962952, epoch 56, iter = 400, loss = 0.23015931993722916, perplexity = 1.2588005459783276, 0:00:09.915375 per 50 iters\n",
      "time = 0:01:29.128320, epoch 56, iter = 450, loss = 0.22290726959705354, perplexity = 1.2497046827439628, 0:00:09.921937 per 50 iters\n",
      "\n",
      "time = 0:01:29.406724, epoch 57, iter = 50, loss = 0.32012085497379306, perplexity = 1.377294207133343, 0:00:16.704117 per 50 iters\n",
      "time = 0:01:29.571045, epoch 57, iter = 100, loss = 0.16043554119765757, perplexity = 1.1740220946434254, 0:00:09.859144 per 50 iters\n",
      "time = 0:01:29.733870, epoch 57, iter = 150, loss = 0.20067644447088243, perplexity = 1.2222292488086473, 0:00:09.769356 per 50 iters\n",
      "time = 0:01:29.898978, epoch 57, iter = 200, loss = 0.16071983762085437, perplexity = 1.1743559123750318, 0:00:09.906361 per 50 iters\n",
      "time = 0:01:30.064406, epoch 57, iter = 250, loss = 0.15500780329108238, perplexity = 1.1676670727156304, 0:00:09.925542 per 50 iters\n",
      "time = 0:01:30.227714, epoch 57, iter = 300, loss = 0.2582079288363457, perplexity = 1.2946079769705525, 0:00:09.798382 per 50 iters\n",
      "time = 0:01:30.392754, epoch 57, iter = 350, loss = 0.21044220060110091, perplexity = 1.2342237137717522, 0:00:09.902244 per 50 iters\n",
      "time = 0:01:30.553415, epoch 57, iter = 400, loss = 0.25149628952145575, perplexity = 1.2859481285721097, 0:00:09.639535 per 50 iters\n",
      "time = 0:01:30.719241, epoch 57, iter = 450, loss = 0.23772243544459343, perplexity = 1.2683570929893417, 0:00:09.949460 per 50 iters\n",
      "\n",
      "time = 0:01:30.997796, epoch 58, iter = 50, loss = 0.3918766079097986, perplexity = 1.479755110060723, 0:00:16.713154 per 50 iters\n",
      "time = 0:01:31.163109, epoch 58, iter = 100, loss = 0.16981306806206703, perplexity = 1.185083300695556, 0:00:09.918638 per 50 iters\n",
      "time = 0:01:31.328280, epoch 58, iter = 150, loss = 0.14365785658359528, perplexity = 1.1544890401193524, 0:00:09.910243 per 50 iters\n",
      "time = 0:01:31.490418, epoch 58, iter = 200, loss = 0.20210372269153595, perplexity = 1.223974955504339, 0:00:09.728040 per 50 iters\n",
      "time = 0:01:31.654610, epoch 58, iter = 250, loss = 0.19554791122674942, perplexity = 1.2159770514788, 0:00:09.851353 per 50 iters\n",
      "time = 0:01:31.815847, epoch 58, iter = 300, loss = 0.191636613458395, perplexity = 1.2112302921968983, 0:00:09.674140 per 50 iters\n",
      "time = 0:01:31.981484, epoch 58, iter = 350, loss = 0.19767921462655066, perplexity = 1.2185714312258138, 0:00:09.938109 per 50 iters\n",
      "time = 0:01:32.148208, epoch 58, iter = 400, loss = 0.18068675249814986, perplexity = 1.198039837522612, 0:00:10.003281 per 50 iters\n",
      "time = 0:01:32.311357, epoch 58, iter = 450, loss = 0.2456650175154209, perplexity = 1.278471236331803, 0:00:09.788818 per 50 iters\n",
      "\n",
      "time = 0:01:32.591750, epoch 59, iter = 50, loss = 0.37026343211531637, perplexity = 1.448116044693536, 0:00:16.823401 per 50 iters\n",
      "time = 0:01:32.754755, epoch 59, iter = 100, loss = 0.17318839639425276, perplexity = 1.1890901042711364, 0:00:09.780137 per 50 iters\n",
      "time = 0:01:32.917859, epoch 59, iter = 150, loss = 0.1830471585690975, perplexity = 1.2008710381053525, 0:00:09.786130 per 50 iters\n",
      "time = 0:01:33.078824, epoch 59, iter = 200, loss = 0.19382025025784969, perplexity = 1.213878069074958, 0:00:09.657773 per 50 iters\n",
      "time = 0:01:33.243696, epoch 59, iter = 250, loss = 0.1863658256828785, perplexity = 1.2048629495763052, 0:00:09.892243 per 50 iters\n",
      "time = 0:01:33.411225, epoch 59, iter = 300, loss = 0.12954365231096746, perplexity = 1.138308800187986, 0:00:10.051565 per 50 iters\n",
      "time = 0:01:33.575829, epoch 59, iter = 350, loss = 0.17395467281341553, perplexity = 1.190001625172183, 0:00:09.875868 per 50 iters\n",
      "time = 0:01:33.738348, epoch 59, iter = 400, loss = 0.24491934940218926, perplexity = 1.2775182764372452, 0:00:09.751012 per 50 iters\n",
      "time = 0:01:33.901862, epoch 59, iter = 450, loss = 0.2186248119175434, perplexity = 1.2443643184319126, 0:00:09.810706 per 50 iters\n",
      "\n",
      "time = 0:01:34.186628, epoch 60, iter = 50, loss = 0.3207314243167639, perplexity = 1.3781353975288708, 0:00:17.085803 per 50 iters\n",
      "time = 0:01:34.349677, epoch 60, iter = 100, loss = 0.14078570313751698, perplexity = 1.1511779277319276, 0:00:09.782777 per 50 iters\n",
      "time = 0:01:34.512420, epoch 60, iter = 150, loss = 0.17800395742058753, perplexity = 1.1948300496704818, 0:00:09.764414 per 50 iters\n",
      "time = 0:01:34.674272, epoch 60, iter = 200, loss = 0.18485638052225112, perplexity = 1.2030456469322377, 0:00:09.710994 per 50 iters\n",
      "time = 0:01:34.841372, epoch 60, iter = 250, loss = 0.15031439885497094, perplexity = 1.1621995795115867, 0:00:10.025892 per 50 iters\n",
      "time = 0:01:34.999991, epoch 60, iter = 300, loss = 0.22121880248188971, perplexity = 1.2475963778860784, 0:00:09.517032 per 50 iters\n",
      "time = 0:01:35.166075, epoch 60, iter = 350, loss = 0.2386291351169348, perplexity = 1.2695076334684827, 0:00:09.964866 per 50 iters\n",
      "time = 0:01:35.331878, epoch 60, iter = 400, loss = 0.212279372215271, perplexity = 1.2364932786960041, 0:00:09.948088 per 50 iters\n",
      "time = 0:01:35.496828, epoch 60, iter = 450, loss = 0.19872775450348854, perplexity = 1.2198498220691143, 0:00:09.896831 per 50 iters\n",
      "\n",
      "time = 0:01:35.773440, epoch 61, iter = 50, loss = 0.3258411678671837, perplexity = 1.3851953378693693, 0:00:16.596603 per 50 iters\n",
      "time = 0:01:35.935931, epoch 61, iter = 100, loss = 0.1307392516732216, perplexity = 1.1396705753700918, 0:00:09.749363 per 50 iters\n",
      "time = 0:01:36.101440, epoch 61, iter = 150, loss = 0.18720829099416733, perplexity = 1.2058784325106922, 0:00:09.930434 per 50 iters\n",
      "time = 0:01:36.263257, epoch 61, iter = 200, loss = 0.17203553073108196, perplexity = 1.1877200330253004, 0:00:09.708897 per 50 iters\n",
      "time = 0:01:36.428491, epoch 61, iter = 250, loss = 0.16111352041363716, perplexity = 1.1748183271067258, 0:00:09.913906 per 50 iters\n",
      "time = 0:01:36.594382, epoch 61, iter = 300, loss = 0.16378549106419085, perplexity = 1.1779616046991244, 0:00:09.953351 per 50 iters\n",
      "time = 0:01:36.755415, epoch 61, iter = 350, loss = 0.22064463563263417, perplexity = 1.2468802550113256, 0:00:09.661849 per 50 iters\n",
      "time = 0:01:36.919646, epoch 61, iter = 400, loss = 0.24220577120780945, perplexity = 1.274056329954964, 0:00:09.853738 per 50 iters\n",
      "time = 0:01:37.083733, epoch 61, iter = 450, loss = 0.2190829860419035, perplexity = 1.244934584594519, 0:00:09.845106 per 50 iters\n",
      "\n",
      "time = 0:01:37.366020, epoch 62, iter = 50, loss = 0.2867289400845766, perplexity = 1.332063095625561, 0:00:16.937103 per 50 iters\n",
      "time = 0:01:37.526931, epoch 62, iter = 100, loss = 0.1574402327882126, perplexity = 1.1705107977243028, 0:00:09.654010 per 50 iters\n",
      "time = 0:01:37.692762, epoch 62, iter = 150, loss = 0.14088713057339192, perplexity = 1.1512946946789593, 0:00:09.949713 per 50 iters\n",
      "time = 0:01:37.858659, epoch 62, iter = 200, loss = 0.180086679905653, perplexity = 1.1973211423076118, 0:00:09.953695 per 50 iters\n",
      "time = 0:01:38.022808, epoch 62, iter = 250, loss = 0.15112556628882884, perplexity = 1.1631427004249015, 0:00:09.848851 per 50 iters\n",
      "time = 0:01:38.187595, epoch 62, iter = 300, loss = 0.17996870070695878, perplexity = 1.197179891651146, 0:00:09.887069 per 50 iters\n",
      "time = 0:01:38.350234, epoch 62, iter = 350, loss = 0.19127044573426247, perplexity = 1.2107868599476475, 0:00:09.758265 per 50 iters\n",
      "time = 0:01:38.512376, epoch 62, iter = 400, loss = 0.21263039216399193, perplexity = 1.236927388689462, 0:00:09.728373 per 50 iters\n",
      "time = 0:01:38.676350, epoch 62, iter = 450, loss = 0.2545741023123264, perplexity = 1.2899121332735834, 0:00:09.838317 per 50 iters\n",
      "\n",
      "time = 0:01:38.957606, epoch 63, iter = 50, loss = 0.25669118016958237, perplexity = 1.2926458704348938, 0:00:16.874842 per 50 iters\n",
      "time = 0:01:39.123710, epoch 63, iter = 100, loss = 0.1231066084653139, perplexity = 1.1310049892267393, 0:00:09.966109 per 50 iters\n",
      "time = 0:01:39.289705, epoch 63, iter = 150, loss = 0.1430018189549446, perplexity = 1.1537319002504465, 0:00:09.959640 per 50 iters\n",
      "time = 0:01:39.455491, epoch 63, iter = 200, loss = 0.1656158397346735, perplexity = 1.18011965955009, 0:00:09.947023 per 50 iters\n",
      "time = 0:01:39.617088, epoch 63, iter = 250, loss = 0.1954990439862013, perplexity = 1.2159176314875817, 0:00:09.695674 per 50 iters\n",
      "time = 0:01:39.776563, epoch 63, iter = 300, loss = 0.1853950098156929, perplexity = 1.2036938171050735, 0:00:09.568361 per 50 iters\n",
      "time = 0:01:39.941165, epoch 63, iter = 350, loss = 0.1744661270827055, perplexity = 1.1906104122539445, 0:00:09.875883 per 50 iters\n",
      "time = 0:01:40.106358, epoch 63, iter = 400, loss = 0.19600479021668435, perplexity = 1.2165327327757525, 0:00:09.911500 per 50 iters\n",
      "time = 0:01:40.271471, epoch 63, iter = 450, loss = 0.17774860713630913, perplexity = 1.1945249884280291, 0:00:09.906639 per 50 iters\n",
      "\n",
      "time = 0:01:40.549314, epoch 64, iter = 50, loss = 0.3455710850283504, perplexity = 1.4127965162666618, 0:00:16.670464 per 50 iters\n",
      "time = 0:01:40.714774, epoch 64, iter = 100, loss = 0.1173521101474762, perplexity = 1.1245153132419237, 0:00:09.927459 per 50 iters\n",
      "time = 0:01:40.878656, epoch 64, iter = 150, loss = 0.1676736794412136, perplexity = 1.1825506570877309, 0:00:09.832793 per 50 iters\n",
      "time = 0:01:41.043121, epoch 64, iter = 200, loss = 0.13242078691720963, perplexity = 1.1415885837565223, 0:00:09.867376 per 50 iters\n",
      "time = 0:01:41.209407, epoch 64, iter = 250, loss = 0.15422697119414808, perplexity = 1.1667556766566751, 0:00:09.977063 per 50 iters\n",
      "time = 0:01:41.371414, epoch 64, iter = 300, loss = 0.16988608449697495, perplexity = 1.1851698344123949, 0:00:09.720266 per 50 iters\n",
      "time = 0:01:41.534323, epoch 64, iter = 350, loss = 0.20099989518523217, perplexity = 1.2226246436742023, 0:00:09.774412 per 50 iters\n",
      "time = 0:01:41.695719, epoch 64, iter = 400, loss = 0.20694238796830178, perplexity = 1.2299117120270457, 0:00:09.683682 per 50 iters\n",
      "time = 0:01:41.858300, epoch 64, iter = 450, loss = 0.24896752893924712, perplexity = 1.28270038175156, 0:00:09.754687 per 50 iters\n",
      "\n",
      "time = 0:01:42.138567, epoch 65, iter = 50, loss = 0.31283774085342886, perplexity = 1.3672996561521638, 0:00:16.815816 per 50 iters\n",
      "time = 0:01:42.302363, epoch 65, iter = 100, loss = 0.1303235410898924, perplexity = 1.1391969007130072, 0:00:09.827372 per 50 iters\n",
      "time = 0:01:42.466814, epoch 65, iter = 150, loss = 0.1395206244289875, perplexity = 1.1497225178439616, 0:00:09.866959 per 50 iters\n",
      "time = 0:01:42.632298, epoch 65, iter = 200, loss = 0.12893959529697896, perplexity = 1.1376214044071706, 0:00:09.928894 per 50 iters\n",
      "time = 0:01:42.798330, epoch 65, iter = 250, loss = 0.1475544884800911, perplexity = 1.1589964350634328, 0:00:09.961781 per 50 iters\n",
      "time = 0:01:42.962921, epoch 65, iter = 300, loss = 0.1715596956014633, perplexity = 1.1871550085493436, 0:00:09.875329 per 50 iters\n",
      "time = 0:01:43.126290, epoch 65, iter = 350, loss = 0.2168876999616623, perplexity = 1.2422045946809583, 0:00:09.802025 per 50 iters\n",
      "time = 0:01:43.288993, epoch 65, iter = 400, loss = 0.1917819620668888, perplexity = 1.2114063556294121, 0:00:09.762099 per 50 iters\n",
      "time = 0:01:43.451619, epoch 65, iter = 450, loss = 0.2342374475300312, perplexity = 1.263944577097088, 0:00:09.757363 per 50 iters\n",
      "\n",
      "time = 0:01:43.733984, epoch 66, iter = 50, loss = 0.25554862461984157, perplexity = 1.2911697941319413, 0:00:16.941804 per 50 iters\n",
      "time = 0:01:43.896839, epoch 66, iter = 100, loss = 0.11481003932654858, perplexity = 1.121660345978362, 0:00:09.771125 per 50 iters\n",
      "time = 0:01:44.059014, epoch 66, iter = 150, loss = 0.16141117811203004, perplexity = 1.1751680728756801, 0:00:09.730343 per 50 iters\n",
      "time = 0:01:44.224731, epoch 66, iter = 200, loss = 0.1399977993220091, perplexity = 1.15027126747778, 0:00:09.942906 per 50 iters\n",
      "time = 0:01:44.391560, epoch 66, iter = 250, loss = 0.13049995809793472, perplexity = 1.1393978921504349, 0:00:10.009655 per 50 iters\n",
      "time = 0:01:44.552902, epoch 66, iter = 300, loss = 0.1585567493736744, perplexity = 1.1718184222998458, 0:00:09.680371 per 50 iters\n",
      "time = 0:01:44.714927, epoch 66, iter = 350, loss = 0.21817396074533463, perplexity = 1.2438034217702525, 0:00:09.721399 per 50 iters\n",
      "time = 0:01:44.877332, epoch 66, iter = 400, loss = 0.20147906973958016, perplexity = 1.2232106346777463, 0:00:09.744197 per 50 iters\n",
      "time = 0:01:45.039601, epoch 66, iter = 450, loss = 0.227401702105999, perplexity = 1.2553340369970076, 0:00:09.736007 per 50 iters\n",
      "\n",
      "time = 0:01:45.320632, epoch 67, iter = 50, loss = 0.2906333599239588, perplexity = 1.3372741957447283, 0:00:16.861373 per 50 iters\n",
      "time = 0:01:45.486418, epoch 67, iter = 100, loss = 0.11307363495230675, perplexity = 1.1197143800280258, 0:00:09.947003 per 50 iters\n",
      "time = 0:01:45.650198, epoch 67, iter = 150, loss = 0.1584889852255583, perplexity = 1.1717390177131344, 0:00:09.826638 per 50 iters\n",
      "time = 0:01:45.812596, epoch 67, iter = 200, loss = 0.13707023844122887, perplexity = 1.146908702770877, 0:00:09.743790 per 50 iters\n",
      "time = 0:01:45.974349, epoch 67, iter = 250, loss = 0.17033000893890857, perplexity = 1.185696077067079, 0:00:09.705038 per 50 iters\n",
      "time = 0:01:46.140167, epoch 67, iter = 300, loss = 0.15287825107574463, perplexity = 1.1651831105164832, 0:00:09.948931 per 50 iters\n",
      "time = 0:01:46.305272, epoch 67, iter = 350, loss = 0.14305326350033284, perplexity = 1.1537912549902805, 0:00:09.906152 per 50 iters\n",
      "time = 0:01:46.468608, epoch 67, iter = 400, loss = 0.186354621052742, perplexity = 1.2048494496082212, 0:00:09.800002 per 50 iters\n",
      "time = 0:01:46.632091, epoch 67, iter = 450, loss = 0.19772049963474272, perplexity = 1.218621740995847, 0:00:09.808838 per 50 iters\n",
      "\n",
      "time = 0:01:46.912790, epoch 68, iter = 50, loss = 0.2586316161602735, perplexity = 1.2951566021744159, 0:00:16.841832 per 50 iters\n",
      "time = 0:01:47.078206, epoch 68, iter = 100, loss = 0.12271551944315434, perplexity = 1.1305627520741295, 0:00:09.924803 per 50 iters\n",
      "time = 0:01:47.241542, epoch 68, iter = 150, loss = 0.12184181578457355, perplexity = 1.129575406647851, 0:00:09.800089 per 50 iters\n",
      "time = 0:01:47.402422, epoch 68, iter = 200, loss = 0.14938576437532902, perplexity = 1.1611208218731193, 0:00:09.652647 per 50 iters\n",
      "time = 0:01:47.565935, epoch 68, iter = 250, loss = 0.15113187573850154, perplexity = 1.163150039238384, 0:00:09.810638 per 50 iters\n",
      "time = 0:01:47.728917, epoch 68, iter = 300, loss = 0.1601897682948038, perplexity = 1.1737335872803087, 0:00:09.778708 per 50 iters\n",
      "time = 0:01:47.893037, epoch 68, iter = 350, loss = 0.20068149656057357, perplexity = 1.2222354236360333, 0:00:09.847000 per 50 iters\n",
      "time = 0:01:48.057635, epoch 68, iter = 400, loss = 0.16736646264791488, perplexity = 1.1821874134670742, 0:00:09.875802 per 50 iters\n",
      "time = 0:01:48.221242, epoch 68, iter = 450, loss = 0.21248649254441263, perplexity = 1.236749408114758, 0:00:09.816302 per 50 iters\n",
      "\n",
      "time = 0:01:48.496153, epoch 69, iter = 50, loss = 0.2520215015113354, perplexity = 1.2866237013415287, 0:00:16.494075 per 50 iters\n",
      "time = 0:01:48.661612, epoch 69, iter = 100, loss = 0.12966425195336342, perplexity = 1.138446088100495, 0:00:09.927394 per 50 iters\n",
      "time = 0:01:48.829229, epoch 69, iter = 150, loss = 0.11850735977292061, perplexity = 1.1258151598154218, 0:00:10.056911 per 50 iters\n",
      "time = 0:01:48.993716, epoch 69, iter = 200, loss = 0.1240533921867609, perplexity = 1.1320763134155378, 0:00:09.868984 per 50 iters\n",
      "time = 0:01:49.155396, epoch 69, iter = 250, loss = 0.12626566937193273, perplexity = 1.134583552346752, 0:00:09.700622 per 50 iters\n",
      "time = 0:01:49.320674, epoch 69, iter = 300, loss = 0.15054229766130447, perplexity = 1.1624644735918546, 0:00:09.916549 per 50 iters\n",
      "time = 0:01:49.486131, epoch 69, iter = 350, loss = 0.18384397484362125, perplexity = 1.2018282930196182, 0:00:09.927288 per 50 iters\n",
      "time = 0:01:49.648653, epoch 69, iter = 400, loss = 0.1728256916254759, perplexity = 1.1886588938255818, 0:00:09.751187 per 50 iters\n",
      "time = 0:01:49.812390, epoch 69, iter = 450, loss = 0.20301101468503474, perplexity = 1.2250859621091967, 0:00:09.824104 per 50 iters\n",
      "\n",
      "time = 0:01:50.089709, epoch 70, iter = 50, loss = 0.26405560739338396, perplexity = 1.3022006062689884, 0:00:16.639046 per 50 iters\n",
      "time = 0:01:50.259557, epoch 70, iter = 100, loss = 0.09130693078041077, perplexity = 1.0956052286250197, 0:00:10.190754 per 50 iters\n",
      "time = 0:01:50.423904, epoch 70, iter = 150, loss = 0.13892522677779198, perplexity = 1.1490381795042472, 0:00:09.860687 per 50 iters\n",
      "time = 0:01:50.589291, epoch 70, iter = 200, loss = 0.1134018548578024, perplexity = 1.1200819528950867, 0:00:09.923105 per 50 iters\n",
      "time = 0:01:50.751198, epoch 70, iter = 250, loss = 0.16134372524917126, perplexity = 1.1750888070982064, 0:00:09.714317 per 50 iters\n",
      "time = 0:01:50.913839, epoch 70, iter = 300, loss = 0.16803145639598369, perplexity = 1.182973822155526, 0:00:09.758326 per 50 iters\n",
      "time = 0:01:51.077746, epoch 70, iter = 350, loss = 0.1713174633681774, perplexity = 1.1868674761665776, 0:00:09.833929 per 50 iters\n",
      "time = 0:01:51.241227, epoch 70, iter = 400, loss = 0.17217906087636947, perplexity = 1.1878905188888391, 0:00:09.808737 per 50 iters\n",
      "time = 0:01:51.403337, epoch 70, iter = 450, loss = 0.20396282002329827, perplexity = 1.2262525605670123, 0:00:09.726531 per 50 iters\n",
      "\n",
      "time = 0:01:51.684563, epoch 71, iter = 50, loss = 0.23255821757018566, perplexity = 1.2618239045424118, 0:00:16.873404 per 50 iters\n",
      "time = 0:01:51.849604, epoch 71, iter = 100, loss = 0.11471657671034335, perplexity = 1.1215555175667784, 0:00:09.902315 per 50 iters\n",
      "time = 0:01:52.013011, epoch 71, iter = 150, loss = 0.12866177096962927, perplexity = 1.1373053894060727, 0:00:09.804280 per 50 iters\n",
      "time = 0:01:52.176578, epoch 71, iter = 200, loss = 0.11707812860608101, perplexity = 1.1242072590055934, 0:00:09.813928 per 50 iters\n",
      "time = 0:01:52.339233, epoch 71, iter = 250, loss = 0.1468124894052744, perplexity = 1.1581367797520639, 0:00:09.759182 per 50 iters\n",
      "time = 0:01:52.500461, epoch 71, iter = 300, loss = 0.1585363108664751, perplexity = 1.171794472325337, 0:00:09.673581 per 50 iters\n",
      "time = 0:01:52.665767, epoch 71, iter = 350, loss = 0.14887256376445293, perplexity = 1.1605250868369006, 0:00:09.918227 per 50 iters\n",
      "time = 0:01:52.828060, epoch 71, iter = 400, loss = 0.17580994635820388, perplexity = 1.1922114529893042, 0:00:09.737425 per 50 iters\n",
      "time = 0:01:52.991769, epoch 71, iter = 450, loss = 0.19964991986751557, perplexity = 1.220975244157222, 0:00:09.822469 per 50 iters\n",
      "\n",
      "time = 0:01:53.268071, epoch 72, iter = 50, loss = 0.275232555270195, perplexity = 1.3168368766174707, 0:00:16.577971 per 50 iters\n",
      "time = 0:01:53.431773, epoch 72, iter = 100, loss = 0.09571072548627853, perplexity = 1.1004406885021956, 0:00:09.821974 per 50 iters\n",
      "time = 0:01:53.595077, epoch 72, iter = 150, loss = 0.1173219422250986, perplexity = 1.1244813894629488, 0:00:09.798106 per 50 iters\n",
      "time = 0:01:53.758246, epoch 72, iter = 200, loss = 0.14423812977969647, perplexity = 1.15515915357016, 0:00:09.789605 per 50 iters\n",
      "time = 0:01:53.922144, epoch 72, iter = 250, loss = 0.1470414312928915, perplexity = 1.1584019561263708, 0:00:09.833750 per 50 iters\n",
      "time = 0:01:54.083370, epoch 72, iter = 300, loss = 0.14425565868616105, perplexity = 1.1551794024243842, 0:00:09.673455 per 50 iters\n",
      "time = 0:01:54.248223, epoch 72, iter = 350, loss = 0.16266936399042606, perplexity = 1.1766475833041021, 0:00:09.891038 per 50 iters\n",
      "time = 0:01:54.413366, epoch 72, iter = 400, loss = 0.16807485580444337, perplexity = 1.1830251636337175, 0:00:09.908474 per 50 iters\n",
      "time = 0:01:54.577997, epoch 72, iter = 450, loss = 0.16929110676050185, perplexity = 1.1844648944795584, 0:00:09.877760 per 50 iters\n",
      "\n",
      "time = 0:01:54.859750, epoch 73, iter = 50, loss = 0.22398270413279533, perplexity = 1.2510493812572727, 0:00:16.905078 per 50 iters\n",
      "time = 0:01:55.024582, epoch 73, iter = 100, loss = 0.09960324443876743, perplexity = 1.1047325223415123, 0:00:09.889792 per 50 iters\n",
      "time = 0:01:55.187864, epoch 73, iter = 150, loss = 0.11459007471799851, perplexity = 1.1214136475328909, 0:00:09.796782 per 50 iters\n",
      "time = 0:01:55.352530, epoch 73, iter = 200, loss = 0.1309367284923792, perplexity = 1.1398956561135867, 0:00:09.879870 per 50 iters\n",
      "time = 0:01:55.518964, epoch 73, iter = 250, loss = 0.1266397724300623, perplexity = 1.1350080829275315, 0:00:09.985876 per 50 iters\n",
      "time = 0:01:55.680608, epoch 73, iter = 300, loss = 0.1457142547518015, perplexity = 1.1568655719770629, 0:00:09.698547 per 50 iters\n",
      "time = 0:01:55.845121, epoch 73, iter = 350, loss = 0.1603755810111761, perplexity = 1.1739517021700914, 0:00:09.870612 per 50 iters\n",
      "time = 0:01:56.008289, epoch 73, iter = 400, loss = 0.20926689013838767, perplexity = 1.232773969844115, 0:00:09.789969 per 50 iters\n",
      "time = 0:01:56.170627, epoch 73, iter = 450, loss = 0.19907930240035057, perplexity = 1.2202787330955176, 0:00:09.740125 per 50 iters\n",
      "\n",
      "time = 0:01:56.447338, epoch 74, iter = 50, loss = 0.2693933115899563, perplexity = 1.3091699515145214, 0:00:16.602560 per 50 iters\n",
      "time = 0:01:56.611103, epoch 74, iter = 100, loss = 0.10011528123170138, perplexity = 1.1052983308843356, 0:00:09.825746 per 50 iters\n",
      "time = 0:01:56.775017, epoch 74, iter = 150, loss = 0.12054642617702484, perplexity = 1.1281131137291434, 0:00:09.834689 per 50 iters\n",
      "time = 0:01:56.939845, epoch 74, iter = 200, loss = 0.10732803769409656, perplexity = 1.1132993987531146, 0:00:09.889545 per 50 iters\n",
      "time = 0:01:57.103331, epoch 74, iter = 250, loss = 0.13320938177406788, perplexity = 1.1424891897022569, 0:00:09.808999 per 50 iters\n",
      "time = 0:01:57.265258, epoch 74, iter = 300, loss = 0.15955393575131893, perplexity = 1.1729875264781384, 0:00:09.715552 per 50 iters\n",
      "time = 0:01:57.427571, epoch 74, iter = 350, loss = 0.1775011622905731, perplexity = 1.19422944594328, 0:00:09.738638 per 50 iters\n",
      "time = 0:01:57.592201, epoch 74, iter = 400, loss = 0.1779009575396776, perplexity = 1.1947069886554018, 0:00:09.877633 per 50 iters\n",
      "time = 0:01:57.756670, epoch 74, iter = 450, loss = 0.21151483856141567, perplexity = 1.235548299251998, 0:00:09.868028 per 50 iters\n",
      "\n",
      "time = 0:01:58.040906, epoch 75, iter = 50, loss = 0.22421559691429138, perplexity = 1.2513407755579602, 0:00:17.054002 per 50 iters\n",
      "time = 0:01:58.204738, epoch 75, iter = 100, loss = 0.1076047146692872, perplexity = 1.1136074656787904, 0:00:09.829821 per 50 iters\n",
      "time = 0:01:58.368330, epoch 75, iter = 150, loss = 0.11312901124358177, perplexity = 1.1197763873745317, 0:00:09.815417 per 50 iters\n",
      "time = 0:01:58.529351, epoch 75, iter = 200, loss = 0.129298460399732, perplexity = 1.1380297302919304, 0:00:09.661117 per 50 iters\n",
      "time = 0:01:58.691106, epoch 75, iter = 250, loss = 0.1568164598941803, perplexity = 1.1697808924877278, 0:00:09.705158 per 50 iters\n",
      "time = 0:01:58.855071, epoch 75, iter = 300, loss = 0.15311240255832673, perplexity = 1.165455971813482, 0:00:09.837804 per 50 iters\n",
      "time = 0:01:59.021633, epoch 75, iter = 350, loss = 0.1463846293091774, perplexity = 1.157641365229796, 0:00:09.993605 per 50 iters\n",
      "time = 0:01:59.185277, epoch 75, iter = 400, loss = 0.16345702692866326, perplexity = 1.1775747500963616, 0:00:09.818470 per 50 iters\n",
      "time = 0:01:59.349715, epoch 75, iter = 450, loss = 0.15668119221925736, perplexity = 1.1696226696476908, 0:00:09.866173 per 50 iters\n",
      "\n",
      "time = 0:01:59.627293, epoch 76, iter = 50, loss = 0.2789752898365259, perplexity = 1.3217746821975302, 0:00:16.654539 per 50 iters\n",
      "time = 0:01:59.790374, epoch 76, iter = 100, loss = 0.10984868071973324, perplexity = 1.1161091688440297, 0:00:09.784732 per 50 iters\n",
      "time = 0:01:59.954216, epoch 76, iter = 150, loss = 0.10452961027622223, perplexity = 1.110188266360758, 0:00:09.830320 per 50 iters\n",
      "time = 0:02:00.119417, epoch 76, iter = 200, loss = 0.12389416277408599, perplexity = 1.1318960679196226, 0:00:09.911992 per 50 iters\n",
      "time = 0:02:00.282052, epoch 76, iter = 250, loss = 0.13184647545218467, perplexity = 1.140933144575663, 0:00:09.757894 per 50 iters\n",
      "time = 0:02:00.446413, epoch 76, iter = 300, loss = 0.13781099885702133, perplexity = 1.147758602085439, 0:00:09.861549 per 50 iters\n",
      "time = 0:02:00.612734, epoch 76, iter = 350, loss = 0.14965027555823326, perplexity = 1.1614279919383754, 0:00:09.979071 per 50 iters\n",
      "time = 0:02:00.775916, epoch 76, iter = 400, loss = 0.17022640119306742, perplexity = 1.1855732361330276, 0:00:09.790742 per 50 iters\n",
      "time = 0:02:00.938425, epoch 76, iter = 450, loss = 0.1688271151483059, perplexity = 1.1839154401845289, 0:00:09.750392 per 50 iters\n",
      "\n",
      "time = 0:02:01.218084, epoch 77, iter = 50, loss = 0.2474518295377493, perplexity = 1.2807576662089548, 0:00:16.779453 per 50 iters\n",
      "time = 0:02:01.380744, epoch 77, iter = 100, loss = 0.10259750931523741, perplexity = 1.108045341384782, 0:00:09.759463 per 50 iters\n",
      "time = 0:02:01.542517, epoch 77, iter = 150, loss = 0.12724814109504223, perplexity = 1.1356987963625853, 0:00:09.706255 per 50 iters\n",
      "time = 0:02:01.705292, epoch 77, iter = 200, loss = 0.11036649830639363, perplexity = 1.1166872594601165, 0:00:09.766342 per 50 iters\n",
      "time = 0:02:01.870040, epoch 77, iter = 250, loss = 0.14007100448012352, perplexity = 1.1503554763500146, 0:00:09.884679 per 50 iters\n",
      "time = 0:02:02.038877, epoch 77, iter = 300, loss = 0.12213529139757157, perplexity = 1.1299069581315022, 0:00:10.130059 per 50 iters\n",
      "time = 0:02:02.203670, epoch 77, iter = 350, loss = 0.153506236448884, perplexity = 1.1659150582691031, 0:00:09.887428 per 50 iters\n",
      "time = 0:02:02.366803, epoch 77, iter = 400, loss = 0.175171954408288, perplexity = 1.1914510742632443, 0:00:09.787856 per 50 iters\n",
      "time = 0:02:02.531013, epoch 77, iter = 450, loss = 0.18585557267069816, perplexity = 1.2042483214482815, 0:00:09.852499 per 50 iters\n",
      "\n",
      "time = 0:02:02.807448, epoch 78, iter = 50, loss = 0.24419053815305233, perplexity = 1.2765875459510216, 0:00:16.585538 per 50 iters\n",
      "time = 0:02:02.972120, epoch 78, iter = 100, loss = 0.10890785917639732, perplexity = 1.1150596030977171, 0:00:09.880196 per 50 iters\n",
      "time = 0:02:03.135308, epoch 78, iter = 150, loss = 0.11961075454950333, perplexity = 1.127058063963031, 0:00:09.791152 per 50 iters\n",
      "time = 0:02:03.300386, epoch 78, iter = 200, loss = 0.12970517612993718, perplexity = 1.1384926790225651, 0:00:09.904581 per 50 iters\n",
      "time = 0:02:03.464379, epoch 78, iter = 250, loss = 0.11271212348714471, perplexity = 1.1193096636011608, 0:00:09.839398 per 50 iters\n",
      "time = 0:02:03.628135, epoch 78, iter = 300, loss = 0.14476175047457218, perplexity = 1.1557641771964664, 0:00:09.825245 per 50 iters\n",
      "time = 0:02:03.789398, epoch 78, iter = 350, loss = 0.14393433831632138, perplexity = 1.154808279379456, 0:00:09.675666 per 50 iters\n",
      "time = 0:02:03.949486, epoch 78, iter = 400, loss = 0.17170570574700833, perplexity = 1.187328357879999, 0:00:09.605186 per 50 iters\n",
      "time = 0:02:04.114381, epoch 78, iter = 450, loss = 0.19867245212197304, perplexity = 1.2197823633341942, 0:00:09.893573 per 50 iters\n",
      "\n",
      "time = 0:02:04.398935, epoch 79, iter = 50, loss = 0.25115680117160083, perplexity = 1.285511638259822, 0:00:17.073069 per 50 iters\n",
      "time = 0:02:04.564470, epoch 79, iter = 100, loss = 0.0974853352457285, perplexity = 1.1023952750889685, 0:00:09.931980 per 50 iters\n",
      "time = 0:02:04.726212, epoch 79, iter = 150, loss = 0.11922279149293899, perplexity = 1.1266208918804024, 0:00:09.704407 per 50 iters\n",
      "time = 0:02:04.889914, epoch 79, iter = 200, loss = 0.13223339132964612, perplexity = 1.1413746751365053, 0:00:09.822043 per 50 iters\n",
      "time = 0:02:05.054210, epoch 79, iter = 250, loss = 0.13702334977686406, perplexity = 1.1468549270144024, 0:00:09.857593 per 50 iters\n",
      "time = 0:02:05.214184, epoch 79, iter = 300, loss = 0.1571279189735651, perplexity = 1.170145288111801, 0:00:09.598321 per 50 iters\n",
      "time = 0:02:05.381752, epoch 79, iter = 350, loss = 0.12955980092287064, perplexity = 1.1383271824434495, 0:00:10.053985 per 50 iters\n",
      "time = 0:02:05.546081, epoch 79, iter = 400, loss = 0.1583574814349413, perplexity = 1.1715849397218332, 0:00:09.859604 per 50 iters\n",
      "time = 0:02:05.707357, epoch 79, iter = 450, loss = 0.17416185228154063, perplexity = 1.1902481946171355, 0:00:09.676439 per 50 iters\n",
      "\n",
      "time = 0:02:05.985894, epoch 80, iter = 50, loss = 0.24284482445567845, perplexity = 1.274870780001346, 0:00:16.712089 per 50 iters\n",
      "time = 0:02:06.150357, epoch 80, iter = 100, loss = 0.10141521118581295, perplexity = 1.1067360755741154, 0:00:09.867690 per 50 iters\n",
      "time = 0:02:06.310827, epoch 80, iter = 150, loss = 0.15444924123585224, perplexity = 1.1670150403129018, 0:00:09.628053 per 50 iters\n",
      "time = 0:02:06.476814, epoch 80, iter = 200, loss = 0.10725170381367206, perplexity = 1.1132144195333715, 0:00:09.959112 per 50 iters\n",
      "time = 0:02:06.642035, epoch 80, iter = 250, loss = 0.11600008457899094, perplexity = 1.1229959671151164, 0:00:09.913107 per 50 iters\n",
      "time = 0:02:06.803157, epoch 80, iter = 300, loss = 0.13683912560343742, perplexity = 1.1466436680735321, 0:00:09.667197 per 50 iters\n",
      "time = 0:02:06.965896, epoch 80, iter = 350, loss = 0.17044392563402652, perplexity = 1.185831155339282, 0:00:09.764202 per 50 iters\n",
      "time = 0:02:07.132265, epoch 80, iter = 400, loss = 0.15199005000293256, perplexity = 1.164148653098801, 0:00:09.982022 per 50 iters\n",
      "time = 0:02:07.293579, epoch 80, iter = 450, loss = 0.18086428775452076, perplexity = 1.1982525507137958, 0:00:09.678728 per 50 iters\n",
      "\n",
      "time = 0:02:07.575026, epoch 81, iter = 50, loss = 0.19845662757754326, perplexity = 1.2195191327681543, 0:00:16.886706 per 50 iters\n",
      "time = 0:02:07.737251, epoch 81, iter = 100, loss = 0.10975812166929245, perplexity = 1.1160080996339456, 0:00:09.733409 per 50 iters\n",
      "time = 0:02:07.899096, epoch 81, iter = 150, loss = 0.10758201848715544, perplexity = 1.1135821913277428, 0:00:09.710515 per 50 iters\n",
      "time = 0:02:08.065661, epoch 81, iter = 200, loss = 0.11238492771983147, perplexity = 1.1189434901253903, 0:00:09.993795 per 50 iters\n",
      "time = 0:02:08.227659, epoch 81, iter = 250, loss = 0.13767180494964124, perplexity = 1.1475988521992488, 0:00:09.719684 per 50 iters\n",
      "time = 0:02:08.385352, epoch 81, iter = 300, loss = 0.14379359760321678, perplexity = 1.154645762275372, 0:00:09.461501 per 50 iters\n",
      "time = 0:02:08.551140, epoch 81, iter = 350, loss = 0.17638065852224827, perplexity = 1.1928920567635914, 0:00:09.947157 per 50 iters\n",
      "time = 0:02:08.716513, epoch 81, iter = 400, loss = 0.1618978250771761, perplexity = 1.1757401040292048, 0:00:09.922254 per 50 iters\n",
      "time = 0:02:08.884439, epoch 81, iter = 450, loss = 0.1519989498704672, perplexity = 1.164159013913709, 0:00:10.075434 per 50 iters\n",
      "\n",
      "time = 0:02:09.159127, epoch 82, iter = 50, loss = 0.2642586794495583, perplexity = 1.302465073675716, 0:00:16.481215 per 50 iters\n",
      "time = 0:02:09.322764, epoch 82, iter = 100, loss = 0.11274350896477699, perplexity = 1.1193447942208643, 0:00:09.818101 per 50 iters\n",
      "time = 0:02:09.486956, epoch 82, iter = 150, loss = 0.091690095779486, perplexity = 1.0960251066375748, 0:00:09.851378 per 50 iters\n",
      "time = 0:02:09.650879, epoch 82, iter = 200, loss = 0.1273042807728052, perplexity = 1.1357625559167526, 0:00:09.835248 per 50 iters\n",
      "time = 0:02:09.815676, epoch 82, iter = 250, loss = 0.11427543804049492, perplexity = 1.1210608651707694, 0:00:09.887656 per 50 iters\n",
      "time = 0:02:09.980008, epoch 82, iter = 300, loss = 0.1401184967905283, perplexity = 1.1504101106867184, 0:00:09.859817 per 50 iters\n",
      "time = 0:02:10.144650, epoch 82, iter = 350, loss = 0.13299222029745578, perplexity = 1.1422411120002427, 0:00:09.878324 per 50 iters\n",
      "time = 0:02:10.311854, epoch 82, iter = 400, loss = 0.16510419584810734, perplexity = 1.1795160129800717, 0:00:10.032120 per 50 iters\n",
      "time = 0:02:10.473413, epoch 82, iter = 450, loss = 0.1932680992782116, perplexity = 1.2132080101138365, 0:00:09.693443 per 50 iters\n",
      "\n",
      "time = 0:02:10.747133, epoch 83, iter = 50, loss = 0.2461393667384982, perplexity = 1.2790778220248358, 0:00:16.423103 per 50 iters\n",
      "time = 0:02:10.910043, epoch 83, iter = 100, loss = 0.09896575920283794, perplexity = 1.1040284960958793, 0:00:09.774388 per 50 iters\n",
      "time = 0:02:11.074144, epoch 83, iter = 150, loss = 0.1099776316806674, perplexity = 1.1162531014737855, 0:00:09.845937 per 50 iters\n",
      "time = 0:02:11.237336, epoch 83, iter = 200, loss = 0.1288758670538664, perplexity = 1.137548908103796, 0:00:09.791418 per 50 iters\n",
      "time = 0:02:11.404507, epoch 83, iter = 250, loss = 0.10924498580396175, perplexity = 1.1154355827540587, 0:00:10.030159 per 50 iters\n",
      "time = 0:02:11.570909, epoch 83, iter = 300, loss = 0.12626206211745739, perplexity = 1.134579459622537, 0:00:09.984006 per 50 iters\n",
      "time = 0:02:11.732754, epoch 83, iter = 350, loss = 0.18142609760165215, perplexity = 1.1989259299339639, 0:00:09.710551 per 50 iters\n",
      "time = 0:02:11.894906, epoch 83, iter = 400, loss = 0.16406082540750502, perplexity = 1.1782859826381478, 0:00:09.729008 per 50 iters\n",
      "time = 0:02:12.059403, epoch 83, iter = 450, loss = 0.17462579580722376, perplexity = 1.1908005306774465, 0:00:09.869726 per 50 iters\n",
      "\n",
      "time = 0:02:12.341246, epoch 84, iter = 50, loss = 0.24472744666039944, perplexity = 1.2772731406991726, 0:00:16.910422 per 50 iters\n",
      "time = 0:02:12.500070, epoch 84, iter = 100, loss = 0.10991596774198115, perplexity = 1.1161842710331777, 0:00:09.529330 per 50 iters\n",
      "time = 0:02:12.662779, epoch 84, iter = 150, loss = 0.11798663139343261, perplexity = 1.1252290685220763, 0:00:09.762409 per 50 iters\n",
      "time = 0:02:12.826673, epoch 84, iter = 200, loss = 0.13141887076199055, perplexity = 1.140445380504379, 0:00:09.833560 per 50 iters\n",
      "time = 0:02:12.993430, epoch 84, iter = 250, loss = 0.13354110419750215, perplexity = 1.1428682418515852, 0:00:10.005279 per 50 iters\n",
      "time = 0:02:13.157808, epoch 84, iter = 300, loss = 0.11893186666071415, perplexity = 1.1262931775588796, 0:00:09.862559 per 50 iters\n",
      "time = 0:02:13.321242, epoch 84, iter = 350, loss = 0.16172388561069964, perplexity = 1.1755356142077358, 0:00:09.805911 per 50 iters\n",
      "time = 0:02:13.482302, epoch 84, iter = 400, loss = 0.1527865954488516, perplexity = 1.1650763198221024, 0:00:09.663498 per 50 iters\n",
      "time = 0:02:13.646984, epoch 84, iter = 450, loss = 0.18025129929184913, perplexity = 1.1975182608034558, 0:00:09.880779 per 50 iters\n",
      "\n",
      "time = 0:02:13.927017, epoch 85, iter = 50, loss = 0.22171145472675563, perplexity = 1.2482111604669055, 0:00:16.801869 per 50 iters\n",
      "time = 0:02:14.091719, epoch 85, iter = 100, loss = 0.08992759011685848, perplexity = 1.094095057541604, 0:00:09.881677 per 50 iters\n",
      "time = 0:02:14.255782, epoch 85, iter = 150, loss = 0.10861016862094403, perplexity = 1.1147277097883104, 0:00:09.843665 per 50 iters\n",
      "time = 0:02:14.416655, epoch 85, iter = 200, loss = 0.13235292389988898, perplexity = 1.1415111147393602, 0:00:09.652189 per 50 iters\n",
      "time = 0:02:14.581746, epoch 85, iter = 250, loss = 0.11638144575059414, perplexity = 1.1234243158454364, 0:00:09.905342 per 50 iters\n",
      "time = 0:02:14.746951, epoch 85, iter = 300, loss = 0.15623665913939477, perplexity = 1.1691028492273217, 0:00:09.912225 per 50 iters\n",
      "time = 0:02:14.910102, epoch 85, iter = 350, loss = 0.15168116368353368, perplexity = 1.1637891190365472, 0:00:09.788862 per 50 iters\n",
      "time = 0:02:15.073288, epoch 85, iter = 400, loss = 0.15773490764200687, perplexity = 1.1708557686471186, 0:00:09.791059 per 50 iters\n",
      "time = 0:02:15.240006, epoch 85, iter = 450, loss = 0.16286534070968628, perplexity = 1.1768782014343577, 0:00:10.002954 per 50 iters\n",
      "\n",
      "time = 0:02:15.522164, epoch 86, iter = 50, loss = 0.25019380774348976, perplexity = 1.2842742948728139, 0:00:16.929364 per 50 iters\n",
      "time = 0:02:15.685208, epoch 86, iter = 100, loss = 0.1178581390529871, perplexity = 1.1250844944940512, 0:00:09.782500 per 50 iters\n",
      "time = 0:02:15.848545, epoch 86, iter = 150, loss = 0.10291405998170376, perplexity = 1.108396149397398, 0:00:09.800075 per 50 iters\n",
      "time = 0:02:16.014091, epoch 86, iter = 200, loss = 0.1068943214416504, perplexity = 1.1128166474061567, 0:00:09.932607 per 50 iters\n",
      "time = 0:02:16.176601, epoch 86, iter = 250, loss = 0.12869676046073436, perplexity = 1.1373451838390687, 0:00:09.750432 per 50 iters\n",
      "time = 0:02:16.338377, epoch 86, iter = 300, loss = 0.15851346120238305, perplexity = 1.1717676975211582, 0:00:09.706387 per 50 iters\n",
      "time = 0:02:16.502536, epoch 86, iter = 350, loss = 0.16391310691833497, perplexity = 1.1781119408678922, 0:00:09.849375 per 50 iters\n",
      "time = 0:02:16.663816, epoch 86, iter = 400, loss = 0.18717302642762662, perplexity = 1.205835908480269, 0:00:09.676656 per 50 iters\n",
      "time = 0:02:16.831617, epoch 86, iter = 450, loss = 0.14414759851992132, perplexity = 1.1550545802903933, 0:00:10.067920 per 50 iters\n",
      "\n",
      "time = 0:02:17.110292, epoch 87, iter = 50, loss = 0.22099405132234096, perplexity = 1.2473160106611092, 0:00:16.720033 per 50 iters\n",
      "time = 0:02:17.276391, epoch 87, iter = 100, loss = 0.1026022033393383, perplexity = 1.1080505425885265, 0:00:09.965781 per 50 iters\n",
      "time = 0:02:17.442732, epoch 87, iter = 150, loss = 0.0955928386747837, perplexity = 1.1003109687044657, 0:00:09.980323 per 50 iters\n",
      "time = 0:02:17.604562, epoch 87, iter = 200, loss = 0.12929044514894486, perplexity = 1.1380206087347946, 0:00:09.709649 per 50 iters\n",
      "time = 0:02:17.768788, epoch 87, iter = 250, loss = 0.12790529787540436, perplexity = 1.136445373809412, 0:00:09.853460 per 50 iters\n",
      "time = 0:02:17.936599, epoch 87, iter = 300, loss = 0.13456023782491683, perplexity = 1.1440335710207696, 0:00:10.068510 per 50 iters\n",
      "time = 0:02:18.096808, epoch 87, iter = 350, loss = 0.172534955739975, perplexity = 1.1883133582617547, 0:00:09.612456 per 50 iters\n",
      "time = 0:02:18.260123, epoch 87, iter = 400, loss = 0.15848521806299687, perplexity = 1.1717346035900893, 0:00:09.798770 per 50 iters\n",
      "time = 0:02:18.423738, epoch 87, iter = 450, loss = 0.18075925223529338, perplexity = 1.1981266982445715, 0:00:09.816809 per 50 iters\n",
      "\n",
      "time = 0:02:18.703954, epoch 88, iter = 50, loss = 0.24182350903749467, perplexity = 1.2735693994906496, 0:00:16.812831 per 50 iters\n",
      "time = 0:02:18.867938, epoch 88, iter = 100, loss = 0.07965472668409347, perplexity = 1.0829131021209024, 0:00:09.838896 per 50 iters\n",
      "time = 0:02:19.036447, epoch 88, iter = 150, loss = 0.08684907626360655, perplexity = 1.0907320499389386, 0:00:10.110386 per 50 iters\n",
      "time = 0:02:19.196716, epoch 88, iter = 200, loss = 0.11787692554295064, perplexity = 1.1251056310811556, 0:00:09.616037 per 50 iters\n",
      "time = 0:02:19.360329, epoch 88, iter = 250, loss = 0.14593514658510684, perplexity = 1.1571211423598107, 0:00:09.816602 per 50 iters\n",
      "time = 0:02:19.529542, epoch 88, iter = 300, loss = 0.13091078341007234, perplexity = 1.139866081810623, 0:00:10.152674 per 50 iters\n",
      "time = 0:02:19.691863, epoch 88, iter = 350, loss = 0.16572021260857583, perplexity = 1.1802428384586625, 0:00:09.739088 per 50 iters\n",
      "time = 0:02:19.855734, epoch 88, iter = 400, loss = 0.17925465628504753, perplexity = 1.1963253571515924, 0:00:09.832154 per 50 iters\n",
      "time = 0:02:20.022170, epoch 88, iter = 450, loss = 0.1631033618748188, perplexity = 1.1771583566952004, 0:00:09.985985 per 50 iters\n",
      "\n",
      "time = 0:02:20.307185, epoch 89, iter = 50, loss = 0.2173976808041334, perplexity = 1.24283825679071, 0:00:17.100782 per 50 iters\n",
      "time = 0:02:20.469696, epoch 89, iter = 100, loss = 0.09646047547459602, perplexity = 1.1012660532654917, 0:00:09.750541 per 50 iters\n",
      "time = 0:02:20.633916, epoch 89, iter = 150, loss = 0.10309701703488827, perplexity = 1.1085989568426133, 0:00:09.853090 per 50 iters\n",
      "time = 0:02:20.799725, epoch 89, iter = 200, loss = 0.10819672670215369, perplexity = 1.114266929884474, 0:00:09.948365 per 50 iters\n",
      "time = 0:02:20.962039, epoch 89, iter = 250, loss = 0.10618737608194351, perplexity = 1.1120302248527085, 0:00:09.738706 per 50 iters\n",
      "time = 0:02:21.126298, epoch 89, iter = 300, loss = 0.1524711464345455, perplexity = 1.1647088556065825, 0:00:09.855426 per 50 iters\n",
      "time = 0:02:21.292280, epoch 89, iter = 350, loss = 0.16984655477106572, perplexity = 1.185122985899645, 0:00:09.958773 per 50 iters\n",
      "time = 0:02:21.458932, epoch 89, iter = 400, loss = 0.1742980556935072, perplexity = 1.1904103215231976, 0:00:09.998996 per 50 iters\n",
      "time = 0:02:21.621977, epoch 89, iter = 450, loss = 0.18810842111706733, perplexity = 1.206964368680958, 0:00:09.782513 per 50 iters\n",
      "\n",
      "time = 0:02:21.905169, epoch 90, iter = 50, loss = 0.24797617822885512, perplexity = 1.2814294059123816, 0:00:16.991419 per 50 iters\n",
      "time = 0:02:22.067875, epoch 90, iter = 100, loss = 0.10748502664268017, perplexity = 1.1134741881748282, 0:00:09.762210 per 50 iters\n",
      "time = 0:02:22.233716, epoch 90, iter = 150, loss = 0.11010242819786072, perplexity = 1.1163924146658755, 0:00:09.950289 per 50 iters\n",
      "time = 0:02:22.400463, epoch 90, iter = 200, loss = 0.12134824976557866, perplexity = 1.1290180241750767, 0:00:10.004678 per 50 iters\n",
      "time = 0:02:22.564191, epoch 90, iter = 250, loss = 0.11841036699712276, perplexity = 1.1257059691734732, 0:00:09.823553 per 50 iters\n",
      "time = 0:02:22.727303, epoch 90, iter = 300, loss = 0.14400957375764847, perplexity = 1.1548951651584074, 0:00:09.786621 per 50 iters\n",
      "time = 0:02:22.890874, epoch 90, iter = 350, loss = 0.13724660851061343, perplexity = 1.1471110009775125, 0:00:09.814112 per 50 iters\n",
      "time = 0:02:23.053442, epoch 90, iter = 400, loss = 0.16948931381106377, perplexity = 1.184699687040789, 0:00:09.753934 per 50 iters\n",
      "time = 0:02:23.215493, epoch 90, iter = 450, loss = 0.16612985089421273, perplexity = 1.1807264101496209, 0:00:09.722970 per 50 iters\n",
      "\n",
      "time = 0:02:23.495872, epoch 91, iter = 50, loss = 0.22752798855537548, perplexity = 1.2554925786859443, 0:00:16.822634 per 50 iters\n",
      "time = 0:02:23.661971, epoch 91, iter = 100, loss = 0.0928558349609375, perplexity = 1.0973035310581796, 0:00:09.965750 per 50 iters\n",
      "time = 0:02:23.825258, epoch 91, iter = 150, loss = 0.10485536739230156, perplexity = 1.1105499770004217, 0:00:09.797091 per 50 iters\n",
      "time = 0:02:23.991516, epoch 91, iter = 200, loss = 0.10343811944127083, perplexity = 1.1089771671150555, 0:00:09.975362 per 50 iters\n",
      "time = 0:02:24.155495, epoch 91, iter = 250, loss = 0.12552388302981854, perplexity = 1.1337422458370503, 0:00:09.838605 per 50 iters\n",
      "time = 0:02:24.321619, epoch 91, iter = 300, loss = 0.12242702767252922, perplexity = 1.1302366410664137, 0:00:09.967306 per 50 iters\n",
      "time = 0:02:24.485857, epoch 91, iter = 350, loss = 0.16670289635658264, perplexity = 1.1814032139624138, 0:00:09.854119 per 50 iters\n",
      "time = 0:02:24.652007, epoch 91, iter = 400, loss = 0.17349344730377197, perplexity = 1.1894528926205892, 0:00:09.968890 per 50 iters\n",
      "time = 0:02:24.816860, epoch 91, iter = 450, loss = 0.18618336029350757, perplexity = 1.204643123844976, 0:00:09.891043 per 50 iters\n",
      "\n",
      "time = 0:02:25.097903, epoch 92, iter = 50, loss = 0.2157892144471407, perplexity = 1.240840800119052, 0:00:16.862445 per 50 iters\n",
      "time = 0:02:25.260369, epoch 92, iter = 100, loss = 0.09618386473506689, perplexity = 1.1009614733750561, 0:00:09.747867 per 50 iters\n",
      "time = 0:02:25.424429, epoch 92, iter = 150, loss = 0.10402379244565964, perplexity = 1.109626855338174, 0:00:09.843429 per 50 iters\n",
      "time = 0:02:25.589580, epoch 92, iter = 200, loss = 0.12083746358752251, perplexity = 1.1284414846303021, 0:00:09.908958 per 50 iters\n",
      "time = 0:02:25.751561, epoch 92, iter = 250, loss = 0.1160853962879628, perplexity = 1.1230917759069874, 0:00:09.718402 per 50 iters\n",
      "time = 0:02:25.917651, epoch 92, iter = 300, loss = 0.13391374468803405, perplexity = 1.1432942001935757, 0:00:09.965276 per 50 iters\n",
      "time = 0:02:26.082421, epoch 92, iter = 350, loss = 0.13785460628569127, perplexity = 1.147808653978119, 0:00:09.886089 per 50 iters\n",
      "time = 0:02:26.249356, epoch 92, iter = 400, loss = 0.14617924157530068, perplexity = 1.1574036243085197, 0:00:10.015933 per 50 iters\n",
      "time = 0:02:26.414310, epoch 92, iter = 450, loss = 0.22193729013204574, perplexity = 1.2484930825730154, 0:00:09.897112 per 50 iters\n",
      "\n",
      "time = 0:02:26.695218, epoch 93, iter = 50, loss = 0.200991189442575, perplexity = 1.2226139998650192, 0:00:16.854340 per 50 iters\n",
      "time = 0:02:26.862953, epoch 93, iter = 100, loss = 0.08666523471474648, perplexity = 1.0905315465004826, 0:00:10.063934 per 50 iters\n",
      "time = 0:02:27.028859, epoch 93, iter = 150, loss = 0.10015795551240445, perplexity = 1.105345499702009, 0:00:09.954268 per 50 iters\n",
      "time = 0:02:27.192026, epoch 93, iter = 200, loss = 0.11036711141467094, perplexity = 1.1166879441105284, 0:00:09.789876 per 50 iters\n",
      "time = 0:02:27.358054, epoch 93, iter = 250, loss = 0.12271768264472485, perplexity = 1.1305651977118956, 0:00:09.961567 per 50 iters\n",
      "time = 0:02:27.524259, epoch 93, iter = 300, loss = 0.1496670789271593, perplexity = 1.1614475080053723, 0:00:09.972168 per 50 iters\n",
      "time = 0:02:27.688506, epoch 93, iter = 350, loss = 0.13115305036306382, perplexity = 1.1401422671470207, 0:00:09.854650 per 50 iters\n",
      "time = 0:02:27.848755, epoch 93, iter = 400, loss = 0.1825531892105937, perplexity = 1.2002779910945984, 0:00:09.614819 per 50 iters\n",
      "time = 0:02:28.012087, epoch 93, iter = 450, loss = 0.20708323009312152, perplexity = 1.2300849476050555, 0:00:09.799807 per 50 iters\n",
      "\n",
      "time = 0:02:28.294615, epoch 94, iter = 50, loss = 0.24851640369743108, perplexity = 1.282121853735772, 0:00:16.951522 per 50 iters\n",
      "time = 0:02:28.459745, epoch 94, iter = 100, loss = 0.12777606658637525, perplexity = 1.136298518998166, 0:00:09.907680 per 50 iters\n",
      "time = 0:02:28.622173, epoch 94, iter = 150, loss = 0.1060864657908678, perplexity = 1.1119180152206822, 0:00:09.745512 per 50 iters\n",
      "time = 0:02:28.783898, epoch 94, iter = 200, loss = 0.12183011502493173, perplexity = 1.129562189834844, 0:00:09.703387 per 50 iters\n",
      "time = 0:02:28.946552, epoch 94, iter = 250, loss = 0.12508091047406197, perplexity = 1.1332401403545276, 0:00:09.759023 per 50 iters\n",
      "time = 0:02:29.111651, epoch 94, iter = 300, loss = 0.12237135395407676, perplexity = 1.1301737183414615, 0:00:09.905804 per 50 iters\n",
      "time = 0:02:29.277976, epoch 94, iter = 350, loss = 0.13599930569529534, perplexity = 1.1456810981428092, 0:00:09.979378 per 50 iters\n",
      "time = 0:02:29.441801, epoch 94, iter = 400, loss = 0.18759613528847693, perplexity = 1.206346216288143, 0:00:09.829357 per 50 iters\n",
      "time = 0:02:29.607649, epoch 94, iter = 450, loss = 0.15082090556621552, perplexity = 1.1627883905041865, 0:00:09.950728 per 50 iters\n",
      "\n",
      "time = 0:02:29.891532, epoch 95, iter = 50, loss = 0.23829085774719716, perplexity = 1.269078260393063, 0:00:17.032802 per 50 iters\n",
      "time = 0:02:30.054511, epoch 95, iter = 100, loss = 0.09317502923309803, perplexity = 1.097653839965439, 0:00:09.778527 per 50 iters\n",
      "time = 0:02:30.220621, epoch 95, iter = 150, loss = 0.09877444233745336, perplexity = 1.1038172970283269, 0:00:09.966417 per 50 iters\n",
      "time = 0:02:30.387989, epoch 95, iter = 200, loss = 0.11373172372579575, perplexity = 1.1204514940076515, 0:00:10.041955 per 50 iters\n",
      "time = 0:02:30.549831, epoch 95, iter = 250, loss = 0.13479522198438645, perplexity = 1.1443024323757036, 0:00:09.710323 per 50 iters\n",
      "time = 0:02:30.713942, epoch 95, iter = 300, loss = 0.12927478885394522, perplexity = 1.1380027916879034, 0:00:09.846478 per 50 iters\n",
      "time = 0:02:30.875789, epoch 95, iter = 350, loss = 0.15781887516379356, perplexity = 1.1709540866320827, 0:00:09.710695 per 50 iters\n",
      "time = 0:02:31.043088, epoch 95, iter = 400, loss = 0.1563971035182476, perplexity = 1.1692904402563415, 0:00:10.037821 per 50 iters\n",
      "time = 0:02:31.205375, epoch 95, iter = 450, loss = 0.2241748920828104, perplexity = 1.2512898409792146, 0:00:09.737090 per 50 iters\n",
      "\n",
      "time = 0:02:31.488644, epoch 96, iter = 50, loss = 0.22419951539486646, perplexity = 1.2513206522587779, 0:00:16.996048 per 50 iters\n",
      "time = 0:02:31.652085, epoch 96, iter = 100, loss = 0.09498739540576935, perplexity = 1.0996449944597961, 0:00:09.806279 per 50 iters\n",
      "time = 0:02:31.816226, epoch 96, iter = 150, loss = 0.12923534743487836, perplexity = 1.1379579081280387, 0:00:09.848327 per 50 iters\n",
      "time = 0:02:31.976102, epoch 96, iter = 200, loss = 0.10855080984067172, perplexity = 1.1146615428749347, 0:00:09.592450 per 50 iters\n",
      "time = 0:02:32.141345, epoch 96, iter = 250, loss = 0.11089327573776245, perplexity = 1.1172756600707718, 0:00:09.914418 per 50 iters\n",
      "time = 0:02:32.302958, epoch 96, iter = 300, loss = 0.13100021138787268, perplexity = 1.13996802228738, 0:00:09.696705 per 50 iters\n",
      "time = 0:02:32.466042, epoch 96, iter = 350, loss = 0.14417709909379484, perplexity = 1.1550886555659847, 0:00:09.784866 per 50 iters\n",
      "time = 0:02:32.633014, epoch 96, iter = 400, loss = 0.1544618559628725, perplexity = 1.1670297619819185, 0:00:10.018223 per 50 iters\n",
      "time = 0:02:32.799285, epoch 96, iter = 450, loss = 0.154314441755414, perplexity = 1.1668577378941842, 0:00:09.976137 per 50 iters\n",
      "\n",
      "time = 0:02:33.078551, epoch 97, iter = 50, loss = 0.2123695956915617, perplexity = 1.2366048444508675, 0:00:16.755855 per 50 iters\n",
      "time = 0:02:33.246175, epoch 97, iter = 100, loss = 0.0936066970974207, perplexity = 1.0981277641358604, 0:00:10.057293 per 50 iters\n",
      "time = 0:02:33.410114, epoch 97, iter = 150, loss = 0.10932687945663928, perplexity = 1.1155269335887326, 0:00:09.835374 per 50 iters\n",
      "time = 0:02:33.571134, epoch 97, iter = 200, loss = 0.12252359747188166, perplexity = 1.1303457930623706, 0:00:09.661065 per 50 iters\n",
      "time = 0:02:33.736270, epoch 97, iter = 250, loss = 0.12338491320610047, perplexity = 1.1313197970813216, 0:00:09.908034 per 50 iters\n",
      "time = 0:02:33.902457, epoch 97, iter = 300, loss = 0.10775792129337787, perplexity = 1.113778090789284, 0:00:09.971093 per 50 iters\n",
      "time = 0:02:34.067233, epoch 97, iter = 350, loss = 0.14628863707184792, perplexity = 1.1575302459785006, 0:00:09.886424 per 50 iters\n",
      "time = 0:02:34.232158, epoch 97, iter = 400, loss = 0.13356030344963074, perplexity = 1.1428901842777486, 0:00:09.895411 per 50 iters\n",
      "time = 0:02:34.396212, epoch 97, iter = 450, loss = 0.21684200182557106, perplexity = 1.2421478295433779, 0:00:09.843104 per 50 iters\n",
      "\n",
      "time = 0:02:34.676409, epoch 98, iter = 50, loss = 0.22620743110775948, perplexity = 1.2538357228381773, 0:00:16.811715 per 50 iters\n",
      "time = 0:02:34.838036, epoch 98, iter = 100, loss = 0.08889067025156691, perplexity = 1.092961156625712, 0:00:09.697477 per 50 iters\n",
      "time = 0:02:35.002734, epoch 98, iter = 150, loss = 0.09643730185925961, perplexity = 1.101240533245287, 0:00:09.881321 per 50 iters\n",
      "time = 0:02:35.164335, epoch 98, iter = 200, loss = 0.13840442299842834, perplexity = 1.1484399118812705, 0:00:09.695976 per 50 iters\n",
      "time = 0:02:35.330597, epoch 98, iter = 250, loss = 0.12185870543122292, perplexity = 1.1295944849384454, 0:00:09.975588 per 50 iters\n",
      "time = 0:02:35.494515, epoch 98, iter = 300, loss = 0.1389245443791151, perplexity = 1.1490373954023814, 0:00:09.834919 per 50 iters\n",
      "time = 0:02:35.659733, epoch 98, iter = 350, loss = 0.13720098681747914, perplexity = 1.14705866902518, 0:00:09.912909 per 50 iters\n",
      "time = 0:02:35.823832, epoch 98, iter = 400, loss = 0.17611040197312833, perplexity = 1.192569713432518, 0:00:09.845791 per 50 iters\n",
      "time = 0:02:35.988615, epoch 98, iter = 450, loss = 0.17581587873399257, perplexity = 1.1922185256566418, 0:00:09.886913 per 50 iters\n",
      "\n",
      "time = 0:02:36.269978, epoch 99, iter = 50, loss = 0.24104514047503472, perplexity = 1.2725784788087582, 0:00:16.881647 per 50 iters\n",
      "time = 0:02:36.433201, epoch 99, iter = 100, loss = 0.09938367672264575, perplexity = 1.104489985372275, 0:00:09.793212 per 50 iters\n",
      "time = 0:02:36.598145, epoch 99, iter = 150, loss = 0.09569472277536989, perplexity = 1.1004230786088887, 0:00:09.896520 per 50 iters\n",
      "time = 0:02:36.764528, epoch 99, iter = 200, loss = 0.11641407921910286, perplexity = 1.1234609776756677, 0:00:09.982857 per 50 iters\n",
      "time = 0:02:36.925489, epoch 99, iter = 250, loss = 0.14345306989271195, perplexity = 1.154252640335756, 0:00:09.657536 per 50 iters\n",
      "time = 0:02:37.092898, epoch 99, iter = 300, loss = 0.10521315552294254, perplexity = 1.1109473896912085, 0:00:10.044423 per 50 iters\n",
      "time = 0:02:37.254033, epoch 99, iter = 350, loss = 0.17290588036179544, perplexity = 1.1887542147019718, 0:00:09.667986 per 50 iters\n",
      "time = 0:02:37.418921, epoch 99, iter = 400, loss = 0.15201797343790532, perplexity = 1.1641811605818726, 0:00:09.892767 per 50 iters\n",
      "time = 0:02:37.583699, epoch 99, iter = 450, loss = 0.19584548205137253, perplexity = 1.2163389446144803, 0:00:09.886523 per 50 iters\n",
      "\n",
      "time = 0:02:37.865629, epoch 100, iter = 50, loss = 0.2074504554271698, perplexity = 1.2305367489122132, 0:00:16.915673 per 50 iters\n",
      "time = 0:02:38.027163, epoch 100, iter = 100, loss = 0.09551671601831913, perplexity = 1.1002272132984732, 0:00:09.691925 per 50 iters\n",
      "time = 0:02:38.192470, epoch 100, iter = 150, loss = 0.10043164283037186, perplexity = 1.1056480601488397, 0:00:09.918283 per 50 iters\n",
      "time = 0:02:38.356298, epoch 100, iter = 200, loss = 0.10528444685041904, perplexity = 1.1110265934286123, 0:00:09.829548 per 50 iters\n",
      "time = 0:02:38.521383, epoch 100, iter = 250, loss = 0.11293215118348598, perplexity = 1.1195559698240374, 0:00:09.905003 per 50 iters\n",
      "time = 0:02:38.689393, epoch 100, iter = 300, loss = 0.12019371435046196, perplexity = 1.1277152850557937, 0:00:10.080437 per 50 iters\n",
      "time = 0:02:38.850001, epoch 100, iter = 350, loss = 0.16160019300878048, perplexity = 1.17539021814136, 0:00:09.636365 per 50 iters\n",
      "time = 0:02:39.010986, epoch 100, iter = 400, loss = 0.1967963732033968, perplexity = 1.217496100632295, 0:00:09.658903 per 50 iters\n",
      "time = 0:02:39.178467, epoch 100, iter = 450, loss = 0.1500521819293499, perplexity = 1.1618948710624915, 0:00:10.048759 per 50 iters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "train_model(model, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = list(range(100))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch,Loss_hist)\n",
    "plt.title('Loss Graph')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch ,Perplexity_hist)\n",
    "plt.title('Perplexity Graph')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "EiNRQYM0zeiW"
   },
   "outputs": [],
   "source": [
    "def translate(model, src, max_len=80, custom_sentence=False):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if custom_sentence == True:\n",
    "        src = tokenize_ja(src)\n",
    "        src = torch.autograd.Variable(torch.LongTensor([[JA_TEXT.vocab.stoi[tok] for tok in src]])).to(device)\n",
    "    \n",
    "    src_mask = (src != input_pad).unsqueeze(-2)\n",
    "    e_outputs = model.encoder(src, src_mask)\n",
    "\n",
    "    outputs = torch.zeros(max_len).type_as(src.data)\n",
    "    outputs[0] = torch.LongTensor([EN_TEXT.vocab.stoi['<sos>']])\n",
    "    \n",
    "    for i in range(1, max_len):    \n",
    "        trg_mask = np.triu(np.ones((1, i, i)), k=1).astype('uint8')\n",
    "        trg_mask = torch.autograd.Variable(torch.from_numpy(trg_mask) == 0).to(device)\n",
    "\n",
    "        out = model.out(\n",
    "            model.decoder(\n",
    "                outputs[:i].unsqueeze(0),\n",
    "                e_outputs,\n",
    "                src_mask,\n",
    "                trg_mask\n",
    "            )\n",
    "        )\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        val, ix = out[:, -1].data.topk(1)\n",
    "\n",
    "        outputs[i] = ix[0][0]\n",
    "        if ix[0][0] == EN_TEXT.vocab.stoi['<eos>']:\n",
    "            break\n",
    "\n",
    "    return ' '.join([EN_TEXT.vocab.itos[ix] for ix in outputs[:i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZMWD-MePw95"
   },
   "source": [
    "# **Results and Analysis**\n",
    "\n",
    "We tried translating 8 sentences using our model and our analysis are as mentioned below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCMPBeatQBy-"
   },
   "source": [
    "The below sentence translated correctly from Japanese to English. One thing to note here is we have not added \"end of the sentence dot\". Even though we have not added the sentence got predicted correctly as there was a subject \"I\" (僕)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "d1l9JS5Lp3hm",
    "outputId": "850df8ab-a105-4016-869d-28e251b69a32"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"<sos> I 'm on my way to the station .\""
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, '僕は駅に行く途中なの', custom_sentence=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VGa9U2kQxy0"
   },
   "source": [
    "The below is the same sentence that we tried without giving the subject \"I\" (僕). And the results are correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5c5Jnn_Wp4Ae",
    "outputId": "702bb316-c1b6-4595-9d73-3e28e57eb0d3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"<sos> I 'm on my way to the station .\""
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, '駅に行く途中なの。', custom_sentence=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQMJRj5ERB2R"
   },
   "source": [
    "The below sentence was an interesting case. Here we didnt give the subject \"I\"(僕) and didnt end the sentence with (。) and as we can see the translated is weird. The correct translation is \"On the way to the station\". So we have analysed that our model predicts fairly if proper end of sentences dot is given or atleast a subject shoud be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "p78qyz5bq6b-",
    "outputId": "5059fba2-8bf6-495d-bf08-283784be2eb9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<sos> the way of the station on my way to the station .'"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, '駅に行く途中なの', custom_sentence=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4DH9oTkR3Q7"
   },
   "source": [
    "This below sentence is another sample sentece taken from the test file and our model seems to translate it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "GyabhuEXp5GE",
    "outputId": "0caabd64-e8f5-4eec-9921-2e88a636947a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"<sos> I do n't know if he 's a doctor .\""
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, '私は彼が医者であるかどうかわからない。', custom_sentence=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luvHSyjfSAWE"
   },
   "source": [
    "The below sentence is also been translated more or less fairly. The correct translation is more like \"I am really sorry to interupt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RFRYtyr7p5fj",
    "outputId": "1873c7ee-65f1-4f1c-85ed-c11446a5a437"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"<sos> I 'm sorry to have troubled you .\""
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, 'お話し中、申し訳ありません。', custom_sentence=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9BQq7BhSXgQ"
   },
   "source": [
    "The below sentence is another interesting case. This sentence translation is actually \"Tokyo Olympic Organization Committee\" which Google predicts it correctly but our model did not predict it well. This sentence is taken from the first dataset (Kyoto_lexicon) where it dealt only about Japanese cultures, organizations and religion. We think that to predict this sentence more accurately we need more data on that dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "W6PgZt_1qjNa",
    "outputId": "89cdd3ff-f5b2-448e-a9fc-50efd020d91f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<sos> Tokyo is the failed to leave Japan .'"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, '東京オリンピック組織委員会。', custom_sentence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21YDK4RdTFDp"
   },
   "source": [
    "The below two sentences are some random sentences which was given to model out of curiosity. And our model seems to translate it well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NF4vsoE6zpXz",
    "outputId": "c61eb0f5-2b63-4888-906a-78ac69689435"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<sos> I think that Japanese is difficult .'"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, '日本語は難しいと思います', custom_sentence=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4qJwWW9XQBKV",
    "outputId": "4a18de1f-43a3-4915-8dec-b26f82d86ade"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<sos> English is easy .'"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, '英語は簡単です' , custom_sentence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQjk73JMTS5Z"
   },
   "source": [
    "# **BLEU Score**\n",
    "\n",
    "We could achieve a BLEU score of 41.49 for our model for Japanese to English Neural Machine Translation which is equal to the State of the art BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "il7QHsJVOIYV"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "4tYxtzJSOb45",
    "outputId": "49dfddcc-2a83-4b1a-8a8a-2c8e25013504"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Japanese</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>睡眠をとったほうがいい。</td>\n",
       "      <td>You need sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>本ばっかり読んでないで、たまには外で体を動かしてきなさい。</td>\n",
       "      <td>Don't just read books. Go outside once in a wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>達人</td>\n",
       "      <td>a master</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>好事魔多し。</td>\n",
       "      <td>Happy events tend to be accompanied by problems.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>伊勢神戸藩</td>\n",
       "      <td>Ise-Kanbe Domain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22887</th>\n",
       "      <td>彼女は研究に夢中だ。</td>\n",
       "      <td>She is absorbed in her study.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22888</th>\n",
       "      <td>京阪バス山科営業所</td>\n",
       "      <td>Yamashina Office, Keihan Bus Co., Ltd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22889</th>\n",
       "      <td>トムは頭に青いバンダナをしている。</td>\n",
       "      <td>Tom is wearing a blue bandana on his head.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22890</th>\n",
       "      <td>大伴王・長岡王・名草王・山階王・采女王</td>\n",
       "      <td>Prince Otomo, Prince Nagaoka, Prince Nagusa, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22891</th>\n",
       "      <td>ウェイトレスはジュースを私の前に置いた。</td>\n",
       "      <td>The waitress set a glass of juice in front of me.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22892 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Japanese                                            English\n",
       "0                       睡眠をとったほうがいい。                                    You need sleep.\n",
       "1      本ばっかり読んでないで、たまには外で体を動かしてきなさい。  Don't just read books. Go outside once in a wh...\n",
       "2                                 達人                                           a master\n",
       "3                             好事魔多し。   Happy events tend to be accompanied by problems.\n",
       "4                              伊勢神戸藩                                   Ise-Kanbe Domain\n",
       "...                              ...                                                ...\n",
       "22887                     彼女は研究に夢中だ。                      She is absorbed in her study.\n",
       "22888                      京阪バス山科営業所             Yamashina Office, Keihan Bus Co., Ltd.\n",
       "22889              トムは頭に青いバンダナをしている。         Tom is wearing a blue bandana on his head.\n",
       "22890            大伴王・長岡王・名草王・山階王・采女王  Prince Otomo, Prince Nagaoka, Prince Nagusa, P...\n",
       "22891           ウェイトレスはジュースを私の前に置いた。  The waitress set a glass of juice in front of me.\n",
       "\n",
       "[22892 rows x 2 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "6fnhEL54RjRL"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "BTsOAwfAOsa4"
   },
   "outputs": [],
   "source": [
    "data_length = len(test_df[\"English\"]);\n",
    "total_score = 0;\n",
    "\n",
    "for i in range(data_length):\n",
    "  # print(i)\n",
    "  reference = tokenize_en(test_df[\"English\"][i]);\n",
    "  candidate = tokenize_en(translate(model, test_df[\"Japanese\"][i] , custom_sentence=True));\n",
    "  candidate = candidate[3:]\n",
    "  score = sentence_bleu(reference, candidate)\n",
    "  total_score += score;\n",
    "\n",
    "# print(total_score/data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f7jvdUJ0bs2",
    "outputId": "729d11b7-ae96-4b07-ba0c-96715ed96b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 41.49447833345196\n"
     ]
    }
   ],
   "source": [
    "print(f'BLEU score = {(total_score/data_length)*100}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liJsweWAvpn2"
   },
   "source": [
    "# REFERENCES\n",
    "\n",
    "1) https://charon.me/posts/pytorch/pytorch_seq2seq_1/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2) https://github.com/bentrevett/pytorch-seq2seq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3) https://www.guru99.com/seq2seq-model.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4) https://arxiv.org/pdf/1409.3215.pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5) https://www.kaggle.com/phunghieu/transformer-demo/log\n",
    "\n",
    "\n",
    "\n",
    "6) https://en.wikipedia.org/wiki/Perplexity\n",
    "\n",
    "    \n",
    "7) https://machinelearningmastery.com/calculate-bleu-score-for-text-python/#:~:text=The%20Bilingual%20Evaluation%20Understudy%20Score,in%20a%20score%20of%200.0.\n",
    "    \n",
    "\n",
    "8) http://nlp.seas.harvard.edu/2018/04/03/attention.html#batches-and-masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Project Japanese to English Translation - with Anki Dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
